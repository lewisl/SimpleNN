{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/lewis/code/nn by hand/notebooks\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/lewis/code/nn by hand/chatgpt_conv_code\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cd(\"../chatgpt_conv_code\")\n",
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "includet(\"../chatgpt_conv_code/sample_code.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :train\n",
       "  features  =>    28×28×60000 Array{Float32, 3}\n",
       "  targets   =>    60000-element Vector{Int64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = MNIST(:train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :test\n",
       "  features  =>    28×28×10000 Array{Float32, 3}\n",
       "  targets   =>    10000-element Vector{Int64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset = MNIST(:test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "minibatch_size = 50\n",
    "prediction_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1, 10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = trainset.features[1:28,1:28,1:batch_size];\n",
    "x_train = Float64.(x_train)\n",
    "x_train = reshape(x_train, 28,28,1,batch_size)\n",
    "size(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainset.targets[1:batch_size]\n",
    "y_train = indicatormat(y_train)\n",
    "y_train = Float64.(y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the variables and outcome identically\n",
    "img_idx = shuffle(1:size(x_train,4));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# where will we find image 1 after shuffling?\n",
    "pos1 = findall(x->x==1, img_idx)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_shuf = x_train[:,:,:,img_idx];\n",
    "y_train_shuf = y_train[:,img_idx];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_shuf[:,:,:,pos1] == x_train[:,:,:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_shuf[:,pos1] == y_train[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count(x->x==1.0,x_train[:,:,1,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Vector{Int64}:\n",
       "  4\n",
       "  9\n",
       "  6\n",
       "  7\n",
       "  1\n",
       "  7\n",
       "  6\n",
       "  7\n",
       "  1\n",
       "  3\n",
       "  ⋮\n",
       "  9\n",
       " 10\n",
       "  4\n",
       " 10\n",
       "  2\n",
       "  8\n",
       "  3\n",
       "  7\n",
       "  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = getvalidx(y_train_shuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944-element Vector{Int64}:\n",
       "    2\n",
       "   21\n",
       "   22\n",
       "   29\n",
       "   33\n",
       "   34\n",
       "   35\n",
       "   37\n",
       "   48\n",
       "   58\n",
       "    ⋮\n",
       " 9930\n",
       " 9937\n",
       " 9941\n",
       " 9947\n",
       " 9952\n",
       " 9960\n",
       " 9967\n",
       " 9982\n",
       " 9992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "findall(digits .== 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Int64}:\n",
       " 8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getvalidx(y_train_shuf[:,9955])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip470\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip471\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M140.696 1486.45 L2352.76 1486.45 L2352.76 47.2441 L140.696 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip472\">\n",
       "    <rect x=\"140\" y=\"47\" width=\"2213\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"501.424,47.2441 501.424,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1015.43,47.2441 1015.43,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1529.43,47.2441 1529.43,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2043.43,47.2441 2043.43,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,278.545 2352.76,278.545 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,535.545 2352.76,535.545 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,792.546 2352.76,792.546 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,1049.55 2352.76,1049.55 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,1306.55 2352.76,1306.55 \"/>\n",
       "<g clip-path=\"url(#clip472)\">\n",
       "<image width=\"1439\" height=\"1439\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAABZ8AAAWfCAYAAADEdIgFAAAgAElEQVR4nOzasUmeDRSG4ddfQbAR\n",
       "CamCiFtYCTY2KVJlj7iJRVZwgpBCk9JOUASrEEjhABZWIsFMkCr3z+GT65rgKQ83Z21ZlpcFAAAA\n",
       "AABC/00PAAAAAADg9RGfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAA\n",
       "gJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA5\n",
       "8RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIz\n",
       "AAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAA\n",
       "AAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAA\n",
       "AOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADI\n",
       "ic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOf\n",
       "AQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAA\n",
       "kBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAn\n",
       "PgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwG\n",
       "AAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAA\n",
       "gJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA5\n",
       "8RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIz\n",
       "AAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAA\n",
       "AAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAA\n",
       "AOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADI\n",
       "ic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOf\n",
       "AQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAA\n",
       "kBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAn\n",
       "PgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwG\n",
       "AAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAA\n",
       "gJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA5\n",
       "8RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIz\n",
       "AAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAA\n",
       "AAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAA\n",
       "AOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADI\n",
       "ic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABAbmN6AKyik5OT6QmZq6ur6Qmp\n",
       "y8vL6QkAAAAALD6fAQAAAAD4H4jPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc\n",
       "+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZ\n",
       "AAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAA\n",
       "AAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAA\n",
       "AHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADk\n",
       "xGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInP\n",
       "AAAAAADkxGcAAAAAAHIb0wNgFe3v709PyHz69Gl6Qur09HR6Qubt27fTE1LX19fTEzJfvnyZnpB5\n",
       "enqangAAAMAr5fMZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADk\n",
       "xGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInP\n",
       "AAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEA\n",
       "AAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAA\n",
       "AJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAg\n",
       "Jz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58\n",
       "BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwA\n",
       "AAAAQG5tWZaX6RHAnOfn5+kJqYuLi+kJmc+fP09PSH39+nV6Qubp6Wl6Qubm5mZ6QurHjx/TE/iL\n",
       "7e3t6QmZ3d3d6QmZu7u76Qmwcr59+zY9IXN2djY9AYBXzuczAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgNzasiwv0yNg1ezu7k5PyPz69Wt6Qur8/Hx6QubD\n",
       "hw/TE1J7e3vTEzIfP36cnpA5OjqangAr5/7+fnoCf/GabrTNzc3pCanDw8PpCZmHh4fpCZk3b95M\n",
       "TwDglfP5DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAA\n",
       "kBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAn\n",
       "PgMAAAAAkFtbluVlegQw5/n5eXpC6uLiYnpC5v3799MTAIAh7969m56Qur+/n56Q+f79+/SEzPHx\n",
       "8fQEAF45n88AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "chvTAwBKOzs70xMy6+vr0xNSv3//np4AAPDPfv78OT0BAFaGz2cAAAAAAHLiMwAAAAAAOfEZAAAA\n",
       "AICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAA\n",
       "OfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLi\n",
       "MwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcA\n",
       "AAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAA\n",
       "AADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAA\n",
       "yInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJAT\n",
       "nwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAIDcxvQAYNbNzc30hNTBwcH0hMzW\n",
       "1tb0hNTj4+P0BACAf3Z7ezs9AQBWhs9nAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn/rBrxypZ92Ec\n",
       "h//CIzy4NQUNanQGza0NDW2KJ9E5dALNDp5IhxE0iUMOTqGgWEo1aPM7NL0fuPHhuo7g+5t+8OEG\n",
       "AAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAA\n",
       "AACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAA\n",
       "ICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5FbTA4BZHz9+nJ6Q+vTp0/SEzO/fv6cnAABDnj17Nj2B\n",
       "fzg7O5ueAABPhstnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMA\n",
       "AAAAALnV9ABg1ufPn6cnpL58+TI9IfPnz5/pCQDAkIODg+kJqdvb2+kJmZ8/f05PyLx8+XJ6Qurm\n",
       "5mZ6Qub6+np6AkDC5TMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBua1mWx+kRAAD81+Hh4fSE1NHR0fSEzMXFxfSEzLt376Yn8A+7u7vTE1Lr\n",
       "9Xp6Qubh4WF6Qub+/n56QurHjx/TEzKb9JZl2ay/8+3bt9MT4Elx+QwAAAAAQE58BgAAAAAgJz4D\n",
       "AAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAA\n",
       "AAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc\n",
       "+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZ\n",
       "AAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAA\n",
       "AAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAA\n",
       "AHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgt5oeAADA5nv9+vX0hMz79++nJ2S+ffs2\n",
       "PSG1vb09PSGzXq+nJ6S+f/8+PSFzcnIyPQGenIuLi+kJwBCXzwAAAAAA5MRnAAAAAABy4jMAAAAA\n",
       "ADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy\n",
       "4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn\n",
       "AAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAA\n",
       "AAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAALmtZVkep0cAALDZ1uv19ITM/v7+\n",
       "9ITM6enp9ITU3t7e9ITM+fn59ITUhw8fpidkjo+PpycAwJPh8hkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAZJDRP0AAA+rSURBVAAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABAbjU9AACAzffr16/pCZnT\n",
       "09PpCfDkfP36dXoCADDA5TMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58B\n",
       "AAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAA\n",
       "AACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAA\n",
       "ICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBO\n",
       "fAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgM\n",
       "AAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAA\n",
       "AACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAA\n",
       "ADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy\n",
       "4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn\n",
       "AAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAA\n",
       "AAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMitpgcAAACw2XZ3d6cnAAADXD4DAAAAAJATnwEAAAAAyInPAAAA\n",
       "AADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAA\n",
       "yInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJAT\n",
       "nwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4D\n",
       "AAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAA\n",
       "AAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc\n",
       "+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyK2mBwAAALDZXr16NT0BABjg8hkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACC3mh4AAADA\n",
       "Znvx4sX0BABggMtnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMA\n",
       "AAAAALmtZVkep0cAAAD8X3t7e9MTMufn59MTUpeXl9MTMs+fP5+eAABPhstnAAAAAABy4jMAAAAA\n",
       "ADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy\n",
       "4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn\n",
       "AAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAA\n",
       "AAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAALnV9AAAAIDC3d3d9ITM5eXl9ITU\n",
       "w8PD9AQAYIDLZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOf\n",
       "AQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgNxqegAAAEDh6upqekLmzZs30xNSOzs70xMAgAEunwEAAAAAyInPAAAAAADkxGcA\n",
       "AAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAA\n",
       "AADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAA\n",
       "yInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJAT\n",
       "nwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4D\n",
       "AAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAA\n",
       "AAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADktpZleZweAQAAAADAZnH5DAAAAABATnwG\n",
       "AAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcn/bsWMBAAAAgEH+1pPY\n",
       "WRjJZwAAAAAAdvIZAAAAAICdfAYAAAAAYCefAQAAAADYyWcAAAAAAHbyGQAAAACAnXwGAAAAAGAn\n",
       "nwEAAAAA2MlnAAAAAAB28hkAAAAAgJ18BgAAAABgJ58BAAAAANjJZwAAAAAAdvIZAAAAAICdfAYA\n",
       "AAAAYCefAQAAAADYyWcAAAAAAHbyGQAAAACAnXwGAAAAAGAnnwEAAAAA2MlnAAAAAAB28hkAAAAA\n",
       "gJ18BgAAAABgJ58BAAAAANjJZwAAAAAAdvIZAAAAAICdfAYAAAAAYCefAQAAAADYyWcAAAAAAHby\n",
       "GQAAAACAnXwGAAAAAGAnnwEAAAAA2MlnAAAAAAB28hkAAAAAgJ18BgAAAABgJ58BAAAAANgF/FLH\n",
       "MdgiekQAAAAASUVORK5CYII=\n",
       "\" transform=\"translate(527, 47)\"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip520\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip521\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M140.696 1486.45 L2352.76 1486.45 L2352.76 47.2441 L140.696 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip522\">\n",
       "    <rect x=\"140\" y=\"47\" width=\"2213\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"501.424,47.2441 501.424,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1015.43,47.2441 1015.43,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1529.43,47.2441 1529.43,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2043.43,47.2441 2043.43,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,278.545 2352.76,278.545 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,535.545 2352.76,535.545 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,792.546 2352.76,792.546 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,1049.55 2352.76,1049.55 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"140.696,1306.55 2352.76,1306.55 \"/>\n",
       "<g clip-path=\"url(#clip522)\">\n",
       "<image width=\"1439\" height=\"1439\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAABZ8AAAWfCAYAAADEdIgFAAAgAElEQVR4nOzasUmeDRSG4ddfQbAR\n",
       "CamCiFtYCTY2KVJlj7iJRVZwgpBCk9JOUASrEEjhABZWIsFMkCr3z+GT65rgKQ83Z21ZlpcFAAAA\n",
       "AABC/00PAAAAAADg9RGfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAA\n",
       "gJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA5\n",
       "8RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIz\n",
       "AAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAA\n",
       "AAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAA\n",
       "AOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADI\n",
       "ic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOf\n",
       "AQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAA\n",
       "kBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAn\n",
       "PgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwG\n",
       "AAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAA\n",
       "gJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA5\n",
       "8RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIz\n",
       "AAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAA\n",
       "AAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAA\n",
       "AOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADI\n",
       "ic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOf\n",
       "AQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAA\n",
       "kBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAn\n",
       "PgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwG\n",
       "AAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAA\n",
       "gJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA5\n",
       "8RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIz\n",
       "AAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAA\n",
       "AAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAA\n",
       "AOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADI\n",
       "ic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABAbmN6AKyik5OT6QmZq6ur6Qmp\n",
       "y8vL6QkAAAAALD6fAQAAAAD4H4jPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc\n",
       "+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZ\n",
       "AAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAA\n",
       "AAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAA\n",
       "AHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADk\n",
       "xGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInP\n",
       "AAAAAADkxGcAAAAAAHIb0wNgFe3v709PyHz69Gl6Qur09HR6Qubt27fTE1LX19fTEzJfvnyZnpB5\n",
       "enqangAAAMAr5fMZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADk\n",
       "xGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInP\n",
       "AAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEA\n",
       "AAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAA\n",
       "AJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAg\n",
       "Jz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58\n",
       "BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwA\n",
       "AAAAQG5tWZaX6RHAnOfn5+kJqYuLi+kJmc+fP09PSH39+nV6Qubp6Wl6Qubm5mZ6QurHjx/TE/iL\n",
       "7e3t6QmZ3d3d6QmZu7u76Qmwcr59+zY9IXN2djY9AYBXzuczAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgNzasiwv0yNg1ezu7k5PyPz69Wt6Qur8/Hx6QubD\n",
       "hw/TE1J7e3vTEzIfP36cnpA5OjqangAr5/7+fnoCf/GabrTNzc3pCanDw8PpCZmHh4fpCZk3b95M\n",
       "TwDglfP5DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAA\n",
       "kBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAn\n",
       "PgMAAAAAkFtbluVlegQw5/n5eXpC6uLiYnpC5v3799MTAIAh7969m56Qur+/n56Q+f79+/SEzPHx\n",
       "8fQEAF45n88AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "chvTAwBKOzs70xMy6+vr0xNSv3//np4AAPDPfv78OT0BAFaGz2cAAAAAAHLiMwAAAAAAOfEZAAAA\n",
       "AICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAA\n",
       "OfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLi\n",
       "MwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcA\n",
       "AAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAA\n",
       "AADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAA\n",
       "yInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJAT\n",
       "nwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAIDcxvQAYNbNzc30hNTBwcH0hMzW\n",
       "1tb0hNTj4+P0BACAf3Z7ezs9AQBWhs9nAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn/rBrxypZ92Ec\n",
       "h//CIzy4NQUNanQGza0NDW2KJ9E5dALNDp5IhxE0iUMOTqGgWEo1aPM7NL0fuPHhuo7g+5t+8OEG\n",
       "AAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAA\n",
       "AACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAA\n",
       "ICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5FbTA4BZHz9+nJ6Q+vTp0/SEzO/fv6cnAABDnj17Nj2B\n",
       "fzg7O5ueAABPhstnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMA\n",
       "AAAAALnV9ABg1ufPn6cnpL58+TI9IfPnz5/pCQDAkIODg+kJqdvb2+kJmZ8/f05PyLx8+XJ6Qurm\n",
       "5mZ6Qub6+np6AkDC5TMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBua1mWx+kRAAD81+Hh4fSE1NHR0fSEzMXFxfSEzLt376Yn8A+7u7vTE1Lr\n",
       "9Xp6Qubh4WF6Qub+/n56QurHjx/TEzKb9JZl2ay/8+3bt9MT4Elx+QwAAAAAQE58BgAAAAAgJz4D\n",
       "AAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAA\n",
       "AAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc\n",
       "+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZ\n",
       "AAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAA\n",
       "AAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAA\n",
       "AHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgt5oeAADA5nv9+vX0hMz79++nJ2S+ffs2\n",
       "PSG1vb09PSGzXq+nJ6S+f/8+PSFzcnIyPQGenIuLi+kJwBCXzwAAAAAA5MRnAAAAAABy4jMAAAAA\n",
       "ADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy\n",
       "4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn\n",
       "AAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAA\n",
       "AAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAALmtZVkep0cAALDZ1uv19ITM/v7+\n",
       "9ITM6enp9ITU3t7e9ITM+fn59ITUhw8fpidkjo+PpycAwJPh8hkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAZJDRP0AAA+rSURBVAAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABAbjU9AACAzffr16/pCZnT\n",
       "09PpCfDkfP36dXoCADDA5TMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58B\n",
       "AAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAA\n",
       "AACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAA\n",
       "ICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBO\n",
       "fAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgM\n",
       "AAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAA\n",
       "AACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAA\n",
       "ADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy\n",
       "4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn\n",
       "AAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAA\n",
       "AAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMitpgcAAACw2XZ3d6cnAAADXD4DAAAAAJATnwEAAAAAyInPAAAA\n",
       "AADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAA\n",
       "yInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJAT\n",
       "nwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4D\n",
       "AAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAA\n",
       "AAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc\n",
       "+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyK2mBwAAALDZXr16NT0BABjg8hkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAA\n",
       "cuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTE\n",
       "ZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8A\n",
       "AAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAA\n",
       "AADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACC3mh4AAADA\n",
       "Znvx4sX0BABggMtnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYA\n",
       "AAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAA\n",
       "AEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACA\n",
       "nPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnx\n",
       "GQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMA\n",
       "AAAAALmtZVkep0cAAAD8X3t7e9MTMufn59MTUpeXl9MTMs+fP5+eAABPhstnAAAAAABy4jMAAAAA\n",
       "ADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy\n",
       "4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAAAAAA5MRn\n",
       "AAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAAAMiJzwAA\n",
       "AAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQE58BAAAA\n",
       "AMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+AwAAAACQ\n",
       "E58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAADnxGQAAAACAnPgMAAAAAEBOfAYAAAAAICc+\n",
       "AwAAAACQE58BAAAAAMiJzwAAAAAA5MRnAAAAAABy4jMAAAAAALnV9AAAAIDC3d3d9ITM5eXl9ITU\n",
       "w8PD9AQAYIDLZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOf\n",
       "AQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAAACAnPgMA\n",
       "AAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABATnwGAAAA\n",
       "ACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAAAABA\n",
       "TnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4\n",
       "DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkA\n",
       "AAAAgJz4DAAAAABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAA\n",
       "AAA58RkAAAAAgNxqegAAAEDh6upqekLmzZs30xNSOzs70xMAgAEunwEAAAAAyInPAAAAAADkxGcA\n",
       "AAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAA\n",
       "AADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJATnwEAAAAA\n",
       "yInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4DAAAAAJAT\n",
       "nwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAAAAAgJz4D\n",
       "AAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAAQE58BgAA\n",
       "AAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADkxGcAAAAAAHLiMwAAAAAAOfEZAAAAAICc+AwAAAAA\n",
       "QE58BgAAAAAgJz4DAAAAAJATnwEAAAAAyInPAAAAAADktpZleZweAQAAAADAZnH5DAAAAABATnwG\n",
       "AAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcuIzAAAAAAA58RkAAAAAgJz4DAAA\n",
       "AABATnwGAAAAACAnPgMAAAAAkBOfAQAAAADIic8AAAAAAOTEZwAAAAAAcn/bsWMBAAAAgEH+1pPY\n",
       "WRjJZwAAAAAAdvIZAAAAAICdfAYAAAAAYCefAQAAAADYyWcAAAAAAHbyGQAAAACAnXwGAAAAAGAn\n",
       "nwEAAAAA2MlnAAAAAAB28hkAAAAAgJ18BgAAAABgJ58BAAAAANjJZwAAAAAAdvIZAAAAAICdfAYA\n",
       "AAAAYCefAQAAAADYyWcAAAAAAHbyGQAAAACAnXwGAAAAAGAnnwEAAAAA2MlnAAAAAAB28hkAAAAA\n",
       "gJ18BgAAAABgJ58BAAAAANjJZwAAAAAAdvIZAAAAAICdfAYAAAAAYCefAQAAAADYyWcAAAAAAHby\n",
       "GQAAAACAnXwGAAAAAGAnnwEAAAAA2MlnAAAAAAB28hkAAAAAgJ18BgAAAABgJ58BAAAAANgF/FLH\n",
       "MdgiekQAAAAASUVORK5CYII=\n",
       "\" transform=\"translate(527, 47)\"/>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display_mnist_digit(x_train_shuf[:,:,:, 9955])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Int64} with 10 entries:\n",
       "  5  => 980\n",
       "  4  => 1032\n",
       "  6  => 863\n",
       "  7  => 1014\n",
       "  2  => 1127\n",
       "  10 => 978\n",
       "  9  => 944\n",
       "  8  => 1070\n",
       "  3  => 991\n",
       "  1  => 1001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "countmap(getvalidx(y_train_shuf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_factor = 0.3333333333333333\n",
      "scale_factor = 0.027777777777777776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayerDefs(ConvLayer([-0.2097845397282482 -0.1797016576905717 -0.4962645312166686; 0.14534054917682807 0.4712949304749917 -0.8482939409663467; -0.18185838994589085 0.27524503192896077 -1.220711783910476;;;; 0.02527735435894241 0.25050412632748764 0.8355551033824538; -0.4765071245996046 -0.22830729850747758 -0.05373856285477063; 0.4851408050929267 0.29856565361163323 -0.734727621624394;;;; 0.5575244937022416 -0.8443440512983529 0.8731799328159261; -0.055761079028744304 0.751116268400012 0.37461316814617623; -0.79836613085538 1.3084372019358832 0.5195830894903669;;;; … ;;;; -0.8109791033023996 -0.4535991821641914 0.3216283074914613; -0.7128942044386622 -0.23561589322645335 -0.22600586882317306; -0.07234595195154428 -0.9969681447866366 -0.1447777990169911;;;; 0.5899590533462148 0.15179967190072904 0.41769253296735354; -0.0005425719554744286 -0.6126366208908363 0.49314975631309216; -0.8664735032964339 0.598377742076984 0.21258437351055676;;;; -0.026881971774367824 -0.08337724425406373 -0.1794080587739208; 0.9862206764294802 -0.9393016185408005 -0.02336732852840717; -0.5952754977567456 -0.11928813726382353 0.06048944484254383], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], 0×0×0×0 BitArray{4}, Array{Float64, 4}(undef, 0, 0, 0, 0), [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; … ;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), ConvLayer([0.15483034769293544 0.04176888507495267 0.0730787070386926; -0.19016976968468707 -0.14246155986814396 0.07876403393222496; 0.023211793278922494 0.03480553509037343 0.17951042827524166;;; -0.06262614242619882 0.08782193250157622 -0.1195754153058758; 0.15532199588157952 0.04251610143872994 -0.07863831374900798; -0.04581835378309843 -0.019998671744881936 -0.19776172002541825;;; 0.2987766980939553 -0.25315331990204115 0.16177422183610488; -0.3382582381425917 0.22642296484995367 0.1404835863318313; 0.01202539156577831 -0.08058738712150906 0.039659848927613386;;; … ;;; 0.05677752696011033 -0.0870766584540535 0.13113601122191573; -0.046889835834706156 -0.32615709603425164 0.19153416735628961; 0.14503838001379854 0.08526530008843519 0.017909636092615363;;; 0.05827010051358526 -0.39037089176321654 0.07435691010631124; 0.04629904539806376 0.00975621788208737 0.10120770595322438; -0.09366941193342998 0.20679627083568175 -0.024684026295306416;;; -0.0444476603376235 -0.17973462957571895 -0.13522712430643452; 0.07927505363142122 0.11103214228539345 0.09420034864958222; 0.10631074473096405 0.0530636539505495 -0.17149632993452169;;;; -0.22791332224093208 0.019737715487298753 0.15471941663077488; -0.05138315702220374 0.19827344039233896 -0.20812718722203782; -0.09263347060636395 -0.10958644088043432 -0.030924112687897357;;; 0.05379644106786424 -0.13909319949673793 -0.16426966791036707; 0.30911469312398165 0.11217130325200914 -0.07701604015479208; 0.27212633042721235 0.03571830276254776 -0.07051379643202058;;; 0.05154699972956689 -0.3190196400685818 0.24651874539506327; 0.14841077001295694 0.17454425202799784 -0.3983390177451079; -0.012864467041378051 0.1771467565022412 0.06449880652099974;;; … ;;; 0.14639413855377678 -0.02902065813239736 0.17939307039579336; -0.04148050121292088 0.20113269565687925 0.20662579333252754; 0.12048327707112134 0.2643244368996107 0.028782671422503868;;; 0.21995170377125112 -0.08322269964117006 0.10245129430445173; 0.05362510040428839 -0.4689893242683326 0.08137199503185132; -0.027735028512991172 -0.37347696651034573 0.11087321380942763;;; 0.245805943013135 0.14040246986506127 0.08066732688146197; -0.020227895950992735 -0.01802523635660943 -0.09099886699659546; -0.08778928638542571 -0.07703784047450726 -0.001436780559783238;;;; 0.1859567657237501 -0.18558385470736422 0.1655263849837644; 0.06178471197381297 -0.08130780672940047 0.3024145494898448; -0.008331783905850794 -0.2891411192833006 0.07063136266982592;;; -0.006125835140059333 0.12801393275661202 0.053785484525712864; -0.0393080586274766 -0.36115608046252845 -0.00573394433908923; -0.11217569444051018 0.38374733230612534 0.08880421533332283;;; -0.10401376663814627 0.24056626910735246 -0.058957280850130994; 0.20004012663106513 0.10670486708102586 -0.2687663204995444; 0.16427176054582462 -0.27358723084127023 0.02670517039896552;;; … ;;; -0.2831072676536527 0.11869974885430815 0.2043345424090394; 0.14018347236893752 -0.05399696715898117 0.3390677678455085; -0.2534438688768112 0.04943131071459071 0.11331874569603946;;; 0.07217792352275099 -0.10837380185012471 -0.3421709631679508; -0.14949089310718128 0.021845017442986326 0.03435320896712357; -0.05994329191962608 -0.15538583271601833 0.021353870544255266;;; 0.17697330471720585 -0.06592549722309288 0.17112479084448695; 0.06645247570730362 0.15812365661860733 0.07129523281566402; 0.23414535773125125 -0.06476685105715714 -0.1989182806235793;;;; -0.16891530133089386 0.10802781850249156 -0.051297841167282955; -0.1093604587462318 0.16111334035138689 0.02007683120285366; -0.05717787676193224 -0.0012397183217072707 -0.01908527022866643;;; 0.09619929819781597 0.021133332105632847 -0.1169160758084047; -0.009679176626285748 -0.22571635643036564 -0.20150580027259424; -0.03505153253935031 0.12490368864645146 -0.01667996382584495;;; 0.027364088271910794 0.04820760655627574 -0.11710069535625768; -0.006851460143053816 0.004866805192486506 -0.1769007586924124; 0.03635440327965672 -0.016250983865141188 0.03752216177949337;;; … ;;; -0.12486679881370177 0.20475779820397477 0.1411314687388549; -0.27235630526512633 -0.020804410281009125 0.1116726835846639; 0.23081337919798806 0.11052914040544626 -0.097828938150432;;; -0.07797781490227619 0.06428428868599424 0.21607652562173096; 0.12179359237444905 -0.007628026876322748 -0.014850113859266538; 0.10410791825551081 -0.10734425207262839 0.031127562949766756;;; 0.3298127963977019 -0.0681087821847407 -0.0171027715185015; -0.288762012553042 0.03378383978917701 0.42795747776920573; -0.27701188969447715 -0.1894592788412396 -0.05514876344216006;;;; -0.13082722384061388 0.0963065036272819 0.055512910922440574; -0.07622861157114579 -0.021988890393486966 0.11517211410589528; 0.008267659253299543 0.07561551654881948 -0.2751197867905727;;; 0.21344828419538964 -0.15998699250380988 0.040005319392447655; -0.22379461766889985 0.05062348135199936 -0.06623386558803039; -0.08323836032395132 -0.32124598250596925 0.32610741269273713;;; 0.04954530617985568 -0.0035876953635499698 -0.11395208637116933; -0.17014069841922544 -0.04913558872542148 -0.160361393310866; -0.025891487363477066 0.04334885412406512 -0.08137603587821016;;; … ;;; 0.08820164885463334 0.028330710036703143 -0.1539740503591076; 0.2496816467280994 -0.027228411645403135 0.03875648916792301; 0.17671511424494019 -0.05264993650369663 -0.2912133424001838;;; 0.0935316547308734 0.025452156628271577 0.08058292695779273; 0.10998171321612955 -0.0609225876358012 -0.07024805520373714; -0.15036351262064646 0.14004085461862736 -0.055995489735464546;;; 0.12470216156126364 0.1480303545894154 -0.10184799009086454; -0.4024383775597391 0.11611441146570052 -0.01273704264233751; 0.11232453429867413 0.09212538681480628 -0.3555415209410282;;;; 0.06719444932859049 -0.1747418652392574 -0.038926861030925186; -0.006420735575073959 0.09358058033936853 0.18257174985464297; -0.073955672634207 -0.24592341353220892 0.005759541748972648;;; -0.1718871491031107 0.09955814831619808 0.12844135153292052; -0.14261724201365822 -0.016714455190158968 -0.017156787342999007; 0.2340893698405853 -0.13210045031190087 -0.32288774684154276;;; -0.22208711956721072 0.15253562905115253 -0.18101145836867585; -0.12195936974321916 0.12184115242883149 -0.22579101294613474; -0.20958774571527494 -0.38050897016460966 0.18038526839190608;;; … ;;; 0.1678208017489352 0.009985858177159825 -0.4203004518382885; 0.016873760783765818 0.2645043473360248 -0.20876090045059376; 0.14784641910522486 0.10627677052831754 -0.17243506412780454;;; -0.09766369891865911 0.2053137049246119 0.08962458518614111; 0.3431570656435394 0.23972635256173125 -0.20250292961362718; 0.24168122805989337 -0.03092779117896808 -0.04194855951634308;;; -0.007732127473150813 -0.2839174067031438 -0.11918258283145074; -0.26174746527076365 -0.08611413055124142 -0.0723641680215178; -0.24381935849880257 -0.12531795693031003 -0.22234630154175805;;;; 0.12958473011610455 -0.0697918502075286 0.17976969074217958; -0.07525919652449865 -0.08619945319324984 0.10531425826145185; -0.004827102730643809 -0.16829633057009477 -0.015334447455632625;;; -0.023492459955384154 -0.18054959481265165 -0.1327660272120608; -0.08713645989476976 -0.1953595151889067 0.06423435426459956; -0.2162246705119183 0.30634378050911587 -0.14427573379947564;;; 0.011956017047621996 -0.22027904050121064 0.2724209407120929; 0.11280716462267051 0.03790661451673985 -0.21230480139272745; -0.21959839591059482 -0.060521480346083165 -0.16430494114163016;;; … ;;; -0.23710786736798956 -0.33794113625292443 -0.018519472424113843; -0.04922467109013613 -0.043161786224290496 0.14769393909670198; -0.3971215538306422 0.22847666376566292 -0.2084013693604472;;; 0.19613591882791342 0.02531902968513815 0.2174285784735332; 0.008101011661379278 -0.19672057914871513 0.16222570373000278; 0.04173678562275738 0.04022560463739029 -0.14295352670371425;;; 0.004432307413885612 -0.31825486407211545 0.3258624510757024; 0.1065216847884721 0.06823569310591711 0.1667377125936802; 0.02622089064751433 -0.08058743413232289 -0.17445585373857053;;;; 0.13254871239826385 0.052960875929371376 -0.015709371035759655; -0.2898734965397956 0.20630573275573805 0.39609068953959775; 0.4011485821618389 -0.05020934845362672 0.038971642183133495;;; 0.1825033452112699 0.05329556585995243 0.04202523214977476; -0.0795544863713595 0.27278919782099115 0.021354894638076383; 0.19807847281099117 -0.12563064605902816 -0.10657302261124459;;; -0.008238141775258568 -0.18241836158173166 -0.10058393530556571; 0.014483318143869703 0.08266951423397675 -0.09390567192845384; -0.09881595556380884 0.08533203864406069 -0.24275915699607375;;; … ;;; -0.008014636813111839 0.21828295034083944 -0.018612896257390658; -0.14396988636127964 -0.38125097797136764 -0.09183770962854979; -0.03236186346639129 -0.03284812761983894 -0.15578210043604718;;; 0.18072463779378134 0.10622750476120621 -0.3398843177848954; -0.0647045723067568 0.21465529630755267 -0.1980325351166533; 0.15060923952802666 -0.04616805630874294 -0.01971864673547035;;; 0.08301638639072831 -0.007303931478739066 0.05398531657098987; 0.17247823159242207 0.13385535879702132 0.01812380488832738; -0.3166001034747098 0.15355597127320303 -0.05660251860334581], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 0×0×0×0 BitArray{4}, Array{Float64, 4}(undef, 0, 0, 0, 0), [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([0.0060062245132749496 0.050811092259897286 … -0.012457539696406178 0.023483155330876904; 0.0063934503197288686 -0.020525406396868402 … -0.008682578561776103 -0.0001693818338907808; … ; 0.02376681596248403 0.011908542748846987 … -0.034382239836295556 -0.0025849500902240135; -0.007765407471358376 0.012410631198493095 … 0.005754582453782103 -0.014190606652220598], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([0.004732764631233259 0.007387091926250573 … 0.0007886741779357124 -0.013960844149015194; -0.014721285812401471 -0.035716013595320076 … 0.007239015867119303 -0.04746880150164176; … ; 0.05460459686958503 -0.00992846777591607 … 0.008303745338170571 -0.00713439407361921; -0.023032359399221995 -0.02465936043979426 … 0.009681368263665653 0.0048945846864785186], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([-0.023993277290626263 -0.07274172727414736 … 0.021761192257107015 0.07536324897079524; -0.014701582977550674 0.01733597846606652 … 0.08638374542734678 -0.01926556029944765; … ; -0.029635595363133836 -0.02460352197549515 … 0.0017076166607955718 -0.04424369313015433; -0.04337326038657082 -0.021715135670486406 … 0.007866507237736242 0.04781328821861521], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([-0.05638622690338751 -0.04816692706592552 … 0.08861839080905518 -0.08558478813048712; 0.0933141341308287 0.27460864486771336 … 0.23576675540692832 -0.05356494024418576; … ; 0.07894141195552891 -0.061045481698385466 … -0.13476027970190346 -0.049427257730928785; 0.10499234949444997 0.04175336606337115 … -0.12928414008530487 0.0332892147744267], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([-0.13255596477729042 -0.33621648357044864 … -0.2824186154602431 0.008947633607774834; 0.5223786943802379 -0.3876505218315186 … -0.4001024233993758 0.08175954978068599; … ; 0.2037203452312323 0.007503698960012201 … -0.21691182731889913 -0.026610138936843098; 0.7062910279845251 -0.09482835705966662 … 0.08840172620630217 0.5022577570710808], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictlayers = init_layers(28,28,1,n_samples=prediction_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_factor = 0.3333333333333333\n",
      "scale_factor = 0.027777777777777776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayerDefs(ConvLayer([-0.2097845397282482 -0.1797016576905717 -0.4962645312166686; 0.14534054917682807 0.4712949304749917 -0.8482939409663467; -0.18185838994589085 0.27524503192896077 -1.220711783910476;;;; 0.02527735435894241 0.25050412632748764 0.8355551033824538; -0.4765071245996046 -0.22830729850747758 -0.05373856285477063; 0.4851408050929267 0.29856565361163323 -0.734727621624394;;;; 0.5575244937022416 -0.8443440512983529 0.8731799328159261; -0.055761079028744304 0.751116268400012 0.37461316814617623; -0.79836613085538 1.3084372019358832 0.5195830894903669;;;; … ;;;; -0.8109791033023996 -0.4535991821641914 0.3216283074914613; -0.7128942044386622 -0.23561589322645335 -0.22600586882317306; -0.07234595195154428 -0.9969681447866366 -0.1447777990169911;;;; 0.5899590533462148 0.15179967190072904 0.41769253296735354; -0.0005425719554744286 -0.6126366208908363 0.49314975631309216; -0.8664735032964339 0.598377742076984 0.21258437351055676;;;; -0.026881971774367824 -0.08337724425406373 -0.1794080587739208; 0.9862206764294802 -0.9393016185408005 -0.02336732852840717; -0.5952754977567456 -0.11928813726382353 0.06048944484254383], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], 0×0×0×0 BitArray{4}, Array{Float64, 4}(undef, 0, 0, 0, 0), [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; … ;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), ConvLayer([0.15483034769293544 0.04176888507495267 0.0730787070386926; -0.19016976968468707 -0.14246155986814396 0.07876403393222496; 0.023211793278922494 0.03480553509037343 0.17951042827524166;;; -0.06262614242619882 0.08782193250157622 -0.1195754153058758; 0.15532199588157952 0.04251610143872994 -0.07863831374900798; -0.04581835378309843 -0.019998671744881936 -0.19776172002541825;;; 0.2987766980939553 -0.25315331990204115 0.16177422183610488; -0.3382582381425917 0.22642296484995367 0.1404835863318313; 0.01202539156577831 -0.08058738712150906 0.039659848927613386;;; … ;;; 0.05677752696011033 -0.0870766584540535 0.13113601122191573; -0.046889835834706156 -0.32615709603425164 0.19153416735628961; 0.14503838001379854 0.08526530008843519 0.017909636092615363;;; 0.05827010051358526 -0.39037089176321654 0.07435691010631124; 0.04629904539806376 0.00975621788208737 0.10120770595322438; -0.09366941193342998 0.20679627083568175 -0.024684026295306416;;; -0.0444476603376235 -0.17973462957571895 -0.13522712430643452; 0.07927505363142122 0.11103214228539345 0.09420034864958222; 0.10631074473096405 0.0530636539505495 -0.17149632993452169;;;; -0.22791332224093208 0.019737715487298753 0.15471941663077488; -0.05138315702220374 0.19827344039233896 -0.20812718722203782; -0.09263347060636395 -0.10958644088043432 -0.030924112687897357;;; 0.05379644106786424 -0.13909319949673793 -0.16426966791036707; 0.30911469312398165 0.11217130325200914 -0.07701604015479208; 0.27212633042721235 0.03571830276254776 -0.07051379643202058;;; 0.05154699972956689 -0.3190196400685818 0.24651874539506327; 0.14841077001295694 0.17454425202799784 -0.3983390177451079; -0.012864467041378051 0.1771467565022412 0.06449880652099974;;; … ;;; 0.14639413855377678 -0.02902065813239736 0.17939307039579336; -0.04148050121292088 0.20113269565687925 0.20662579333252754; 0.12048327707112134 0.2643244368996107 0.028782671422503868;;; 0.21995170377125112 -0.08322269964117006 0.10245129430445173; 0.05362510040428839 -0.4689893242683326 0.08137199503185132; -0.027735028512991172 -0.37347696651034573 0.11087321380942763;;; 0.245805943013135 0.14040246986506127 0.08066732688146197; -0.020227895950992735 -0.01802523635660943 -0.09099886699659546; -0.08778928638542571 -0.07703784047450726 -0.001436780559783238;;;; 0.1859567657237501 -0.18558385470736422 0.1655263849837644; 0.06178471197381297 -0.08130780672940047 0.3024145494898448; -0.008331783905850794 -0.2891411192833006 0.07063136266982592;;; -0.006125835140059333 0.12801393275661202 0.053785484525712864; -0.0393080586274766 -0.36115608046252845 -0.00573394433908923; -0.11217569444051018 0.38374733230612534 0.08880421533332283;;; -0.10401376663814627 0.24056626910735246 -0.058957280850130994; 0.20004012663106513 0.10670486708102586 -0.2687663204995444; 0.16427176054582462 -0.27358723084127023 0.02670517039896552;;; … ;;; -0.2831072676536527 0.11869974885430815 0.2043345424090394; 0.14018347236893752 -0.05399696715898117 0.3390677678455085; -0.2534438688768112 0.04943131071459071 0.11331874569603946;;; 0.07217792352275099 -0.10837380185012471 -0.3421709631679508; -0.14949089310718128 0.021845017442986326 0.03435320896712357; -0.05994329191962608 -0.15538583271601833 0.021353870544255266;;; 0.17697330471720585 -0.06592549722309288 0.17112479084448695; 0.06645247570730362 0.15812365661860733 0.07129523281566402; 0.23414535773125125 -0.06476685105715714 -0.1989182806235793;;;; -0.16891530133089386 0.10802781850249156 -0.051297841167282955; -0.1093604587462318 0.16111334035138689 0.02007683120285366; -0.05717787676193224 -0.0012397183217072707 -0.01908527022866643;;; 0.09619929819781597 0.021133332105632847 -0.1169160758084047; -0.009679176626285748 -0.22571635643036564 -0.20150580027259424; -0.03505153253935031 0.12490368864645146 -0.01667996382584495;;; 0.027364088271910794 0.04820760655627574 -0.11710069535625768; -0.006851460143053816 0.004866805192486506 -0.1769007586924124; 0.03635440327965672 -0.016250983865141188 0.03752216177949337;;; … ;;; -0.12486679881370177 0.20475779820397477 0.1411314687388549; -0.27235630526512633 -0.020804410281009125 0.1116726835846639; 0.23081337919798806 0.11052914040544626 -0.097828938150432;;; -0.07797781490227619 0.06428428868599424 0.21607652562173096; 0.12179359237444905 -0.007628026876322748 -0.014850113859266538; 0.10410791825551081 -0.10734425207262839 0.031127562949766756;;; 0.3298127963977019 -0.0681087821847407 -0.0171027715185015; -0.288762012553042 0.03378383978917701 0.42795747776920573; -0.27701188969447715 -0.1894592788412396 -0.05514876344216006;;;; -0.13082722384061388 0.0963065036272819 0.055512910922440574; -0.07622861157114579 -0.021988890393486966 0.11517211410589528; 0.008267659253299543 0.07561551654881948 -0.2751197867905727;;; 0.21344828419538964 -0.15998699250380988 0.040005319392447655; -0.22379461766889985 0.05062348135199936 -0.06623386558803039; -0.08323836032395132 -0.32124598250596925 0.32610741269273713;;; 0.04954530617985568 -0.0035876953635499698 -0.11395208637116933; -0.17014069841922544 -0.04913558872542148 -0.160361393310866; -0.025891487363477066 0.04334885412406512 -0.08137603587821016;;; … ;;; 0.08820164885463334 0.028330710036703143 -0.1539740503591076; 0.2496816467280994 -0.027228411645403135 0.03875648916792301; 0.17671511424494019 -0.05264993650369663 -0.2912133424001838;;; 0.0935316547308734 0.025452156628271577 0.08058292695779273; 0.10998171321612955 -0.0609225876358012 -0.07024805520373714; -0.15036351262064646 0.14004085461862736 -0.055995489735464546;;; 0.12470216156126364 0.1480303545894154 -0.10184799009086454; -0.4024383775597391 0.11611441146570052 -0.01273704264233751; 0.11232453429867413 0.09212538681480628 -0.3555415209410282;;;; 0.06719444932859049 -0.1747418652392574 -0.038926861030925186; -0.006420735575073959 0.09358058033936853 0.18257174985464297; -0.073955672634207 -0.24592341353220892 0.005759541748972648;;; -0.1718871491031107 0.09955814831619808 0.12844135153292052; -0.14261724201365822 -0.016714455190158968 -0.017156787342999007; 0.2340893698405853 -0.13210045031190087 -0.32288774684154276;;; -0.22208711956721072 0.15253562905115253 -0.18101145836867585; -0.12195936974321916 0.12184115242883149 -0.22579101294613474; -0.20958774571527494 -0.38050897016460966 0.18038526839190608;;; … ;;; 0.1678208017489352 0.009985858177159825 -0.4203004518382885; 0.016873760783765818 0.2645043473360248 -0.20876090045059376; 0.14784641910522486 0.10627677052831754 -0.17243506412780454;;; -0.09766369891865911 0.2053137049246119 0.08962458518614111; 0.3431570656435394 0.23972635256173125 -0.20250292961362718; 0.24168122805989337 -0.03092779117896808 -0.04194855951634308;;; -0.007732127473150813 -0.2839174067031438 -0.11918258283145074; -0.26174746527076365 -0.08611413055124142 -0.0723641680215178; -0.24381935849880257 -0.12531795693031003 -0.22234630154175805;;;; 0.12958473011610455 -0.0697918502075286 0.17976969074217958; -0.07525919652449865 -0.08619945319324984 0.10531425826145185; -0.004827102730643809 -0.16829633057009477 -0.015334447455632625;;; -0.023492459955384154 -0.18054959481265165 -0.1327660272120608; -0.08713645989476976 -0.1953595151889067 0.06423435426459956; -0.2162246705119183 0.30634378050911587 -0.14427573379947564;;; 0.011956017047621996 -0.22027904050121064 0.2724209407120929; 0.11280716462267051 0.03790661451673985 -0.21230480139272745; -0.21959839591059482 -0.060521480346083165 -0.16430494114163016;;; … ;;; -0.23710786736798956 -0.33794113625292443 -0.018519472424113843; -0.04922467109013613 -0.043161786224290496 0.14769393909670198; -0.3971215538306422 0.22847666376566292 -0.2084013693604472;;; 0.19613591882791342 0.02531902968513815 0.2174285784735332; 0.008101011661379278 -0.19672057914871513 0.16222570373000278; 0.04173678562275738 0.04022560463739029 -0.14295352670371425;;; 0.004432307413885612 -0.31825486407211545 0.3258624510757024; 0.1065216847884721 0.06823569310591711 0.1667377125936802; 0.02622089064751433 -0.08058743413232289 -0.17445585373857053;;;; 0.13254871239826385 0.052960875929371376 -0.015709371035759655; -0.2898734965397956 0.20630573275573805 0.39609068953959775; 0.4011485821618389 -0.05020934845362672 0.038971642183133495;;; 0.1825033452112699 0.05329556585995243 0.04202523214977476; -0.0795544863713595 0.27278919782099115 0.021354894638076383; 0.19807847281099117 -0.12563064605902816 -0.10657302261124459;;; -0.008238141775258568 -0.18241836158173166 -0.10058393530556571; 0.014483318143869703 0.08266951423397675 -0.09390567192845384; -0.09881595556380884 0.08533203864406069 -0.24275915699607375;;; … ;;; -0.008014636813111839 0.21828295034083944 -0.018612896257390658; -0.14396988636127964 -0.38125097797136764 -0.09183770962854979; -0.03236186346639129 -0.03284812761983894 -0.15578210043604718;;; 0.18072463779378134 0.10622750476120621 -0.3398843177848954; -0.0647045723067568 0.21465529630755267 -0.1980325351166533; 0.15060923952802666 -0.04616805630874294 -0.01971864673547035;;; 0.08301638639072831 -0.007303931478739066 0.05398531657098987; 0.17247823159242207 0.13385535879702132 0.01812380488832738; -0.3166001034747098 0.15355597127320303 -0.05660251860334581], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 0×0×0×0 BitArray{4}, Array{Float64, 4}(undef, 0, 0, 0, 0), [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; … ;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0;;; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([0.0060062245132749496 0.050811092259897286 … -0.012457539696406178 0.023483155330876904; 0.0063934503197288686 -0.020525406396868402 … -0.008682578561776103 -0.0001693818338907808; … ; 0.02376681596248403 0.011908542748846987 … -0.034382239836295556 -0.0025849500902240135; -0.007765407471358376 0.012410631198493095 … 0.005754582453782103 -0.014190606652220598], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([0.004732764631233259 0.007387091926250573 … 0.0007886741779357124 -0.013960844149015194; -0.014721285812401471 -0.035716013595320076 … 0.007239015867119303 -0.04746880150164176; … ; 0.05460459686958503 -0.00992846777591607 … 0.008303745338170571 -0.00713439407361921; -0.023032359399221995 -0.02465936043979426 … 0.009681368263665653 0.0048945846864785186], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([-0.023993277290626263 -0.07274172727414736 … 0.021761192257107015 0.07536324897079524; -0.014701582977550674 0.01733597846606652 … 0.08638374542734678 -0.01926556029944765; … ; -0.029635595363133836 -0.02460352197549515 … 0.0017076166607955718 -0.04424369313015433; -0.04337326038657082 -0.021715135670486406 … 0.007866507237736242 0.04781328821861521], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([-0.05638622690338751 -0.04816692706592552 … 0.08861839080905518 -0.08558478813048712; 0.0933141341308287 0.27460864486771336 … 0.23576675540692832 -0.05356494024418576; … ; 0.07894141195552891 -0.061045481698385466 … -0.13476027970190346 -0.049427257730928785; 0.10499234949444997 0.04175336606337115 … -0.12928414008530487 0.0332892147744267], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), LinearLayer([-0.13255596477729042 -0.33621648357044864 … -0.2824186154602431 0.008947633607774834; 0.5223786943802379 -0.3876505218315186 … -0.4001024233993758 0.08175954978068599; … ; 0.2037203452312323 0.007503698960012201 … -0.21691182731889913 -0.026610138936843098; 0.7062910279845251 -0.09482835705966662 … 0.08840172620630217 0.5022577570710808], [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0×0 BitMatrix, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainlayers = init_layers(28,28,1, n_samples=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_num = 200\n",
      "n_samples = 50\n",
      "\n",
      "epoch 1 batch 1 Loss = 3.504168521122907 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 1 Loss_pred = 3.504168521122907 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 2 Loss = 4.22961903863323 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 2 Loss_pred = 4.046491112589327 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 3 Loss = 3.986063666091701 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 3 Loss_pred = 4.1280159097009435 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 4 Loss = 3.8102778265134916 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 4 Loss_pred = 3.9478291347461547 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 5 Loss = 3.4896380175980246 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 5 Loss_pred = 4.309692560057474 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 6 Loss = 3.6210272645378674 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 6 Loss_pred = 4.599528813379821 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 7 Loss = 3.472384086957492 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 7 Loss_pred = 4.28268807711907 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 8 Loss = 3.6101184875120818 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 8 Loss_pred = 4.192096749116453 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 9 Loss = 3.477422417222705 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 9 Loss_pred = 3.833663201706479 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 10 Loss = 3.5043724370418134 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 10 Loss_pred = 4.293839707537203 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 11 Loss = 3.400735805157422 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 11 Loss_pred = 3.9461721814764417 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 12 Loss = 3.4980004481291394 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 12 Loss_pred = 4.4820148653659935 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 13 Loss = 3.323386192558909 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 13 Loss_pred = 4.609445891638891 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 14 Loss = 3.357020562238929 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 14 Loss_pred = 3.9382110622799953 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 15 Loss = 3.3200453280636895 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 15 Loss_pred = 4.166428725103151 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 16 Loss = 3.4367626581624164 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 16 Loss_pred = 4.168979513965373 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 17 Loss = 3.5065356617104393 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 17 Loss_pred = 4.381620839990899 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 18 Loss = 3.282725248998613 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 18 Loss_pred = 4.02243610077953 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 19 Loss = 3.308114481381273 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 19 Loss_pred = 3.799196351422771 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 20 Loss = 3.3969840962287647 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 20 Loss_pred = 4.085349428376088 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 21 Loss = 3.349570341374722 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 21 Loss_pred = 4.307405077062614 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 22 Loss = 3.694766462716291 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 22 Loss_pred = 4.177196711377296 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 23 Loss = 3.3123922238768433 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 23 Loss_pred = 4.523870270766974 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 24 Loss = 3.4239290797862725 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 24 Loss_pred = 3.770365356386274 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 25 Loss = 3.4608095310678575 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 25 Loss_pred = 4.251854875838573 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 26 Loss = 3.375509118826727 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 26 Loss_pred = 4.863578191686671 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 27 Loss = 3.44392042648838 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 27 Loss_pred = 3.841813434843183 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 28 Loss = 3.4512316167356474 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 28 Loss_pred = 4.466934910961444 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 29 Loss = 3.3971266624873704 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 29 Loss_pred = 3.947402094940372 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 30 Loss = 3.3335591891296708 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 30 Loss_pred = 4.703737018135165 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 31 Loss = 3.392131773445753 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 31 Loss_pred = 3.9450811686286142 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 32 Loss = 3.3857004383531093 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 32 Loss_pred = 4.198041527496768 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 33 Loss = 3.284182662248817 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 33 Loss_pred = 3.646648225374935 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 34 Loss = 3.2712869059450056 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 34 Loss_pred = 4.16185592225547 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 35 Loss = 3.356187557021265 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 35 Loss_pred = 4.24205648696188 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 36 Loss = 3.3694477095018467 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 36 Loss_pred = 4.399895345458666 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 37 Loss = 3.255965479471842 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 37 Loss_pred = 3.826290361454651 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 38 Loss = 3.27237642037376 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 38 Loss_pred = 3.9722305710547556 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 39 Loss = 3.3680673627900957 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 39 Loss_pred = 3.759160057538671 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 40 Loss = 3.285335889664827 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 40 Loss_pred = 3.814889891299091 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 41 Loss = 3.248982080318318 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 41 Loss_pred = 4.092984203221277 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 42 Loss = 3.293387902552583 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 42 Loss_pred = 4.066608787134901 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 43 Loss = 3.304967768464069 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 43 Loss_pred = 4.286368637329196 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 44 Loss = 3.25635974559744 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 44 Loss_pred = 4.063073566412825 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 45 Loss = 3.4483976559639844 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 45 Loss_pred = 3.979984126984878 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 46 Loss = 3.210000034884786 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 46 Loss_pred = 4.216254701346675 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 47 Loss = 3.3981165659722 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 47 Loss_pred = 3.9395877486842488 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 48 Loss = 3.379320722132309 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 48 Loss_pred = 3.6456483308103014 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 49 Loss = 3.2570083488394337 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 49 Loss_pred = 3.8698913192021234 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 50 Loss = 3.2918958952703954 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 50 Loss_pred = 4.410410008744144 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 51 Loss = 3.3087098056610404 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 51 Loss_pred = 3.8404209751792404 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 52 Loss = 3.3073207847384114 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 52 Loss_pred = 4.089099340640342 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 53 Loss = 3.2977576928736068 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 53 Loss_pred = 4.262353267194254 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 54 Loss = 3.4675226415126374 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 54 Loss_pred = 4.4098033621056905 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 55 Loss = 3.2766914553917945 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 55 Loss_pred = 4.032389088561104 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 56 Loss = 3.477372094820381 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 56 Loss_pred = 4.106496431223305 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 57 Loss = 3.247919997186264 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 57 Loss_pred = 4.004014786033129 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 58 Loss = 3.1890864976229487 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 58 Loss_pred = 3.962216848393919 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 59 Loss = 3.3714832006818938 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 59 Loss_pred = 4.081783759676669 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 60 Loss = 3.3170330180352914 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 60 Loss_pred = 4.18479465450683 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 61 Loss = 3.3375607067018236 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 61 Loss_pred = 4.286509229152268 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 62 Loss = 3.259804201372221 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 62 Loss_pred = 4.276049858124327 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 63 Loss = 3.2656679983007684 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 63 Loss_pred = 4.053033025855573 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 64 Loss = 3.317628214392647 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 64 Loss_pred = 3.8488998033570785 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 65 Loss = 3.268495951127211 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 65 Loss_pred = 4.437190789024752 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 66 Loss = 3.2392396104891326 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 66 Loss_pred = 3.81071638042132 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 67 Loss = 3.338418181300519 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 67 Loss_pred = 3.9989953264839477 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 68 Loss = 3.3634125815918834 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 68 Loss_pred = 3.9878238213226247 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 69 Loss = 3.352015419973625 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 69 Loss_pred = 3.996642224260847 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 70 Loss = 3.3604714593719907 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 70 Loss_pred = 4.372605834790838 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 71 Loss = 3.2883879296297422 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 71 Loss_pred = 4.162082659126773 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 72 Loss = 3.2902721669922905 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 72 Loss_pred = 3.8620801653605463 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 73 Loss = 3.2337544610329476 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 73 Loss_pred = 4.512971371356834 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 74 Loss = 3.246163321941387 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 74 Loss_pred = 4.347743572412311 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 75 Loss = 3.365670982836296 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 75 Loss_pred = 4.008896183607684 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 76 Loss = 3.3345955253287993 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 76 Loss_pred = 4.345399248344791 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 77 Loss = 3.3689336228176674 Accuracy = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 77 Loss_pred = 4.0596269904815845 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 78 Loss = 3.2564720566282643 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 78 Loss_pred = 3.88914628385266 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 79 Loss = 3.348890856240672 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 79 Loss_pred = 4.163629926177731 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 80 Loss = 3.3519195624883658 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 80 Loss_pred = 3.905942284594278 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 81 Loss = 3.2826256631453354 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 81 Loss_pred = 3.7632057762657802 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 82 Loss = 3.2777355976266955 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 82 Loss_pred = 4.19490325063816 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 83 Loss = 3.3003989004921306 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 83 Loss_pred = 3.785766415726178 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 84 Loss = 3.23809386812743 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 84 Loss_pred = 3.763420999144092 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 85 Loss = 3.3174881816045856 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 85 Loss_pred = 3.8327216125205137 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 86 Loss = 3.2069823402141955 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 86 Loss_pred = 4.101195480304022 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 87 Loss = 3.302189571257208 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 87 Loss_pred = 3.968900725079417 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 88 Loss = 3.380761879866501 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 88 Loss_pred = 4.146790185185917 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 89 Loss = 3.32733689579179 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 89 Loss_pred = 3.688602753421816 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 90 Loss = 3.3383685256648046 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 90 Loss_pred = 4.267973861504304 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 91 Loss = 3.164336662875263 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 91 Loss_pred = 4.049389677880829 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 92 Loss = 3.310973537890476 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 92 Loss_pred = 3.904822124231807 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 93 Loss = 3.332243013523312 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 93 Loss_pred = 3.757337427547993 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 94 Loss = 3.385346855609218 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 94 Loss_pred = 3.847341043011715 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 95 Loss = 3.2842043525551343 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 95 Loss_pred = 4.061604254223757 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 96 Loss = 3.209648748294253 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 96 Loss_pred = 3.8390941934088123 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 97 Loss = 3.205476495243512 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 97 Loss_pred = 4.232933858039122 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 98 Loss = 3.3050970851175836 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 98 Loss_pred = 3.577461889662658 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 99 Loss = 3.272541060193679 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 99 Loss_pred = 4.018597424423635 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 100 Loss = 3.2221896774153875 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 100 Loss_pred = 4.277730971701359 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 101 Loss = 3.346497950892027 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 101 Loss_pred = 4.172284173581349 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 102 Loss = 3.25184189042364 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 102 Loss_pred = 3.8969725716106405 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 103 Loss = 3.255058289664479 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 103 Loss_pred = 4.078241185381491 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 104 Loss = 3.2912535207501294 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 104 Loss_pred = 4.445969584729711 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 105 Loss = 3.3577976587687655 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 105 Loss_pred = 3.8662624669685126 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 106 Loss = 3.3384310461422126 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 106 Loss_pred = 4.170902108383123 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 107 Loss = 3.3088199641250915 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 107 Loss_pred = 4.160560358448067 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 108 Loss = 3.2901753394463427 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 108 Loss_pred = 4.061746250754455 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 109 Loss = 3.23930708635611 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 109 Loss_pred = 4.007049547435455 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 110 Loss = 3.2193359377717212 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 110 Loss_pred = 4.283596623724595 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 111 Loss = 3.305777148973813 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 111 Loss_pred = 3.860700810352057 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 112 Loss = 3.227106526148895 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 112 Loss_pred = 4.04958954313533 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 113 Loss = 3.381578939313239 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 113 Loss_pred = 3.880034268539468 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 114 Loss = 3.237252201381573 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 114 Loss_pred = 3.9644067800444813 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 115 Loss = 3.282045500441548 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 115 Loss_pred = 4.345674382219829 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 116 Loss = 3.3773684473344447 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 116 Loss_pred = 3.652692905044253 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 117 Loss = 3.2267356245833754 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 117 Loss_pred = 3.462823595800771 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 118 Loss = 3.3198407493846536 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 118 Loss_pred = 4.139688286065873 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 119 Loss = 3.2956702430128986 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 119 Loss_pred = 3.996807541944617 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 120 Loss = 3.2695925924453872 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 120 Loss_pred = 4.076029188372797 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 121 Loss = 3.2157445015907156 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 121 Loss_pred = 4.310857295212255 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 122 Loss = 3.2540568358942523 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 122 Loss_pred = 4.0999526166653855 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 123 Loss = 3.2428015190697175 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 123 Loss_pred = 4.058834603031591 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 124 Loss = 3.2417448007946703 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 124 Loss_pred = 4.020783009275914 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 125 Loss = 3.3770078054206203 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 125 Loss_pred = 4.071795010502923 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 126 Loss = 3.241037374364642 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 126 Loss_pred = 4.201029060059622 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 127 Loss = 3.263029915595911 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 127 Loss_pred = 4.210753327224094 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 128 Loss = 3.215394976017415 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 128 Loss_pred = 3.9134272944464006 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 129 Loss = 3.2383106243230815 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 129 Loss_pred = 4.2345630811801485 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 130 Loss = 3.2834959626969273 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 130 Loss_pred = 3.8902991750982348 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 131 Loss = 3.149730649562029 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 131 Loss_pred = 4.182184640973505 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 132 Loss = 3.276362599130704 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 132 Loss_pred = 3.783162321340873 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 133 Loss = 3.230227217116935 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 133 Loss_pred = 3.452497083705225 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 134 Loss = 3.2453529691466914 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 134 Loss_pred = 4.232372093877778 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 135 Loss = 3.2406508456033056 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 135 Loss_pred = 4.070207813985288 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 136 Loss = 3.3039429844263997 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 136 Loss_pred = 4.230103725571088 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 137 Loss = 3.2488719612163424 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 137 Loss_pred = 4.014224550673336 Accuracy_pred = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 138 Loss = 3.305420562968584 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 138 Loss_pred = 3.7067676541964447 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 139 Loss = 3.2862395689439463 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 139 Loss_pred = 4.126975091148039 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 140 Loss = 3.2697296027882636 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 140 Loss_pred = 4.118228412848711 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 141 Loss = 3.248700883063784 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 141 Loss_pred = 3.969300920594881 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 142 Loss = 3.2793640470488263 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 142 Loss_pred = 3.7900323522981982 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 143 Loss = 3.2773319920558484 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 143 Loss_pred = 3.5539556249477755 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 144 Loss = 3.2313604289715965 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 1 batch 144 Loss_pred = 4.031818138291133 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 145 Loss = 3.297104228216925 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 145 Loss_pred = 4.364051261795457 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 146 Loss = 3.315690653803755 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 146 Loss_pred = 3.757642429058478 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 147 Loss = 3.254005954598039 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 147 Loss_pred = 4.372944713932596 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 148 Loss = 3.2729411303194 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 148 Loss_pred = 3.8305844700921643 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 149 Loss = 3.246424625126087 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 149 Loss_pred = 3.672219107853497 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 150 Loss = 3.277404673409076 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 150 Loss_pred = 4.016774432350933 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 151 Loss = 3.2512249147119237 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 151 Loss_pred = 4.226923197842568 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 152 Loss = 3.298442037562909 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 152 Loss_pred = 3.971174209077878 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 153 Loss = 3.267551821363119 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 153 Loss_pred = 3.8890584193319295 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 154 Loss = 3.2287522099388593 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 154 Loss_pred = 4.220102026349491 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 155 Loss = 3.3091485491102994 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 155 Loss_pred = 3.899515086656311 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 156 Loss = 3.2587470233741587 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 156 Loss_pred = 4.048390099402865 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 157 Loss = 3.2742601042362707 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 157 Loss_pred = 4.459489342014061 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 158 Loss = 3.2531003510866636 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 158 Loss_pred = 3.7487309154621364 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 159 Loss = 3.244222843666559 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 159 Loss_pred = 4.18845238100106 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 160 Loss = 3.273932489409446 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 160 Loss_pred = 4.183783271552464 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 161 Loss = 3.2366466200571558 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 161 Loss_pred = 4.053829510220841 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 162 Loss = 3.279364821491151 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 162 Loss_pred = 4.095561731727434 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 163 Loss = 3.2487002047816564 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 163 Loss_pred = 4.416750431600145 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 164 Loss = 3.245071780000431 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 164 Loss_pred = 3.9261101264032527 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 165 Loss = 3.3139734956803455 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 165 Loss_pred = 4.1388369950272095 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 166 Loss = 3.290145707216965 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 166 Loss_pred = 4.1691111819724265 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 167 Loss = 3.4033977339016976 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 167 Loss_pred = 3.983588932130824 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 168 Loss = 3.2256962735167702 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 168 Loss_pred = 4.0888941228147795 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 169 Loss = 3.2720446489982056 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 169 Loss_pred = 3.8758077739046013 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 170 Loss = 3.2587676448215106 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 170 Loss_pred = 3.9214677768704598 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 171 Loss = 3.287555407645374 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 171 Loss_pred = 3.571965348961528 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 172 Loss = 3.271525016535697 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 172 Loss_pred = 4.087116565898177 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 173 Loss = 3.2885925572426653 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 173 Loss_pred = 4.107331637681048 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 174 Loss = 3.2517182330527064 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 174 Loss_pred = 4.201388320281657 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 175 Loss = 3.2784439424808216 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 175 Loss_pred = 3.8918662329307923 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 176 Loss = 3.2542254068145353 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 176 Loss_pred = 4.15267661914938 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 177 Loss = 3.230158288823319 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 177 Loss_pred = 3.9160597907068793 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 178 Loss = 3.2594004566727723 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 178 Loss_pred = 4.103994315773599 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 179 Loss = 3.2957488196922275 Accuracy = 0.02\n",
      "\n",
      "\n",
      "epoch 1 batch 179 Loss_pred = 4.150848098416409 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 180 Loss = 3.263951807723535 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 180 Loss_pred = 4.363884449613371 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 181 Loss = 3.2793405949298897 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 181 Loss_pred = 4.230867293132373 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 182 Loss = 3.2546927694927508 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 182 Loss_pred = 3.661582368416984 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 183 Loss = 3.2476808266159565 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 183 Loss_pred = 4.636920608767026 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 184 Loss = 3.2274879445437614 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 184 Loss_pred = 3.9961099295260962 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 185 Loss = 3.2658291919750564 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 185 Loss_pred = 4.317982775877162 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 186 Loss = 3.2249472623131146 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 186 Loss_pred = 4.253423498042792 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 1 batch 187 Loss = 3.2998185261149007 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 187 Loss_pred = 4.151158355548649 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 188 Loss = 3.2754014259725204 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 188 Loss_pred = 4.097096077192893 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 189 Loss = 3.2513376922010844 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 189 Loss_pred = 3.5252674249595897 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 190 Loss = 3.267404223605906 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 190 Loss_pred = 4.486594960249 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 1 batch 191 Loss = 3.23745327474172 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 191 Loss_pred = 4.299749082198822 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 192 Loss = 3.2761431851895053 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 192 Loss_pred = 4.017830357685299 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 193 Loss = 3.2793348846109214 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 193 Loss_pred = 3.878552842993149 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 194 Loss = 3.3139888845262577 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 194 Loss_pred = 3.6167122952506134 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 195 Loss = 3.200363499819583 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 195 Loss_pred = 4.403062461793953 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 1 batch 196 Loss = 3.246858525115697 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 1 batch 196 Loss_pred = 3.8031801905358296 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 197 Loss = 3.292549609838766 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 1 batch 197 Loss_pred = 3.796576184479788 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 198 Loss = 3.198639350290415 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 1 batch 198 Loss_pred = 4.224241152279031 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 199 Loss = 3.2133523537566355 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 1 batch 199 Loss_pred = 4.165381779807373 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 1 batch 200 Loss = 3.2297716605207145 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 1 batch 200 Loss_pred = 4.048140513599789 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "\n",
      "epoch 1 Loss = 3.2588720091511414 Accuracy = 0.1\n",
      "\n",
      "\n",
      "\n",
      "epoch 2 batch 1 Loss = 3.247781825298099 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 1 Loss_pred = 3.247781825298099 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 2 Loss = 3.252430750517535 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 2 Loss_pred = 3.268492444943358 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 3 Loss = 3.1822035527845878 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 3 Loss_pred = 3.203180033816555 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 4 Loss = 3.1680059943686247 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 4 Loss_pred = 3.1817859242356725 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 5 Loss = 3.197731037597786 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 5 Loss_pred = 3.2085755501123647 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 6 Loss = 3.2089333696113784 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 6 Loss_pred = 3.1905042244858124 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 7 Loss = 3.196500287474458 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 7 Loss_pred = 3.229329397546946 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 8 Loss = 3.2318924513254856 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 8 Loss_pred = 3.2937600266984193 Accuracy_pred = 0.04\n",
      "\n",
      "\n",
      "epoch 2 batch 9 Loss = 3.173328382004496 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 9 Loss_pred = 3.160835750042508 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 10 Loss = 3.1799691352321253 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 10 Loss_pred = 3.1891318034953304 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 11 Loss = 3.256422069485032 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 11 Loss_pred = 3.278869407188224 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 12 Loss = 3.2171286136857953 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 12 Loss_pred = 3.204372795980634 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 13 Loss = 3.1990475604157225 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 13 Loss_pred = 3.2218865037630664 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 14 Loss = 3.1459383917382535 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 2 batch 14 Loss_pred = 3.1961637938636853 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 15 Loss = 3.1564687947095353 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 15 Loss_pred = 3.218576622263703 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 16 Loss = 3.2946866152895073 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 16 Loss_pred = 3.2736120538180313 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 17 Loss = 3.2184806348112387 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 17 Loss_pred = 3.2314344161263806 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 18 Loss = 3.160435610436715 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 18 Loss_pred = 3.2405355831773175 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 19 Loss = 3.142274942369681 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 19 Loss_pred = 3.2306539142183097 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 20 Loss = 3.1404167315369684 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 20 Loss_pred = 3.2063280718216043 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 21 Loss = 3.1341497665666296 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 21 Loss_pred = 3.198135230472818 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 22 Loss = 3.1859318626299173 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 22 Loss_pred = 3.1778990181638322 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 23 Loss = 3.1740683962667635 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 23 Loss_pred = 3.236277151238868 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 2 batch 24 Loss = 3.1571864597901436 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 24 Loss_pred = 3.201055600912628 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 25 Loss = 3.1854574416066455 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 25 Loss_pred = 3.20372551058649 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 26 Loss = 3.1944314300171834 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 26 Loss_pred = 3.2334814243368566 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 27 Loss = 3.1669627115819483 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 27 Loss_pred = 3.2301021194217547 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 28 Loss = 3.1747423555222873 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 28 Loss_pred = 3.1843735639297006 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 29 Loss = 3.28678445232835 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 29 Loss_pred = 3.254447299560609 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 2 batch 30 Loss = 3.127368231091667 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 30 Loss_pred = 3.22783588549107 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 31 Loss = 3.1758599245287353 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 31 Loss_pred = 3.2284978779409403 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 32 Loss = 3.192584119029107 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 32 Loss_pred = 3.208313383474473 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 33 Loss = 3.1456727921823098 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 2 batch 33 Loss_pred = 3.2222041560373955 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 34 Loss = 3.150461849970337 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 34 Loss_pred = 3.2119689230245547 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 35 Loss = 3.1229356657354153 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 35 Loss_pred = 3.2222536880067674 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 36 Loss = 3.157984301076091 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 36 Loss_pred = 3.202555378900179 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 37 Loss = 3.1787927077326223 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 37 Loss_pred = 3.2033325697008674 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 38 Loss = 3.1354214682549846 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 38 Loss_pred = 3.1691026243657916 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 39 Loss = 3.1740783210097887 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 39 Loss_pred = 3.2497725117470284 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 40 Loss = 3.0998999208999827 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 40 Loss_pred = 3.2153918995437882 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 41 Loss = 3.148620664975208 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 41 Loss_pred = 3.1760405543119186 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 42 Loss = 3.1014031499160666 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 42 Loss_pred = 3.2106250400691985 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 43 Loss = 3.173659014377538 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 43 Loss_pred = 3.196087984064935 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 44 Loss = 3.142081311224148 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 44 Loss_pred = 3.2116591854205763 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 45 Loss = 3.213737251741899 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 45 Loss_pred = 3.2206857844155525 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 46 Loss = 3.142335868209083 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 46 Loss_pred = 3.214613190943179 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 47 Loss = 3.165602959131742 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 47 Loss_pred = 3.20016844218241 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 48 Loss = 3.2389703272231563 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 48 Loss_pred = 3.27417708094593 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 49 Loss = 3.171155367247122 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 49 Loss_pred = 3.200693066198959 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 50 Loss = 3.1632952392208353 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 50 Loss_pred = 3.225914091909284 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 51 Loss = 3.1618613459732865 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 51 Loss_pred = 3.2825596889969115 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 2 batch 52 Loss = 3.1423805627616903 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 52 Loss_pred = 3.2117508005409707 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 53 Loss = 3.158650200238455 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 53 Loss_pred = 3.2408540213311836 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 54 Loss = 3.2432626567136 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 54 Loss_pred = 3.1882875706341305 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 55 Loss = 3.1162071445656783 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 55 Loss_pred = 3.1939271374928593 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 56 Loss = 3.21741342435774 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 56 Loss_pred = 3.2166882923459843 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 57 Loss = 3.0405182810838913 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 57 Loss_pred = 3.2039106497758145 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 58 Loss = 3.092932131184516 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 58 Loss_pred = 3.1924348070533104 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 59 Loss = 3.216988211812343 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 59 Loss_pred = 3.2161862519973883 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 60 Loss = 3.1149270289771485 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 60 Loss_pred = 3.2258882349603746 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 61 Loss = 3.1297296769535765 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 61 Loss_pred = 3.232826858170301 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 62 Loss = 3.1340859938920036 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 2 batch 62 Loss_pred = 3.131948588202684 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 63 Loss = 3.197774902611934 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 63 Loss_pred = 3.249546651790022 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 64 Loss = 3.186309978296056 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 64 Loss_pred = 3.2614340980466667 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 65 Loss = 3.114800697521132 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 65 Loss_pred = 3.174977386314162 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 66 Loss = 3.0806316664934266 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 66 Loss_pred = 3.177305791688899 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 67 Loss = 3.1247480384798125 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 67 Loss_pred = 3.214091536160927 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 68 Loss = 3.1253939056134965 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 68 Loss_pred = 3.2159843623443414 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 69 Loss = 3.2227503237630426 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 69 Loss_pred = 3.2215470451467336 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 70 Loss = 3.218140913709201 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 70 Loss_pred = 3.239433472920983 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 71 Loss = 3.144309168043457 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 71 Loss_pred = 3.218174821408319 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 72 Loss = 3.1536311409115125 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 72 Loss_pred = 3.2310082126176143 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 73 Loss = 3.087186657990747 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 73 Loss_pred = 3.2123464879321775 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 74 Loss = 3.0970175970585614 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 74 Loss_pred = 3.254081341325642 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 75 Loss = 3.213875085235309 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 75 Loss_pred = 3.2379606285135845 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 76 Loss = 3.1992462677083826 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 76 Loss_pred = 3.225201008017401 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 77 Loss = 3.186591838577145 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 77 Loss_pred = 3.255208923005193 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 78 Loss = 3.0891673089653398 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 78 Loss_pred = 3.213675462131241 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 79 Loss = 3.22070553215953 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 79 Loss_pred = 3.2294801051945496 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 80 Loss = 3.195518969226704 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 80 Loss_pred = 3.16310730570887 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 81 Loss = 3.1454738191295606 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 81 Loss_pred = 3.239841796395816 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 82 Loss = 3.1516770351540444 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 82 Loss_pred = 3.194242887993817 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 83 Loss = 3.1020038369792786 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 83 Loss_pred = 3.2110453044887164 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 84 Loss = 3.129260655806771 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 84 Loss_pred = 3.222588844353321 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 85 Loss = 3.112816492615807 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 85 Loss_pred = 3.2186244682164564 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 86 Loss = 3.079368182772402 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 86 Loss_pred = 3.1288013905139542 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 87 Loss = 3.137163945654263 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 87 Loss_pred = 3.2313580006336835 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 2 batch 88 Loss = 3.238574308776981 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 88 Loss_pred = 3.2281288474699235 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 89 Loss = 3.194399595490448 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 89 Loss_pred = 3.2317362386935837 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 90 Loss = 3.184010682548843 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 90 Loss_pred = 3.2082873318319014 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 91 Loss = 3.0801558186908675 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 91 Loss_pred = 3.17072672387801 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 92 Loss = 3.1382951438423654 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 92 Loss_pred = 3.2182518776254656 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 93 Loss = 3.204308341981593 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 93 Loss_pred = 3.22089368304042 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 94 Loss = 3.2776363575259984 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 94 Loss_pred = 3.256080069009129 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 95 Loss = 3.1618037600589566 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 95 Loss_pred = 3.1771079538609786 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 96 Loss = 3.118810384610962 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 96 Loss_pred = 3.171439792139121 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 97 Loss = 3.091099614999623 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 97 Loss_pred = 3.1786851194722634 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 98 Loss = 3.1443044002365825 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 98 Loss_pred = 3.2236686517946294 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 99 Loss = 3.1559679341503717 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 99 Loss_pred = 3.2424267338240202 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 100 Loss = 3.0856562678426194 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 100 Loss_pred = 3.1964994591987352 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 101 Loss = 3.2041188139712533 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 101 Loss_pred = 3.2563939816841567 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 102 Loss = 3.1548134933477456 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 102 Loss_pred = 3.2381775229792646 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 103 Loss = 3.169670757335009 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 103 Loss_pred = 3.193242274699671 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 104 Loss = 3.190789934538191 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 104 Loss_pred = 3.209572259678396 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 105 Loss = 3.246686618759526 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 105 Loss_pred = 3.1922231393300127 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 106 Loss = 3.2090148936497505 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 106 Loss_pred = 3.2396164605549114 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 107 Loss = 3.1506554259890174 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 107 Loss_pred = 3.2070416364393965 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 108 Loss = 3.205974343725036 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 108 Loss_pred = 3.1901636682109977 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 109 Loss = 3.1326271529405894 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 109 Loss_pred = 3.201399788151087 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 110 Loss = 3.0875308549606513 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 110 Loss_pred = 3.165172034686544 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 111 Loss = 3.236041584658013 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 111 Loss_pred = 3.211250351516245 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 112 Loss = 3.1566412368276136 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 112 Loss_pred = 3.2121128773819954 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 113 Loss = 3.2100168956388773 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 113 Loss_pred = 3.231270057589244 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 114 Loss = 3.1580615573501434 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 114 Loss_pred = 3.2219767059462616 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 115 Loss = 3.1709440471799084 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 115 Loss_pred = 3.194310543166531 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 116 Loss = 3.2818032718730263 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 116 Loss_pred = 3.281917313660325 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 117 Loss = 3.1807212614990137 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 117 Loss_pred = 3.19844112472383 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 118 Loss = 3.227467700524507 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 118 Loss_pred = 3.2026062880949078 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 119 Loss = 3.1843990282615176 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 119 Loss_pred = 3.162049067781557 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 120 Loss = 3.1747513101310063 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 120 Loss_pred = 3.2097514364584776 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 121 Loss = 3.1242345445272317 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 2 batch 121 Loss_pred = 3.183563365712497 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 122 Loss = 3.1828225792606935 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 122 Loss_pred = 3.2063731600439236 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 123 Loss = 3.162436467406834 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 123 Loss_pred = 3.133364035630046 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 124 Loss = 3.1791805454746838 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 124 Loss_pred = 3.2121122096983243 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 125 Loss = 3.2382934892834783 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 125 Loss_pred = 3.2559292325842937 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 126 Loss = 3.3207823867332893 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 126 Loss_pred = 3.1840320092720567 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 127 Loss = 3.2051057388821937 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 127 Loss_pred = 3.1711546959014028 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 128 Loss = 3.165681442169275 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 128 Loss_pred = 3.1660475353820856 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 129 Loss = 3.174904790738073 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 129 Loss_pred = 3.2239525596713885 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 130 Loss = 3.2185113040922273 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 130 Loss_pred = 3.158627484590588 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 131 Loss = 3.075315958904653 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 131 Loss_pred = 3.1534756614439856 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 132 Loss = 3.193240532833837 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 132 Loss_pred = 3.245355434450063 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 133 Loss = 3.1129500780129424 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 133 Loss_pred = 3.1692852811075873 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 134 Loss = 3.1498327121303933 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 134 Loss_pred = 3.2287120745449682 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 135 Loss = 3.1582059037595394 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 135 Loss_pred = 3.1645406383338046 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 136 Loss = 3.236162596904247 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 136 Loss_pred = 3.1775763061606916 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 137 Loss = 3.1566500494945977 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 137 Loss_pred = 3.2404163332505016 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 138 Loss = 3.1844443362301025 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 138 Loss_pred = 3.186379185308218 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 139 Loss = 3.226000704560172 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 139 Loss_pred = 3.2185278515026945 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 140 Loss = 3.2090220837939625 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 140 Loss_pred = 3.186660639894011 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 141 Loss = 3.182812913768383 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 141 Loss_pred = 3.1280597595559017 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 2 batch 142 Loss = 3.181810571225948 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 142 Loss_pred = 3.2004819262528827 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 143 Loss = 3.2018530611668585 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 143 Loss_pred = 3.2055670337537516 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 144 Loss = 3.176532341683543 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 144 Loss_pred = 3.152869795260228 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 145 Loss = 3.238286171983792 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 145 Loss_pred = 3.178881769816668 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 146 Loss = 3.244975601666436 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 146 Loss_pred = 3.2034546266234067 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 2 batch 147 Loss = 3.1895785150941 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 147 Loss_pred = 3.1326254942025673 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 148 Loss = 3.2040979640500336 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 148 Loss_pred = 3.212962800928682 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 149 Loss = 3.17918484978245 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 149 Loss_pred = 3.2158485228804743 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 150 Loss = 3.238340568708189 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 2 batch 150 Loss_pred = 3.1934761458943584 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 151 Loss = 3.1625673332330724 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 151 Loss_pred = 3.1530293909029785 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 152 Loss = 3.2518119184519554 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 152 Loss_pred = 3.216253836958721 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 153 Loss = 3.2024564842027936 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 153 Loss_pred = 3.237385420429321 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 154 Loss = 3.1648263004478303 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 154 Loss_pred = 3.1416755807852743 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 155 Loss = 3.228666588115022 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 155 Loss_pred = 3.1921522525827135 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 156 Loss = 3.1464833177783262 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 156 Loss_pred = 3.188910088113598 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 157 Loss = 3.209268278976183 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 157 Loss_pred = 3.160316530432479 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 158 Loss = 3.1798858194082142 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 158 Loss_pred = 3.1682316652597473 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 159 Loss = 3.215590932768711 Accuracy = 0.06\n",
      "\n",
      "\n",
      "epoch 2 batch 159 Loss_pred = 3.196740453213591 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 160 Loss = 3.1881636178250337 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 160 Loss_pred = 3.1674281302097427 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 161 Loss = 3.158903343378155 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 161 Loss_pred = 3.163009688860048 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 162 Loss = 3.233555626951538 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 162 Loss_pred = 3.2170300570060615 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 163 Loss = 3.236290564466245 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 163 Loss_pred = 3.1802034675585347 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 164 Loss = 3.195685477530097 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 164 Loss_pred = 3.2089601954407816 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 165 Loss = 3.192250951497467 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 165 Loss_pred = 3.2292829585204936 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 166 Loss = 3.2244563980406564 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 166 Loss_pred = 3.248601009568687 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 167 Loss = 3.301155436219697 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 167 Loss_pred = 3.2360287920341593 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 168 Loss = 3.1782323145418463 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 168 Loss_pred = 3.1876988927349146 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 169 Loss = 3.185368251297001 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 169 Loss_pred = 3.1455119890137455 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 170 Loss = 3.171255206767503 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 170 Loss_pred = 3.146143294059786 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 171 Loss = 3.241994336484688 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 171 Loss_pred = 3.2250133595937878 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 172 Loss = 3.232081825459212 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 172 Loss_pred = 3.189249983863453 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 173 Loss = 3.238809186167499 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 2 batch 173 Loss_pred = 3.217785194064236 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 174 Loss = 3.1321073896255194 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 174 Loss_pred = 3.1808810346431398 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 175 Loss = 3.219399046527152 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 175 Loss_pred = 3.204347791185755 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 176 Loss = 3.2021123836704146 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 176 Loss_pred = 3.184488112413309 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 177 Loss = 3.1720006881953537 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 177 Loss_pred = 3.1494485982472034 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 178 Loss = 3.192925578910675 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 178 Loss_pred = 3.190569626559654 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 179 Loss = 3.2577983249947318 Accuracy = 0.04\n",
      "\n",
      "\n",
      "epoch 2 batch 179 Loss_pred = 3.201529010142743 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 180 Loss = 3.2151262070576747 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 180 Loss_pred = 3.2066459758127834 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 181 Loss = 3.2325380694408654 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 181 Loss_pred = 3.1978669671055187 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 182 Loss = 3.1971153071771847 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 182 Loss_pred = 3.187089075333944 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 183 Loss = 3.2359812872641633 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 183 Loss_pred = 3.184077633293103 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 184 Loss = 3.1775846981574065 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 184 Loss_pred = 3.168672111880289 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 185 Loss = 3.2300110892427334 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 185 Loss_pred = 3.145522393527799 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 186 Loss = 3.192220848859832 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 186 Loss_pred = 3.185601776227477 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 187 Loss = 3.2313221855515586 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 187 Loss_pred = 3.227854174925578 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 188 Loss = 3.2522470024018744 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 188 Loss_pred = 3.1567621179198704 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 189 Loss = 3.1997400395506532 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 2 batch 189 Loss_pred = 3.1827161652180194 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 190 Loss = 3.20996309982628 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 190 Loss_pred = 3.166908781209335 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 191 Loss = 3.1690067979876675 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 191 Loss_pred = 3.1713978636295383 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 2 batch 192 Loss = 3.2457189378797686 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 192 Loss_pred = 3.162473947021211 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 193 Loss = 3.243812436701849 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 193 Loss_pred = 3.181099424694384 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 2 batch 194 Loss = 3.245366794852665 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 194 Loss_pred = 3.2277332194534445 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 195 Loss = 3.2185023276047606 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 2 batch 195 Loss_pred = 3.0963884084454993 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 196 Loss = 3.1965213838550004 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 2 batch 196 Loss_pred = 3.12761904833663 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 197 Loss = 3.2826267718639652 Accuracy = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 197 Loss_pred = 3.2146885316812486 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 2 batch 198 Loss = 3.159266999212028 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 2 batch 198 Loss_pred = 3.1074760051910277 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 2 batch 199 Loss = 3.159787453680812 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 2 batch 199 Loss_pred = 3.112706689207231 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 2 batch 200 Loss = 3.1874124946815607 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 2 batch 200 Loss_pred = 3.140068900148445 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "\n",
      "epoch 2 Loss = 3.2334057770572646 Accuracy = 0.11\n",
      "\n",
      "\n",
      "\n",
      "epoch 3 batch 1 Loss = 3.1650351221983426 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 1 Loss_pred = 3.1650351221983426 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 2 Loss = 3.1838743165231413 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 2 Loss_pred = 3.1945467347897614 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 3 Loss = 3.0995971060348992 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 3 Loss_pred = 3.1253970478044892 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 4 Loss = 3.0613736575827786 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 4 Loss_pred = 3.0821544248142443 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 5 Loss = 3.1128271587246474 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 5 Loss_pred = 3.1221145484147392 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 6 Loss = 3.1077541563533475 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 6 Loss_pred = 3.112107118298151 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 7 Loss = 3.12941987292219 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 7 Loss_pred = 3.1666518221718754 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 8 Loss = 3.163857414765552 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 8 Loss_pred = 3.2371120270622056 Accuracy_pred = 0.06\n",
      "\n",
      "\n",
      "epoch 3 batch 9 Loss = 3.1425243887515144 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 9 Loss_pred = 3.126282047439862 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 10 Loss = 3.089817279627655 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 10 Loss_pred = 3.1029936206514925 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 11 Loss = 3.148234672273951 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 11 Loss_pred = 3.1854727662805975 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 12 Loss = 3.1105030586114544 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 12 Loss_pred = 3.148907609247428 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 13 Loss = 3.109440445482457 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 13 Loss_pred = 3.1846703192361883 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 14 Loss = 3.0336921998633977 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 14 Loss_pred = 3.123015719099527 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 15 Loss = 3.0573746335877128 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 15 Loss_pred = 3.1547760709320394 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 16 Loss = 3.116841334390789 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 16 Loss_pred = 3.2129786162587695 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 17 Loss = 3.1508174823710937 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 17 Loss_pred = 3.1922980322617 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 18 Loss = 3.0063269051716817 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 18 Loss_pred = 3.16887910050441 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 19 Loss = 3.016565060969425 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 19 Loss_pred = 3.1669762884336894 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 20 Loss = 3.1085542227466743 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 20 Loss_pred = 3.1455581897447336 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 21 Loss = 3.007605371885717 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 21 Loss_pred = 3.1103782809052296 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 22 Loss = 3.0200627900827324 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 22 Loss_pred = 3.1041212032636274 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 23 Loss = 3.0273695841509904 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 23 Loss_pred = 3.1711736889919138 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 3 batch 24 Loss = 3.0511700381516746 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 24 Loss_pred = 3.1408623631412 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 25 Loss = 3.0888303441738607 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 25 Loss_pred = 3.100586481285659 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 26 Loss = 3.0394840055190673 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 26 Loss_pred = 3.1384243687932627 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 27 Loss = 3.07549785479976 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 27 Loss_pred = 3.1757097115194437 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 3 batch 28 Loss = 3.042609413005844 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 28 Loss_pred = 3.130339290887842 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 29 Loss = 3.2129455763236923 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 29 Loss_pred = 3.2243295278378437 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 3 batch 30 Loss = 3.0016212674300404 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 30 Loss_pred = 3.140780460101587 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 31 Loss = 3.0443869485637545 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 31 Loss_pred = 3.1995138046897798 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 32 Loss = 3.049865856544774 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 32 Loss_pred = 3.1255795995103823 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 33 Loss = 2.995553824453565 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 33 Loss_pred = 3.104435676031327 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 34 Loss = 3.0701017107932134 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 34 Loss_pred = 3.1657231496343976 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 35 Loss = 2.9588771001951453 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 35 Loss_pred = 3.1666458429881175 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 36 Loss = 3.0869037865820683 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 36 Loss_pred = 3.105944024545788 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 37 Loss = 2.9854253311931167 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 37 Loss_pred = 3.129097188028899 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 38 Loss = 2.953008075370749 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 38 Loss_pred = 3.0633879249894465 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 39 Loss = 3.104282112054684 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 39 Loss_pred = 3.1865413433290235 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 40 Loss = 2.973814716665846 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 40 Loss_pred = 3.1526926341683303 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 41 Loss = 3.0737413310271964 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 41 Loss_pred = 3.093041138807012 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 42 Loss = 2.9513968641796606 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 42 Loss_pred = 3.1143240892764608 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 43 Loss = 3.0105985702591638 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 43 Loss_pred = 3.121293573495466 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 44 Loss = 3.022606030482509 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 3 batch 44 Loss_pred = 3.141513106527777 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 45 Loss = 3.1049027682705983 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 45 Loss_pred = 3.1448710470651124 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 46 Loss = 3.084135082530416 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 46 Loss_pred = 3.1496177393081934 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 47 Loss = 3.0585449781836855 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 47 Loss_pred = 3.1372335689415793 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 48 Loss = 3.10419064437495 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 48 Loss_pred = 3.2116392507126967 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 49 Loss = 3.0270687901916857 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 49 Loss_pred = 3.1264884610338446 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 50 Loss = 3.0019298623713833 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 50 Loss_pred = 3.146240834872941 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 51 Loss = 3.054972484131247 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 51 Loss_pred = 3.19941741072622 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 52 Loss = 3.0304749690780692 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 52 Loss_pred = 3.152273799162513 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 53 Loss = 3.06748024351867 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 53 Loss_pred = 3.1646285409442028 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 54 Loss = 3.144522256727246 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 54 Loss_pred = 3.1110589050566735 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 55 Loss = 2.965689535868839 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 55 Loss_pred = 3.0889535737159446 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 56 Loss = 3.05613789539243 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 56 Loss_pred = 3.1201188295689395 Accuracy_pred = 0.08\n",
      "\n",
      "\n",
      "epoch 3 batch 57 Loss = 2.8856861886163587 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 57 Loss_pred = 3.138089722921269 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 58 Loss = 2.9352629838616697 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 58 Loss_pred = 3.121793659828643 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 59 Loss = 3.1188191938799026 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 59 Loss_pred = 3.1620551316472745 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 60 Loss = 3.0160720595678465 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 60 Loss_pred = 3.1623614625334246 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 61 Loss = 2.9789750337204226 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 61 Loss_pred = 3.146332382660238 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 62 Loss = 3.0195984200690114 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 62 Loss_pred = 3.086106236409428 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 63 Loss = 3.0985588032065006 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 63 Loss_pred = 3.2039156868645216 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 64 Loss = 3.065001360336129 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 64 Loss_pred = 3.236012520944208 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 65 Loss = 2.984839056568216 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 65 Loss_pred = 3.0832736112004957 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 66 Loss = 2.9722960943886108 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 66 Loss_pred = 3.093559394780292 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 67 Loss = 2.992565050581659 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 67 Loss_pred = 3.1436250731216546 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 68 Loss = 2.9960852989546503 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 68 Loss_pred = 3.142730968844851 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 69 Loss = 3.088383073377485 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 69 Loss_pred = 3.1372158597008446 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 70 Loss = 3.0992260668197855 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 70 Loss_pred = 3.147112410624481 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 71 Loss = 2.996995657257778 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 71 Loss_pred = 3.1451448156925768 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 72 Loss = 3.027600675105952 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 72 Loss_pred = 3.2020655421952355 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 73 Loss = 2.955015397748565 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 73 Loss_pred = 3.11532776406624 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 74 Loss = 2.9728889684109596 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 74 Loss_pred = 3.200228261659484 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 75 Loss = 3.1060872322064914 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 75 Loss_pred = 3.2154568114869164 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 3 batch 76 Loss = 3.1673593634670243 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 76 Loss_pred = 3.164095785580965 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 77 Loss = 3.05307956425552 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 77 Loss_pred = 3.1338885008495723 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 78 Loss = 2.94727712906336 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 78 Loss_pred = 3.1776997811361922 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 79 Loss = 3.057371502974572 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 79 Loss_pred = 3.1854138460133106 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 80 Loss = 3.1009414108324154 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 80 Loss_pred = 3.0872034252712695 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 81 Loss = 3.0692700674304887 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 81 Loss_pred = 3.1624365018836373 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 82 Loss = 2.9501361519198963 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 82 Loss_pred = 3.1326769329577053 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 83 Loss = 2.969598311326368 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 83 Loss_pred = 3.1249812743842784 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 84 Loss = 3.0144595657416233 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 84 Loss_pred = 3.1434364825920142 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 85 Loss = 2.9871687681255104 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 85 Loss_pred = 3.115870632650548 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 86 Loss = 2.974130651925998 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 86 Loss_pred = 3.0383784075981346 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 87 Loss = 2.9818424238824672 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 87 Loss_pred = 3.176651805624471 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 88 Loss = 3.1455456972019853 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 88 Loss_pred = 3.1394713553379843 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 89 Loss = 3.055827202637664 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 89 Loss_pred = 3.1982409258359175 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 90 Loss = 3.044781607896679 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 90 Loss_pred = 3.182701728828154 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 91 Loss = 2.888018540989004 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 91 Loss_pred = 3.070962230808409 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 92 Loss = 3.0396684493389223 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 92 Loss_pred = 3.1598960109788874 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 93 Loss = 3.051401897110858 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 93 Loss_pred = 3.1848474905504127 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 94 Loss = 3.141022147829561 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 94 Loss_pred = 3.2211002804955053 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 95 Loss = 3.0731070657934265 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 95 Loss_pred = 3.1179653296733063 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 96 Loss = 2.994183629198685 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 96 Loss_pred = 3.1021374335851397 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 97 Loss = 2.979930705547537 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 97 Loss_pred = 3.104561324245968 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 98 Loss = 3.0005522261231725 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 98 Loss_pred = 3.163077914626921 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 99 Loss = 3.094827804517056 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 99 Loss_pred = 3.1601530284824872 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 100 Loss = 2.9170542745416954 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 100 Loss_pred = 3.0929281922492793 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 101 Loss = 3.0541270052956326 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 101 Loss_pred = 3.202368312075289 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 102 Loss = 3.0494500268136076 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 102 Loss_pred = 3.1900093258083064 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 103 Loss = 3.0255795155342695 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 103 Loss_pred = 3.1360324682249887 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 104 Loss = 3.027092103235633 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 104 Loss_pred = 3.12922555461605 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 105 Loss = 3.1335116516453527 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 105 Loss_pred = 3.164647843693099 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 106 Loss = 3.1019931050758025 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 106 Loss_pred = 3.1495337996077253 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 107 Loss = 3.063185291102734 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 107 Loss_pred = 3.14494382947563 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 108 Loss = 3.075509078749218 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 108 Loss_pred = 3.146940952608103 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 109 Loss = 2.996987052343614 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 109 Loss_pred = 3.135081623708213 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 110 Loss = 3.0038298345046996 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 110 Loss_pred = 3.0962140487228145 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 111 Loss = 3.0984964362248286 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 111 Loss_pred = 3.121528644807519 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 112 Loss = 3.0432409304315935 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 112 Loss_pred = 3.1523401330839618 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 113 Loss = 3.089306369258918 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 113 Loss_pred = 3.1484390518248793 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 114 Loss = 3.0505324838395973 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 114 Loss_pred = 3.1957861998977424 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 115 Loss = 3.0783122086968286 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 115 Loss_pred = 3.131975487588402 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 116 Loss = 3.1865568409487226 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 116 Loss_pred = 3.2173948534261245 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 117 Loss = 3.1017074066354997 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 117 Loss_pred = 3.1747976025645683 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 118 Loss = 3.1320888646674745 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 118 Loss_pred = 3.1269356171482863 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 119 Loss = 3.036624982572883 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 119 Loss_pred = 3.0531582232292944 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 120 Loss = 3.059103360186859 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 120 Loss_pred = 3.1551455762250074 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 121 Loss = 3.057975130543979 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 121 Loss_pred = 3.1212850173057247 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 122 Loss = 3.0733486044943756 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 122 Loss_pred = 3.129474095388342 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 123 Loss = 3.0672225145824505 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 123 Loss_pred = 3.053741598435092 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 124 Loss = 3.058267073014605 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 124 Loss_pred = 3.1495387872656786 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 125 Loss = 3.1561131255855885 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 125 Loss_pred = 3.1905206640851023 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 126 Loss = 3.1231931813403366 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 126 Loss_pred = 3.142750053575348 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 127 Loss = 3.0954242427301177 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 127 Loss_pred = 3.1174440673142834 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 128 Loss = 3.005805810674261 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 128 Loss_pred = 3.087457489230748 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 129 Loss = 3.0784069794672217 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 129 Loss_pred = 3.165081523074125 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 130 Loss = 3.1276672766663967 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 130 Loss_pred = 3.0817570909777876 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 131 Loss = 2.976138226267254 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 131 Loss_pred = 3.0680634300404006 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 132 Loss = 3.0945996058986798 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 132 Loss_pred = 3.148731011795236 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 133 Loss = 2.980637585495123 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 133 Loss_pred = 3.108340094717338 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 134 Loss = 3.0222592448153103 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 134 Loss_pred = 3.1297019056950552 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 135 Loss = 3.102819075467729 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 135 Loss_pred = 3.080576200043527 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 136 Loss = 3.1261434156782206 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 136 Loss_pred = 3.1045294971745294 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 137 Loss = 3.0531069399500166 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 137 Loss_pred = 3.1651117199364043 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 138 Loss = 3.074137252647852 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 138 Loss_pred = 3.0749867328362037 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 139 Loss = 3.1946210212457444 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 139 Loss_pred = 3.1721584529695885 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 140 Loss = 3.062483227774879 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 140 Loss_pred = 3.0978371548273134 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 141 Loss = 3.0953548684533327 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 141 Loss_pred = 3.053179273502118 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 3 batch 142 Loss = 3.052677400420871 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 142 Loss_pred = 3.1045725217655185 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 143 Loss = 3.0858918406237352 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 143 Loss_pred = 3.1367849777166676 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 144 Loss = 3.095140812089255 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 144 Loss_pred = 3.108765650315603 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 145 Loss = 3.142906030881052 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 145 Loss_pred = 3.1011868424613938 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 146 Loss = 3.153561233922777 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 146 Loss_pred = 3.163548748449748 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 147 Loss = 3.0717714093724857 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 147 Loss_pred = 3.065037907103159 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 148 Loss = 3.1422235361786726 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 148 Loss_pred = 3.1658846542928973 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 149 Loss = 3.112970821382888 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 149 Loss_pred = 3.1425664135352056 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 150 Loss = 3.1583364993154497 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 150 Loss_pred = 3.1164000480026006 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 151 Loss = 3.074807837188414 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 151 Loss_pred = 3.0580716498976175 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 152 Loss = 3.2126411955945677 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 152 Loss_pred = 3.1643202648486857 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 153 Loss = 3.167092669913485 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 153 Loss_pred = 3.1715095358007 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 154 Loss = 3.072210703795946 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 154 Loss_pred = 3.057241016562129 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 3 batch 155 Loss = 3.1415235498602283 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 155 Loss_pred = 3.154368747282277 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 156 Loss = 3.076210573969643 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 156 Loss_pred = 3.1057525506322423 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 157 Loss = 3.1273236279193384 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 157 Loss_pred = 3.0697041723202227 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 158 Loss = 3.1017729814219686 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 158 Loss_pred = 3.0706661874840178 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 159 Loss = 3.1431242956442347 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 159 Loss_pred = 3.125640437464715 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 160 Loss = 3.123552250335169 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 160 Loss_pred = 3.0581435317775316 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 161 Loss = 3.0470528114431943 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 161 Loss_pred = 3.073220366301188 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 162 Loss = 3.1567061026853453 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 162 Loss_pred = 3.136561147938489 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 163 Loss = 3.121553173257666 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 163 Loss_pred = 3.109754643864808 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 164 Loss = 3.0963097859024726 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 164 Loss_pred = 3.169045523337077 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 165 Loss = 3.1201433502666873 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 165 Loss_pred = 3.184375938254842 Accuracy_pred = 0.1\n",
      "\n",
      "\n",
      "epoch 3 batch 166 Loss = 3.0801280827467203 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 166 Loss_pred = 3.1858281372135573 Accuracy_pred = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 167 Loss = 3.246279795834545 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 167 Loss_pred = 3.175728009883249 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 168 Loss = 3.1080103180971794 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 168 Loss_pred = 3.087391213594005 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 169 Loss = 3.066792639249411 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 169 Loss_pred = 3.0653495285146195 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 170 Loss = 3.063600718671206 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 170 Loss_pred = 3.037237440463802 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 171 Loss = 3.2555690452455726 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 171 Loss_pred = 3.1576493548515217 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 172 Loss = 3.1984004280448652 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 172 Loss_pred = 3.1402787967061108 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 173 Loss = 3.150892289645979 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 173 Loss_pred = 3.1594568587472485 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 174 Loss = 3.0497217830151295 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 174 Loss_pred = 3.102556295361868 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 175 Loss = 3.071123481833045 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 175 Loss_pred = 3.1165358764770406 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 176 Loss = 3.144348781035749 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 176 Loss_pred = 3.1029226220017962 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 177 Loss = 3.117602549359058 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 177 Loss_pred = 3.113994118810075 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 178 Loss = 3.091774611341301 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 178 Loss_pred = 3.0878731753647055 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 179 Loss = 3.20701912893424 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 179 Loss_pred = 3.1340774476110016 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 180 Loss = 3.1130964329693915 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 180 Loss_pred = 3.1068934919843394 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 181 Loss = 3.2088820629174375 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 181 Loss_pred = 3.1018884336063053 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 182 Loss = 3.059302037832645 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 182 Loss_pred = 3.0886565320249693 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 183 Loss = 3.143359221022099 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 183 Loss_pred = 3.1381127904662094 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 184 Loss = 3.1352384483920126 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 184 Loss_pred = 3.108149345959092 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 185 Loss = 3.184346986116848 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 185 Loss_pred = 3.094397368346396 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 186 Loss = 3.131180146269245 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 186 Loss_pred = 3.098034868410299 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 187 Loss = 3.1740663400035203 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 187 Loss_pred = 3.148330943470178 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 188 Loss = 3.1838660595004327 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 3 batch 188 Loss_pred = 3.0739146114700304 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 189 Loss = 3.1301737748813467 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 189 Loss_pred = 3.094323805530006 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 190 Loss = 3.149283455756765 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 190 Loss_pred = 3.0811003921118005 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 191 Loss = 3.108403804941675 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 191 Loss_pred = 3.0736837106917463 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 192 Loss = 3.1984241430302336 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 3 batch 192 Loss_pred = 3.1109243177697694 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 193 Loss = 3.155305682834318 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 3 batch 193 Loss_pred = 3.0967220068310937 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 3 batch 194 Loss = 3.172243722240941 Accuracy = 0.1\n",
      "\n",
      "\n",
      "epoch 3 batch 194 Loss_pred = 3.137930501500479 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 3 batch 195 Loss = 3.141082327978012 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 195 Loss_pred = 3.03373220641153 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 196 Loss = 3.1073118546540175 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 3 batch 196 Loss_pred = 3.0212606320649127 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 3 batch 197 Loss = 3.2505926074224782 Accuracy = 0.12\n",
      "\n",
      "\n",
      "epoch 3 batch 197 Loss_pred = 3.18517906722005 Accuracy_pred = 0.16\n",
      "\n",
      "\n",
      "epoch 3 batch 198 Loss = 3.082320545226724 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 198 Loss_pred = 2.9955745648830607 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 3 batch 199 Loss = 3.0920687513448555 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 3 batch 199 Loss_pred = 2.9796156143590458 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 3 batch 200 Loss = 3.118001505861765 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 3 batch 200 Loss_pred = 3.049720881901689 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "\n",
      "epoch 3 Loss = 3.2548422900658522 Accuracy = 0.15\n",
      "\n",
      "\n",
      "\n",
      "epoch 4 batch 1 Loss = 3.0622050910282126 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 1 Loss_pred = 3.0622050910282126 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 2 Loss = 3.057819116570205 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 2 Loss_pred = 3.0916213352460464 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 3 Loss = 2.9999995293930994 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 3 Loss_pred = 3.04001835625237 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 4 Loss = 2.9520330585930954 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 4 Loss_pred = 2.9871690912965034 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 5 Loss = 3.0259427018382365 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 5 Loss_pred = 3.0207999929400065 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 6 Loss = 3.044855716487225 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 6 Loss_pred = 3.046765592693787 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 7 Loss = 2.9562541264095823 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 7 Loss_pred = 3.0405141887015383 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 8 Loss = 3.0630412536691503 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 8 Loss_pred = 3.1599031467500067 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 9 Loss = 3.0389795824350263 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 9 Loss_pred = 3.0213748413437655 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 10 Loss = 2.9915798742373405 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 10 Loss_pred = 3.030463938107934 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 11 Loss = 3.0122189774091113 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 11 Loss_pred = 3.0950676542386595 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 4 batch 12 Loss = 3.048151291130231 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 12 Loss_pred = 3.10676913747846 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 13 Loss = 2.98704111530152 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 13 Loss_pred = 3.134374392219397 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 14 Loss = 2.9005183652070357 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 14 Loss_pred = 3.004611166450077 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 15 Loss = 2.919051079348037 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 15 Loss_pred = 3.030779529264411 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 16 Loss = 2.9489536690590183 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 16 Loss_pred = 3.1002273670162874 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 17 Loss = 3.0196380423211977 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 17 Loss_pred = 3.1195347820272192 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 18 Loss = 2.862331071096552 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 18 Loss_pred = 3.073389579041804 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 19 Loss = 2.858684551115697 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 4 batch 19 Loss_pred = 3.035893761070011 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 20 Loss = 2.9719464653748306 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 20 Loss_pred = 2.9984320974341605 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 21 Loss = 2.891889225246148 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 4 batch 21 Loss_pred = 2.974989564737377 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 22 Loss = 2.899541643043037 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 22 Loss_pred = 2.977539935680677 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 23 Loss = 2.8739651987809602 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 23 Loss_pred = 3.0740661815342807 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 24 Loss = 2.8959123704358496 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 24 Loss_pred = 2.987888498989793 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 25 Loss = 2.932674050960936 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 25 Loss_pred = 2.9919208741610714 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 26 Loss = 2.8768066956933747 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 26 Loss_pred = 3.0593194335676372 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 27 Loss = 2.9170617518759863 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 27 Loss_pred = 3.0497648909176918 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 28 Loss = 2.9217856341355604 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 28 Loss_pred = 3.0156271719365293 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 29 Loss = 3.0834839279648953 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 29 Loss_pred = 3.120494866818489 Accuracy_pred = 0.14\n",
      "\n",
      "\n",
      "epoch 4 batch 30 Loss = 2.9145344929928423 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 30 Loss_pred = 3.0517058579844343 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 31 Loss = 2.863679952578352 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 31 Loss_pred = 3.0869931650962563 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 32 Loss = 2.866516936146758 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 32 Loss_pred = 2.9956068792316204 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 33 Loss = 2.8896511666049527 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 33 Loss_pred = 2.9552235913841907 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 34 Loss = 2.921031509726499 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 34 Loss_pred = 3.031998800311823 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 35 Loss = 2.891275220656729 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 35 Loss_pred = 3.031537050462889 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 36 Loss = 2.9472303212845175 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 36 Loss_pred = 2.973566973538078 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 37 Loss = 2.863087183267992 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 37 Loss_pred = 3.0022063290684176 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 38 Loss = 2.8578158430564207 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 38 Loss_pred = 2.93022546963144 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 39 Loss = 3.0006993670975106 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 39 Loss_pred = 3.069539183812031 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 40 Loss = 2.8668458585590497 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 40 Loss_pred = 3.04024508769335 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 41 Loss = 2.96554313613143 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 41 Loss_pred = 2.990209502916749 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 42 Loss = 2.848667410667229 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 42 Loss_pred = 3.0288260530895923 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 43 Loss = 2.832848422599846 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 43 Loss_pred = 3.0259832011700314 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 44 Loss = 2.828928216462687 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 4 batch 44 Loss_pred = 3.0147314313243068 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 45 Loss = 2.9571606643114148 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 45 Loss_pred = 3.038717965701437 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 46 Loss = 2.9307046665333507 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 46 Loss_pred = 3.03917888515564 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 47 Loss = 2.8903387089218926 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 47 Loss_pred = 2.9900834398215976 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 48 Loss = 2.9925527723676355 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 48 Loss_pred = 3.104630614093093 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 49 Loss = 2.8920642577115814 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 49 Loss_pred = 3.029518404624234 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 50 Loss = 2.871517100446655 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 50 Loss_pred = 3.057783599084883 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 51 Loss = 2.8774442980883568 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 51 Loss_pred = 3.110682197797022 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 52 Loss = 2.876612531609051 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 52 Loss_pred = 3.018651323565966 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 53 Loss = 2.971099491810656 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 53 Loss_pred = 3.1233583453195877 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 54 Loss = 3.0531210931380475 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 54 Loss_pred = 3.0396558159357245 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 55 Loss = 2.800120920904865 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 55 Loss_pred = 2.9606002277919354 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 56 Loss = 2.920757372511772 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 56 Loss_pred = 3.0257287764178726 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 57 Loss = 2.75699764669126 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 57 Loss_pred = 2.9582787705671048 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 58 Loss = 2.7971694140766665 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 58 Loss_pred = 2.9543649837986785 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 59 Loss = 3.0139758497675 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 59 Loss_pred = 3.092673462483426 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 60 Loss = 2.883477873132703 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 60 Loss_pred = 3.065187776851006 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 61 Loss = 2.878376420399489 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 61 Loss_pred = 3.0327570834335473 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 62 Loss = 2.8588300478194 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 4 batch 62 Loss_pred = 2.9785502955380183 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 63 Loss = 2.948187689354788 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 63 Loss_pred = 3.1369501189018996 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 64 Loss = 2.87621369744649 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 64 Loss_pred = 3.077103890973889 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 65 Loss = 2.801360497690711 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 4 batch 65 Loss_pred = 2.9881658767221553 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 66 Loss = 2.8151833619303797 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 66 Loss_pred = 2.970893420054935 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 67 Loss = 2.8744904776348177 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 67 Loss_pred = 3.0542271710937268 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 68 Loss = 2.8696112295559555 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 68 Loss_pred = 3.0999098007197246 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 69 Loss = 2.9451874116955103 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 69 Loss_pred = 3.0228574969271977 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 70 Loss = 2.9460935361261136 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 70 Loss_pred = 3.0467945341059464 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 71 Loss = 2.8437840122056572 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 71 Loss_pred = 3.005149503037814 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 72 Loss = 2.911370240047445 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 72 Loss_pred = 3.0643208043893844 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 73 Loss = 2.8445263265142273 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 73 Loss_pred = 3.038811995048228 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 74 Loss = 2.784625119979029 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 74 Loss_pred = 3.0639997486836172 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 75 Loss = 2.982304560416101 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 75 Loss_pred = 3.1022706362889325 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 76 Loss = 3.0183349903903816 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 76 Loss_pred = 3.091973518788849 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 77 Loss = 2.9166635027367103 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 77 Loss_pred = 3.010893796462961 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 78 Loss = 2.7898319423571385 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 78 Loss_pred = 3.0407476756794938 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 79 Loss = 2.873869343596557 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 79 Loss_pred = 3.0882095308071245 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 80 Loss = 2.916132570799569 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 80 Loss_pred = 2.986567830539182 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 81 Loss = 2.902596864587604 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 81 Loss_pred = 3.0505822822328255 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 82 Loss = 2.845202151498026 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 82 Loss_pred = 2.9896866136669495 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 83 Loss = 2.7584376775918775 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 4 batch 83 Loss_pred = 2.9886893322391392 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 84 Loss = 2.8768092828385265 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 84 Loss_pred = 3.067021429703948 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 85 Loss = 2.811688641192767 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 85 Loss_pred = 3.0278824476484467 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 86 Loss = 2.8412049154462187 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 86 Loss_pred = 2.959157139640371 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 87 Loss = 2.8326717664787804 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 4 batch 87 Loss_pred = 3.073369977880217 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 88 Loss = 2.9568562903543567 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 88 Loss_pred = 3.026511704264392 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 89 Loss = 2.843240908826092 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 89 Loss_pred = 3.045504934897288 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 90 Loss = 2.886884284543571 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 90 Loss_pred = 3.0810976366959437 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 91 Loss = 2.7501563454357267 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 91 Loss_pred = 2.9648201037532154 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 92 Loss = 2.9130637500194148 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 92 Loss_pred = 3.078700022208734 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 93 Loss = 2.9328121927173947 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 93 Loss_pred = 3.0828731054362697 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 94 Loss = 2.979986692640389 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 94 Loss_pred = 3.1636825092916463 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 95 Loss = 2.994689893074975 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 95 Loss_pred = 3.027467615548377 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 96 Loss = 2.8334603703449623 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 96 Loss_pred = 3.0180611793153878 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 97 Loss = 2.7694846602616963 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 97 Loss_pred = 2.9528310922851473 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 98 Loss = 2.8317404916178184 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 98 Loss_pred = 3.0826938079095796 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 99 Loss = 2.8978370111653438 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 99 Loss_pred = 3.037391542554675 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 100 Loss = 2.7965566003673725 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 100 Loss_pred = 2.9395489728091126 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 101 Loss = 2.8831963046296405 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 101 Loss_pred = 3.069176773953455 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 102 Loss = 2.880421204603291 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 102 Loss_pred = 3.1016862226635085 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 103 Loss = 2.81345442300125 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 103 Loss_pred = 2.992111382650587 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 104 Loss = 2.8819721520486095 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 104 Loss_pred = 2.9761811476985573 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 4 batch 105 Loss = 2.9802970533276385 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 105 Loss_pred = 3.0430061519178215 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 106 Loss = 2.9672995930770716 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 106 Loss_pred = 3.0532887069342385 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 107 Loss = 3.0047108245995076 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 107 Loss_pred = 3.0509207881925797 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 108 Loss = 2.8883919596742373 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 108 Loss_pred = 3.0338564156864702 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 109 Loss = 2.8678601802601817 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 109 Loss_pred = 3.0389474040388786 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 110 Loss = 2.887475250449319 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 110 Loss_pred = 3.012392646458963 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 111 Loss = 3.0167217630062355 Accuracy = 0.14\n",
      "\n",
      "\n",
      "epoch 4 batch 111 Loss_pred = 3.021754633428816 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 112 Loss = 2.873249164823675 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 4 batch 112 Loss_pred = 3.052524564060076 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 113 Loss = 2.866159031088728 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 4 batch 113 Loss_pred = 3.0233472325555444 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 114 Loss = 2.9224569666699685 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 114 Loss_pred = 3.1237076630077594 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 115 Loss = 2.9321594173543666 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 115 Loss_pred = 3.0233788919912814 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 116 Loss = 3.0559729620244394 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 116 Loss_pred = 3.1273798155023447 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 117 Loss = 2.949063320693741 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 117 Loss_pred = 3.0881635354461845 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 118 Loss = 3.0100976416315017 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 118 Loss_pred = 3.012970425762907 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 119 Loss = 2.9100161774071918 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 119 Loss_pred = 2.9429152499945834 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 120 Loss = 2.9062771049902203 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 120 Loss_pred = 2.972484840257536 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 121 Loss = 2.9215406012524636 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 121 Loss_pred = 3.0122664999636215 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 122 Loss = 2.922919443788509 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 4 batch 122 Loss_pred = 3.04775466459826 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 123 Loss = 2.9244833776018755 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 123 Loss_pred = 2.958845161502777 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 124 Loss = 2.92903339774541 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 124 Loss_pred = 3.046048138969724 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 125 Loss = 2.988372859005285 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 125 Loss_pred = 3.0414436698586047 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 126 Loss = 2.9991285258685982 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 126 Loss_pred = 3.0361860209485076 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 127 Loss = 2.9744697064023478 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 127 Loss_pred = 3.010626970973925 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 128 Loss = 2.887071242755521 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 128 Loss_pred = 2.9799988165636955 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 129 Loss = 2.9445643848672933 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 129 Loss_pred = 3.0952597035960094 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 130 Loss = 2.9145953615202638 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 130 Loss_pred = 2.991889284372113 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 131 Loss = 2.805809819953019 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 131 Loss_pred = 2.9222511686388537 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 132 Loss = 2.9663991599529673 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 132 Loss_pred = 3.0122586816130075 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 133 Loss = 2.810130424141662 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 4 batch 133 Loss_pred = 2.985702018937123 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 134 Loss = 2.980641406220432 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 134 Loss_pred = 3.0307559153508876 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 135 Loss = 2.956082826258549 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 135 Loss_pred = 2.992010476357734 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 136 Loss = 3.0146982740159314 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 136 Loss_pred = 2.9829953846250534 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 137 Loss = 2.929433903861221 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 137 Loss_pred = 3.0930035015736195 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 138 Loss = 2.890223927641374 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 138 Loss_pred = 2.9400230816540205 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 139 Loss = 3.023430608180745 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 139 Loss_pred = 3.104978766429582 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 140 Loss = 2.898128312928806 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 140 Loss_pred = 2.9766914356992378 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 141 Loss = 2.925792551121129 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 141 Loss_pred = 2.9605992153740357 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 142 Loss = 2.923981705081687 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 142 Loss_pred = 2.9679613804143457 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 143 Loss = 2.9323820866688926 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 143 Loss_pred = 3.022463952304855 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 144 Loss = 3.042508706575511 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 144 Loss_pred = 3.0031116423860373 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 145 Loss = 3.033019656955819 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 145 Loss_pred = 2.9145037234036297 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 146 Loss = 3.0247931600893665 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 146 Loss_pred = 3.074988597206608 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 147 Loss = 2.941211881190387 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 147 Loss_pred = 2.975276745892948 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 148 Loss = 3.1137596277330215 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 148 Loss_pred = 3.046618255659561 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 149 Loss = 3.004827431518497 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 149 Loss_pred = 3.0426253257326183 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 150 Loss = 3.055080217863162 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 150 Loss_pred = 2.9919515118580042 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 151 Loss = 2.9727863217201844 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 151 Loss_pred = 2.9232070758528863 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 152 Loss = 3.1226486391415196 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 152 Loss_pred = 3.0475017283160066 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 153 Loss = 3.1185765553397964 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 153 Loss_pred = 3.1041918808150046 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 154 Loss = 2.9247300927629363 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 154 Loss_pred = 2.9648691207721134 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 155 Loss = 3.047747781909759 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 155 Loss_pred = 3.025431616551044 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 156 Loss = 2.94747313762892 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 156 Loss_pred = 2.9684540257001393 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 157 Loss = 2.98480720271853 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 157 Loss_pred = 2.962888797583478 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 158 Loss = 2.960447411295902 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 158 Loss_pred = 2.9452744057292772 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 4 batch 159 Loss = 3.0487414530274317 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 159 Loss_pred = 3.067080925781794 Accuracy_pred = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 160 Loss = 2.921773242170397 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 160 Loss_pred = 2.924167556133649 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 161 Loss = 2.9609597041190647 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 161 Loss_pred = 3.0095157409871844 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 162 Loss = 3.0675116493337238 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 162 Loss_pred = 3.064578969463184 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 163 Loss = 2.963668342157864 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 163 Loss_pred = 2.983072855438437 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 164 Loss = 3.0185729696063746 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 164 Loss_pred = 3.085397153213449 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 165 Loss = 2.972239334642675 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 165 Loss_pred = 3.0857527075484286 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 166 Loss = 2.9789498436800774 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 166 Loss_pred = 3.066164120467703 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 167 Loss = 3.126690863214941 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 167 Loss_pred = 3.034720363422347 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 168 Loss = 3.0205775053815853 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 168 Loss_pred = 2.983490992574041 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 169 Loss = 2.9686237546562593 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 169 Loss_pred = 2.9758187142889403 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 170 Loss = 2.898217103276283 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 170 Loss_pred = 2.919872846564872 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 171 Loss = 3.042804410068569 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 171 Loss_pred = 3.0158724952179443 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 172 Loss = 3.122820999440537 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 172 Loss_pred = 3.0406008252130845 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 173 Loss = 3.0580536234051965 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 173 Loss_pred = 3.0663550451079202 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 174 Loss = 2.9414998883472094 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 174 Loss_pred = 2.9959990703531503 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 175 Loss = 2.9391350362009407 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 175 Loss_pred = 2.975700047405713 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 4 batch 176 Loss = 3.0204124997588258 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 176 Loss_pred = 2.9824863166645077 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 177 Loss = 2.978432777610694 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 177 Loss_pred = 2.9998776034073615 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 178 Loss = 3.062696336963687 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 178 Loss_pred = 2.936742564760906 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 179 Loss = 3.1024972286451544 Accuracy = 0.18\n",
      "\n",
      "\n",
      "epoch 4 batch 179 Loss_pred = 3.001510006807838 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 180 Loss = 2.9675697786786492 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 180 Loss_pred = 3.016438773746305 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 181 Loss = 3.063198523491536 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 181 Loss_pred = 2.9844485627549093 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 4 batch 182 Loss = 2.9350905583143505 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 182 Loss_pred = 2.951714348449623 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 183 Loss = 3.052428895625404 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 183 Loss_pred = 3.047897964953959 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 184 Loss = 3.056224658786236 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 184 Loss_pred = 3.025121399097218 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 185 Loss = 3.0574304139489206 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 185 Loss_pred = 2.9835597452682086 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 4 batch 186 Loss = 3.049209502040743 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 186 Loss_pred = 2.970545952552267 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 187 Loss = 3.085348784143454 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 187 Loss_pred = 3.0242825021316806 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 188 Loss = 3.0515767735248853 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 188 Loss_pred = 2.9872625348060944 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 189 Loss = 3.012509904469456 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 189 Loss_pred = 2.976382722418855 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 190 Loss = 3.0065178087348006 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 190 Loss_pred = 2.9604057573412974 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 191 Loss = 3.024833799414031 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 191 Loss_pred = 2.941398495689823 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 192 Loss = 3.080520815328452 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 192 Loss_pred = 3.001520320970332 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 193 Loss = 3.0150308679339277 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 4 batch 193 Loss_pred = 2.950771101846508 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 4 batch 194 Loss = 3.038540882752727 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 194 Loss_pred = 2.984389099602942 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 195 Loss = 3.0784097000744497 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 4 batch 195 Loss_pred = 2.9278831473122624 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 4 batch 196 Loss = 2.991396598537554 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 196 Loss_pred = 2.847049001785891 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 197 Loss = 3.1233037059120283 Accuracy = 0.16\n",
      "\n",
      "\n",
      "epoch 4 batch 197 Loss_pred = 3.0299062294962162 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 198 Loss = 3.0119115476307012 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 4 batch 198 Loss_pred = 2.889099540133859 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 4 batch 199 Loss = 2.9872328362090013 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 4 batch 199 Loss_pred = 2.852112991077164 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 4 batch 200 Loss = 3.039973591709677 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 4 batch 200 Loss_pred = 2.8847769185903047 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "\n",
      "epoch 4 Loss = 3.251383783106984 Accuracy = 0.14\n",
      "\n",
      "\n",
      "\n",
      "epoch 5 batch 1 Loss = 2.9233765472148985 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 1 Loss_pred = 2.9233765472148985 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 2 Loss = 2.8400708962632217 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 2 Loss_pred = 2.8857507164282823 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 3 Loss = 2.826919700450755 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 3 Loss_pred = 2.8838956446311124 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 4 Loss = 2.761394551685395 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 4 Loss_pred = 2.8129730116168736 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 5 Loss = 2.8341631010795276 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 5 Loss_pred = 2.871720125991144 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 6 Loss = 2.944480646879496 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 6 Loss_pred = 2.9181824569819503 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 7 Loss = 2.796735385476091 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 7 Loss_pred = 2.8800685250388613 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 8 Loss = 2.871576755384999 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 8 Loss_pred = 3.023265943817853 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 9 Loss = 2.941816814624398 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 9 Loss_pred = 2.911277172514715 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 10 Loss = 2.8054232439205715 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 10 Loss_pred = 2.846756862211032 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 11 Loss = 2.845716221973778 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 11 Loss_pred = 2.965770907542801 Accuracy_pred = 0.2\n",
      "\n",
      "\n",
      "epoch 5 batch 12 Loss = 2.889709182785239 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 12 Loss_pred = 2.9701945454809318 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 13 Loss = 2.854492343599251 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 13 Loss_pred = 3.0562482031222986 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 5 batch 14 Loss = 2.7521482390251224 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 14 Loss_pred = 2.849325116996263 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 15 Loss = 2.762234875736667 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 15 Loss_pred = 2.876292275834104 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 16 Loss = 2.78761399221167 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 16 Loss_pred = 2.9520046730166216 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 17 Loss = 2.8577463305310435 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 17 Loss_pred = 2.9903017810027546 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 18 Loss = 2.6910206586568415 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 18 Loss_pred = 2.937639149655422 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 19 Loss = 2.691925478012984 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 5 batch 19 Loss_pred = 2.8975922349487844 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 20 Loss = 2.81794126437589 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 20 Loss_pred = 2.833509091487396 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 21 Loss = 2.655128600586622 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 5 batch 21 Loss_pred = 2.8187616298837894 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 22 Loss = 2.7559003784105642 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 22 Loss_pred = 2.8338888405934655 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 23 Loss = 2.654601998767863 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 23 Loss_pred = 2.8967514705178976 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 24 Loss = 2.728376638164333 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 24 Loss_pred = 2.83505566501179 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 25 Loss = 2.7512469178096617 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 25 Loss_pred = 2.875623825990197 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 26 Loss = 2.670191176882602 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 26 Loss_pred = 2.888028059238164 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 27 Loss = 2.7674933162838067 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 27 Loss_pred = 2.9129727247320303 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 28 Loss = 2.7915694229093426 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 28 Loss_pred = 2.9248396464728654 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 29 Loss = 2.97274209592989 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 29 Loss_pred = 3.0069098640819663 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 30 Loss = 2.764940774295577 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 30 Loss_pred = 2.8811033749299937 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 31 Loss = 2.725176910375302 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 31 Loss_pred = 2.943725269500401 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 32 Loss = 2.692511204933965 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 32 Loss_pred = 2.863132670567993 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 33 Loss = 2.6565633492690295 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 33 Loss_pred = 2.80188461202604 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 34 Loss = 2.7318523077386874 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 34 Loss_pred = 2.8794197805603785 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 35 Loss = 2.694683543939297 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 35 Loss_pred = 2.896461166431203 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 36 Loss = 2.7431556559569663 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 36 Loss_pred = 2.8137604854295994 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 5 batch 37 Loss = 2.7052470520177225 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 37 Loss_pred = 2.9119644255699813 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 38 Loss = 2.713221502722035 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 38 Loss_pred = 2.8231571441487007 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 39 Loss = 2.7632478710035646 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 39 Loss_pred = 2.921137797832228 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 40 Loss = 2.670352061478223 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 40 Loss_pred = 2.910435698575494 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 41 Loss = 2.7463010907208125 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 41 Loss_pred = 2.8511159695726462 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 42 Loss = 2.6539724656274446 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 42 Loss_pred = 2.877598471571687 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 43 Loss = 2.688170157672114 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 43 Loss_pred = 2.8861614475810105 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 44 Loss = 2.6707871190170813 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 5 batch 44 Loss_pred = 2.857876146018731 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 45 Loss = 2.761548226263809 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 45 Loss_pred = 2.9153685431698286 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 46 Loss = 2.7602427378335865 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 46 Loss_pred = 2.934199332080404 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 47 Loss = 2.7673206103226153 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 47 Loss_pred = 2.8766146629367664 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 48 Loss = 2.823136089849149 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 48 Loss_pred = 2.951203833364829 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 49 Loss = 2.691575049379768 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 49 Loss_pred = 2.8919692094361733 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 50 Loss = 2.730535261594919 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 50 Loss_pred = 2.961617844617893 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 51 Loss = 2.7280543725803956 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 51 Loss_pred = 2.9514488689471112 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 52 Loss = 2.6895692939539226 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 52 Loss_pred = 2.8912541799218725 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 53 Loss = 2.736114184697193 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 53 Loss_pred = 2.9940187559394027 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 54 Loss = 2.8747272225325617 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 54 Loss_pred = 2.934035912506347 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 55 Loss = 2.6343105448755546 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 5 batch 55 Loss_pred = 2.782767022687059 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 56 Loss = 2.766525263219217 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 56 Loss_pred = 2.911468284226317 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 57 Loss = 2.5637038121454885 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 57 Loss_pred = 2.808997895046681 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 58 Loss = 2.649267342131707 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 58 Loss_pred = 2.795834653096683 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 59 Loss = 2.8650127347900454 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 59 Loss_pred = 2.99161629664669 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 60 Loss = 2.7374652437144564 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 5 batch 60 Loss_pred = 2.9553283862391173 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 61 Loss = 2.7300538570342745 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 61 Loss_pred = 2.9257876059962786 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 62 Loss = 2.672079424597076 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 62 Loss_pred = 2.8423755193553295 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 63 Loss = 2.7840066067392626 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 63 Loss_pred = 3.0173913567636603 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 64 Loss = 2.707665239967541 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 64 Loss_pred = 2.9340901349205377 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 65 Loss = 2.62877268267461 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 5 batch 65 Loss_pred = 2.878050607832757 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 66 Loss = 2.719313993571266 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 66 Loss_pred = 2.9021101769798485 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 67 Loss = 2.696500594970855 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 67 Loss_pred = 2.914569758119792 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 68 Loss = 2.7179472347194262 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 68 Loss_pred = 2.965204444629904 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 69 Loss = 2.8070404623917984 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 69 Loss_pred = 2.9338929077100695 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 70 Loss = 2.765182043890551 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 70 Loss_pred = 2.9199103039625953 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 71 Loss = 2.6382335522714344 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 5 batch 71 Loss_pred = 2.855543426895063 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 72 Loss = 2.7495588122530283 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 72 Loss_pred = 2.987123146027969 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 73 Loss = 2.699494838247235 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 73 Loss_pred = 2.9472668834467766 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 74 Loss = 2.6453622516440873 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 74 Loss_pred = 2.9639693062273444 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 75 Loss = 2.8129371556883878 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 75 Loss_pred = 3.0096261020280544 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 76 Loss = 2.862086760507048 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 76 Loss_pred = 3.028624475604322 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 77 Loss = 2.7927044544202535 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 77 Loss_pred = 2.930939854721632 Accuracy_pred = 0.22\n",
      "\n",
      "\n",
      "epoch 5 batch 78 Loss = 2.6857542126019114 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 78 Loss_pred = 2.9443759855571843 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 79 Loss = 2.728550023496759 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 79 Loss_pred = 2.9392040122464067 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 80 Loss = 2.7647637280562156 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 80 Loss_pred = 2.874023323956135 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 81 Loss = 2.7031185359289602 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 81 Loss_pred = 2.959184498233755 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 82 Loss = 2.6386373287674214 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 82 Loss_pred = 2.8318725162567313 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 83 Loss = 2.6165849050151513 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 5 batch 83 Loss_pred = 2.8744296566564236 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 84 Loss = 2.7073095180170834 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 84 Loss_pred = 2.936946996521216 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 85 Loss = 2.6761970461165885 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 85 Loss_pred = 2.9235343502747346 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 86 Loss = 2.7254496954473697 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 86 Loss_pred = 2.8319349316402316 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 87 Loss = 2.651699398805722 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 87 Loss_pred = 2.938714871968809 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 88 Loss = 2.7839890105597664 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 88 Loss_pred = 2.92455392384837 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 89 Loss = 2.7005228369017575 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 89 Loss_pred = 2.9302191193956992 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 90 Loss = 2.7021876496678634 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 90 Loss_pred = 2.9310892489640246 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 91 Loss = 2.5999194542084405 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 91 Loss_pred = 2.864346625835547 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 92 Loss = 2.6394410142741265 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 92 Loss_pred = 2.9695944131210545 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 93 Loss = 2.771488324561525 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 93 Loss_pred = 2.983955403041572 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 94 Loss = 2.761233394691166 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 94 Loss_pred = 3.043740488186025 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 95 Loss = 2.781210469874897 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 95 Loss_pred = 2.9310634621902643 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 96 Loss = 2.684524555520484 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 96 Loss_pred = 2.9152629412253495 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 5 batch 97 Loss = 2.6072596886028125 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 97 Loss_pred = 2.830664992661253 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 98 Loss = 2.631369013103211 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 98 Loss_pred = 2.9668745261966247 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 99 Loss = 2.7427081170687355 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 99 Loss_pred = 2.941706328787359 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 100 Loss = 2.6270542053306465 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 100 Loss_pred = 2.8388531167245254 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 101 Loss = 2.702071469454223 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 101 Loss_pred = 2.9518812470268303 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 102 Loss = 2.726354261179301 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 102 Loss_pred = 3.0249496559319486 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 103 Loss = 2.688234651142999 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 103 Loss_pred = 2.855294687628992 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 104 Loss = 2.7038325217759267 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 104 Loss_pred = 2.8415847543004076 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 105 Loss = 2.845121852099569 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 5 batch 105 Loss_pred = 2.949933121837467 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 106 Loss = 2.7967164639908657 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 106 Loss_pred = 2.96426473168692 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 107 Loss = 2.8258005087310436 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 107 Loss_pred = 2.9413880124480443 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 108 Loss = 2.70088679072849 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 108 Loss_pred = 2.8711704701043455 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 109 Loss = 2.7150734079515666 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 109 Loss_pred = 2.9396073953300106 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 5 batch 110 Loss = 2.726144956586468 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 110 Loss_pred = 2.9648132995677225 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 111 Loss = 2.83055328828004 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 111 Loss_pred = 2.9197120732690243 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 5 batch 112 Loss = 2.7225656231524584 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 112 Loss_pred = 2.9565711750237664 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 113 Loss = 2.6729525635556923 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 113 Loss_pred = 2.8806668605108485 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 114 Loss = 2.7369783115849304 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 114 Loss_pred = 2.9961026763527583 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 115 Loss = 2.7398886995904124 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 115 Loss_pred = 2.873403625047038 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 116 Loss = 2.8313482161323043 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 116 Loss_pred = 3.0096416933612637 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 117 Loss = 2.8476332963257667 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 117 Loss_pred = 3.0250387185348986 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 118 Loss = 2.846588011444876 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 118 Loss_pred = 2.89894035597178 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 119 Loss = 2.72958709521359 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 119 Loss_pred = 2.8336221826752594 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 120 Loss = 2.7495947446282356 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 120 Loss_pred = 2.8681622302707868 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 121 Loss = 2.773948277808396 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 121 Loss_pred = 2.8520083986129277 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 122 Loss = 2.761661384679905 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 122 Loss_pred = 2.942718148017674 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 123 Loss = 2.7743619634370793 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 123 Loss_pred = 2.8038719368212806 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 124 Loss = 2.786062660331645 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 124 Loss_pred = 2.9383301491131886 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 125 Loss = 2.8578422260708183 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 125 Loss_pred = 2.931331148542127 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 126 Loss = 2.8024658817826835 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 126 Loss_pred = 2.8889120595280726 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 127 Loss = 2.8177923258918907 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 127 Loss_pred = 2.879143784047298 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 128 Loss = 2.727040337596842 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 128 Loss_pred = 2.84876509282156 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 129 Loss = 2.807397686976238 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 129 Loss_pred = 2.953414982283895 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 130 Loss = 2.7033235939786584 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 130 Loss_pred = 2.819016900578109 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 131 Loss = 2.6656549070522018 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 131 Loss_pred = 2.7510238536290057 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 132 Loss = 2.785947852634053 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 132 Loss_pred = 2.849471700483097 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 133 Loss = 2.5686099744863724 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 5 batch 133 Loss_pred = 2.8471923799801933 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 134 Loss = 2.8116348868004675 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 134 Loss_pred = 2.8825870116107395 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 135 Loss = 2.780198933141279 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 135 Loss_pred = 2.8570139955172844 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 136 Loss = 2.8433448455152646 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 136 Loss_pred = 2.8683983051931365 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 137 Loss = 2.764122845410029 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 137 Loss_pred = 2.946399446275625 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 138 Loss = 2.7314336455060935 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 138 Loss_pred = 2.7822884408591655 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 5 batch 139 Loss = 2.885653643805241 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 139 Loss_pred = 3.0318453507331418 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 140 Loss = 2.771343352379363 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 140 Loss_pred = 2.8502100841646962 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 141 Loss = 2.7579954749469615 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 141 Loss_pred = 2.858175936374635 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 142 Loss = 2.7268414267146706 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 142 Loss_pred = 2.853346328713249 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 143 Loss = 2.7464086604737203 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 5 batch 143 Loss_pred = 2.909999803633736 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 144 Loss = 2.79877464973656 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 144 Loss_pred = 2.876237149919468 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 5 batch 145 Loss = 2.8518546041136994 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 145 Loss_pred = 2.7630082600250025 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 146 Loss = 2.900255325865164 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 146 Loss_pred = 2.918121259887398 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 147 Loss = 2.755714493423296 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 147 Loss_pred = 2.82838418721873 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 148 Loss = 2.9575490903135573 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 148 Loss_pred = 2.9320420920502306 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 149 Loss = 2.843148074803967 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 149 Loss_pred = 2.904301086623309 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 150 Loss = 2.8906895693119132 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 150 Loss_pred = 2.858085624985779 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 151 Loss = 2.8600928914820556 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 151 Loss_pred = 2.7639561979670577 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 152 Loss = 3.1522132143944486 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 152 Loss_pred = 2.946415486552207 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 153 Loss = 2.9840797277296645 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 153 Loss_pred = 2.9635283195877395 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 154 Loss = 2.8379092787177784 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 154 Loss_pred = 2.832561406192545 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 155 Loss = 2.9555149472719604 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 155 Loss_pred = 2.905517537363222 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 156 Loss = 2.7954042063124733 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 156 Loss_pred = 2.8018470231624395 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 157 Loss = 2.8625066006567157 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 157 Loss_pred = 2.862059377353529 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 158 Loss = 2.802336363017665 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 158 Loss_pred = 2.836619805959224 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 159 Loss = 2.89973436164337 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 159 Loss_pred = 2.9657450054385124 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 160 Loss = 2.801875040079303 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 160 Loss_pred = 2.811367203867631 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 161 Loss = 2.8158551786663906 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 161 Loss_pred = 2.91178285307945 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 162 Loss = 2.9648855319958103 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 162 Loss_pred = 2.9448443584035853 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 163 Loss = 2.8601001808764797 Accuracy = 0.22\n",
      "\n",
      "\n",
      "epoch 5 batch 163 Loss_pred = 2.8502443514692266 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 164 Loss = 2.856167709184344 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 164 Loss_pred = 2.915121425173969 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 165 Loss = 2.8281153002876134 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 165 Loss_pred = 2.96767805408358 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 166 Loss = 2.8364672706969727 Accuracy = 0.28\n",
      "\n",
      "\n",
      "epoch 5 batch 166 Loss_pred = 2.9656667750634713 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 167 Loss = 2.9865573872321334 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 167 Loss_pred = 2.909785990632206 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 168 Loss = 2.870293175579096 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 168 Loss_pred = 2.889466059595032 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 169 Loss = 2.8205380428735305 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 169 Loss_pred = 2.856054509333314 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 170 Loss = 2.748364222757031 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 170 Loss_pred = 2.772562637743992 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 171 Loss = 2.8706497815210805 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 171 Loss_pred = 2.8798431353605816 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 172 Loss = 2.9898922248746365 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 172 Loss_pred = 2.903161459974664 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 173 Loss = 2.8869236581251356 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 173 Loss_pred = 2.8915826493189924 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 174 Loss = 2.829058328971527 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 174 Loss_pred = 2.8876424413368604 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 175 Loss = 2.7619069749827627 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 175 Loss_pred = 2.7994347705989493 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 176 Loss = 2.806368284415257 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 176 Loss_pred = 2.857129371849236 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 177 Loss = 2.8170776933533612 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 177 Loss_pred = 2.8414658066017973 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 178 Loss = 2.840078431104371 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 178 Loss_pred = 2.8080728988897774 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 179 Loss = 2.9616724734844158 Accuracy = 0.2\n",
      "\n",
      "\n",
      "epoch 5 batch 179 Loss_pred = 2.885793893732219 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 180 Loss = 2.8015794671985383 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 180 Loss_pred = 2.9123889196507267 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 181 Loss = 2.8539284224390293 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 181 Loss_pred = 2.869476685361963 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 182 Loss = 2.7310079672832486 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 182 Loss_pred = 2.761625419928502 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 183 Loss = 2.8684343301581134 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 183 Loss_pred = 2.912454094517999 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 184 Loss = 2.880141808198457 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 184 Loss_pred = 2.937090088673855 Accuracy_pred = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 185 Loss = 2.897151768084947 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 185 Loss_pred = 2.863252846271233 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 186 Loss = 2.973232067009222 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 186 Loss_pred = 2.8290869115636696 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 187 Loss = 2.983506005899499 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 5 batch 187 Loss_pred = 2.9184530250150273 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 188 Loss = 2.9457801615039627 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 188 Loss_pred = 2.8623011151279236 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 189 Loss = 2.9298576452109852 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 189 Loss_pred = 2.823038583471216 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 190 Loss = 2.928071381348467 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 190 Loss_pred = 2.828820711413919 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 191 Loss = 2.8751096912861263 Accuracy = 0.3\n",
      "\n",
      "\n",
      "epoch 5 batch 191 Loss_pred = 2.8290792078698663 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 5 batch 192 Loss = 2.983812544799045 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 192 Loss_pred = 2.829381258283831 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 5 batch 193 Loss = 2.9223339795235717 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 193 Loss_pred = 2.800783337521455 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 5 batch 194 Loss = 2.8869688601620216 Accuracy = 0.24\n",
      "\n",
      "\n",
      "epoch 5 batch 194 Loss_pred = 2.82803362931496 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 5 batch 195 Loss = 2.994616934609583 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 195 Loss_pred = 2.8304143387843466 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 5 batch 196 Loss = 2.8348736822446523 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 196 Loss_pred = 2.697242579580955 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 197 Loss = 3.034329270673735 Accuracy = 0.26\n",
      "\n",
      "\n",
      "epoch 5 batch 197 Loss_pred = 2.853092037694497 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 5 batch 198 Loss = 2.9064695032330214 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 198 Loss_pred = 2.712998392901097 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 5 batch 199 Loss = 2.8886498294936427 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 5 batch 199 Loss_pred = 2.690948319120605 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 5 batch 200 Loss = 2.913475561539509 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 5 batch 200 Loss_pred = 2.713478033869769 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "\n",
      "epoch 5 Loss = 3.252951983080525 Accuracy = 0.14\n",
      "\n",
      "\n",
      "\n",
      "epoch 6 batch 1 Loss = 2.7291587743209136 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 1 Loss_pred = 2.7291587743209136 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 2 Loss = 2.736266962521591 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 2 Loss_pred = 2.747903685531753 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 3 Loss = 2.6956069922454775 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 3 Loss_pred = 2.7586673610860983 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 4 Loss = 2.597257178435608 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 4 Loss_pred = 2.6273352536913683 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 5 Loss = 2.674890664828672 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 5 Loss_pred = 2.7799296459009346 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 6 Loss = 2.747800606040808 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 6 Loss_pred = 2.800791238381884 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 7 Loss = 2.5693891117798957 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 7 Loss_pred = 2.7660734983551154 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 8 Loss = 2.740544771661784 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 8 Loss_pred = 2.9332959609982003 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 6 batch 9 Loss = 2.738912537843439 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 9 Loss_pred = 2.7523987471155547 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 10 Loss = 2.6362980464798857 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 10 Loss_pred = 2.735602320116141 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 11 Loss = 2.6205075083400677 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 11 Loss_pred = 2.78189652557697 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 12 Loss = 2.768011280481978 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 12 Loss_pred = 2.8851434115369683 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 13 Loss = 2.720561341066115 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 13 Loss_pred = 2.9243582490818425 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 14 Loss = 2.5856402173282778 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 14 Loss_pred = 2.7155651176017246 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 15 Loss = 2.610062767637986 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 15 Loss_pred = 2.789125275216219 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 16 Loss = 2.6207559409875887 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 16 Loss_pred = 2.866894253153221 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 17 Loss = 2.671496107351914 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 17 Loss_pred = 2.8342789244529216 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 18 Loss = 2.523212419083971 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 18 Loss_pred = 2.8203103211648775 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 19 Loss = 2.5028103161886204 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 19 Loss_pred = 2.781614315932318 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 20 Loss = 2.6555316977591406 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 20 Loss_pred = 2.667288579157591 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 21 Loss = 2.48069294847655 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 6 batch 21 Loss_pred = 2.7051743198335414 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 22 Loss = 2.598821803315309 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 22 Loss_pred = 2.6695660697300787 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 23 Loss = 2.460912832026616 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 6 batch 23 Loss_pred = 2.7769364105957766 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 24 Loss = 2.5411527618935326 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 24 Loss_pred = 2.6991541480626213 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 25 Loss = 2.6400901333414413 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 25 Loss_pred = 2.7897572239414865 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 26 Loss = 2.4863987268920273 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 26 Loss_pred = 2.8243799295430643 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 27 Loss = 2.6078788817198317 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 27 Loss_pred = 2.846481358204382 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 28 Loss = 2.5549843168064137 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 28 Loss_pred = 2.8474020854156987 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 29 Loss = 2.7529491662357612 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 29 Loss_pred = 2.8152447631993405 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 30 Loss = 2.6178071326975165 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 30 Loss_pred = 2.7949086844192483 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 31 Loss = 2.5817078991759135 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 31 Loss_pred = 2.838064108624126 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 32 Loss = 2.511214533517006 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 32 Loss_pred = 2.693404441253763 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 33 Loss = 2.4447170514924714 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 33 Loss_pred = 2.6723985256397116 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 34 Loss = 2.5388335197040224 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 34 Loss_pred = 2.778187625221575 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 35 Loss = 2.515868123008712 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 35 Loss_pred = 2.8190632859022036 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 36 Loss = 2.54881200642014 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 36 Loss_pred = 2.7176699408018448 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 37 Loss = 2.5139306867854816 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 37 Loss_pred = 2.7285195410972887 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 38 Loss = 2.570563608663313 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 38 Loss_pred = 2.683255161544787 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 39 Loss = 2.57350368947499 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 39 Loss_pred = 2.8093769983024393 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 40 Loss = 2.501355617696617 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 40 Loss_pred = 2.7601023452693254 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 41 Loss = 2.5503052299580875 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 41 Loss_pred = 2.6769193736828902 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 42 Loss = 2.4801799707245196 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 42 Loss_pred = 2.705417600314405 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 43 Loss = 2.5150851024270935 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 43 Loss_pred = 2.8307166490196316 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 44 Loss = 2.4712141406925183 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 44 Loss_pred = 2.6883518564970417 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 45 Loss = 2.5345428078892733 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 45 Loss_pred = 2.811600727268473 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 46 Loss = 2.5797005492115925 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 46 Loss_pred = 2.7985384783613405 Accuracy_pred = 0.28\n",
      "\n",
      "\n",
      "epoch 6 batch 47 Loss = 2.616869128281355 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 47 Loss_pred = 2.7803610217841883 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 48 Loss = 2.619588744387654 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 48 Loss_pred = 2.7453575752353685 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 49 Loss = 2.560527784617284 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 49 Loss_pred = 2.762373185182816 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 50 Loss = 2.545263890107828 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 50 Loss_pred = 2.866480464410777 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 51 Loss = 2.578612448297132 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 51 Loss_pred = 2.792144283530862 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 52 Loss = 2.508809655851122 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 52 Loss_pred = 2.7716744251128937 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 53 Loss = 2.5526598023543112 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 53 Loss_pred = 2.8674763500656186 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 54 Loss = 2.7577905069071575 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 6 batch 54 Loss_pred = 2.817517153755236 Accuracy_pred = 0.32\n",
      "\n",
      "\n",
      "epoch 6 batch 55 Loss = 2.422375757793531 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 55 Loss_pred = 2.637111498180119 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 56 Loss = 2.631229205566971 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 56 Loss_pred = 2.79671090356172 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 57 Loss = 2.4164201360507036 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 57 Loss_pred = 2.6476575035835586 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 58 Loss = 2.5063232762710217 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 58 Loss_pred = 2.6668909494396007 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 59 Loss = 2.6608724476133636 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 59 Loss_pred = 2.8779985711149143 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 60 Loss = 2.5036528299283156 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 6 batch 60 Loss_pred = 2.8255104567297304 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 61 Loss = 2.5441052913006716 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 61 Loss_pred = 2.816169510009699 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 62 Loss = 2.49715501881723 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 62 Loss_pred = 2.6970181833330624 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 63 Loss = 2.5653025269670753 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 63 Loss_pred = 2.8544134136410113 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 64 Loss = 2.5070757964863573 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 64 Loss_pred = 2.74556972212164 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 65 Loss = 2.5065139374088874 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 6 batch 65 Loss_pred = 2.7029603594394382 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 66 Loss = 2.5412572576074055 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 66 Loss_pred = 2.767099332767245 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 67 Loss = 2.48242234970604 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 67 Loss_pred = 2.7708754777875453 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 68 Loss = 2.5655950028957353 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 68 Loss_pred = 2.8199117636969997 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 69 Loss = 2.6260348924936556 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 69 Loss_pred = 2.7985507514385923 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 70 Loss = 2.587032875216772 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 70 Loss_pred = 2.7879741929978215 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 71 Loss = 2.4159448102503225 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 71 Loss_pred = 2.741535593445077 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 72 Loss = 2.5722786849172787 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 72 Loss_pred = 2.867394974446705 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 73 Loss = 2.556139326100679 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 73 Loss_pred = 2.854135058948764 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 74 Loss = 2.4751573712372785 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 74 Loss_pred = 2.8081016557998235 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 75 Loss = 2.6347077437961977 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 75 Loss_pred = 2.8911274911772655 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 76 Loss = 2.6936161887435133 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 76 Loss_pred = 2.9189037870531975 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 77 Loss = 2.5424939909697826 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 77 Loss_pred = 2.7914062070928227 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 78 Loss = 2.5148516428783583 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 78 Loss_pred = 2.786420745278205 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 79 Loss = 2.497128265293791 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 79 Loss_pred = 2.8542210358678894 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 80 Loss = 2.5583529047130162 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 80 Loss_pred = 2.7641256026188104 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 81 Loss = 2.5079492574180158 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 81 Loss_pred = 2.8367764323643834 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 82 Loss = 2.4766972910194474 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 82 Loss_pred = 2.7184011810242703 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 83 Loss = 2.4275244233517577 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 83 Loss_pred = 2.6891614641794788 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 84 Loss = 2.5710439028745724 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 6 batch 84 Loss_pred = 2.8039631108000744 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 6 batch 85 Loss = 2.536677900814342 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 85 Loss_pred = 2.8268723502576876 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 86 Loss = 2.585350125155128 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 86 Loss_pred = 2.7207775709037505 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 87 Loss = 2.4559665741846506 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 87 Loss_pred = 2.766295124808552 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 88 Loss = 2.52662969231128 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 6 batch 88 Loss_pred = 2.7474733773383706 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 89 Loss = 2.5371478558630147 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 89 Loss_pred = 2.7186394259165136 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 90 Loss = 2.535185381598727 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 90 Loss_pred = 2.772719433303779 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 91 Loss = 2.433761285805802 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 91 Loss_pred = 2.7439688354214775 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 92 Loss = 2.471869812357652 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 92 Loss_pred = 2.814655249108195 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 93 Loss = 2.5803321029771236 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 93 Loss_pred = 2.79323227730229 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 94 Loss = 2.5806659212938463 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 6 batch 94 Loss_pred = 2.8818273415617863 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 95 Loss = 2.6043231506438373 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 95 Loss_pred = 2.8435650319297827 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 96 Loss = 2.5132341421741073 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 96 Loss_pred = 2.7709927435285273 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 97 Loss = 2.4136226881773184 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 6 batch 97 Loss_pred = 2.710238914510242 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 98 Loss = 2.4839525786297627 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 98 Loss_pred = 2.8250859476994106 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 99 Loss = 2.5349425218731256 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 99 Loss_pred = 2.8203498959518156 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 100 Loss = 2.457396851012387 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 100 Loss_pred = 2.7469324269030335 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 101 Loss = 2.4946791331773728 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 101 Loss_pred = 2.778699180628942 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 102 Loss = 2.5503933933014853 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 102 Loss_pred = 2.952012016753343 Accuracy_pred = 0.3\n",
      "\n",
      "\n",
      "epoch 6 batch 103 Loss = 2.4372706965976287 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 103 Loss_pred = 2.734364680741693 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 104 Loss = 2.5027274874421823 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 104 Loss_pred = 2.755008966183073 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 105 Loss = 2.650620888550399 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 6 batch 105 Loss_pred = 2.8696093800070868 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 106 Loss = 2.5902087270022625 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 106 Loss_pred = 2.84053342728479 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 107 Loss = 2.6371563663115647 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 107 Loss_pred = 2.83483158288956 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 108 Loss = 2.4822584238120653 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 108 Loss_pred = 2.7173016090296747 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 109 Loss = 2.5621067986595545 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 109 Loss_pred = 2.803679069713818 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 110 Loss = 2.5620671364963914 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 110 Loss_pred = 2.878212948445508 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 111 Loss = 2.6701211330478323 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 111 Loss_pred = 2.812988815436619 Accuracy_pred = 0.24\n",
      "\n",
      "\n",
      "epoch 6 batch 112 Loss = 2.5160537716635387 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 112 Loss_pred = 2.752803744100636 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 113 Loss = 2.4291334386598384 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 113 Loss_pred = 2.7069750861246953 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 114 Loss = 2.576695701728074 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 114 Loss_pred = 2.864771219020229 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 115 Loss = 2.5221193724632918 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 115 Loss_pred = 2.7709230000142413 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 116 Loss = 2.5987687691652526 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 116 Loss_pred = 2.7800203444262537 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 117 Loss = 2.6655546522873483 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 117 Loss_pred = 2.8978668745334693 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 118 Loss = 2.580024468268468 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 118 Loss_pred = 2.784828011454897 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 119 Loss = 2.5582289809507537 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 119 Loss_pred = 2.706629420183657 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 120 Loss = 2.600739874622625 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 120 Loss_pred = 2.7322230649768984 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 121 Loss = 2.6704918253348215 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 121 Loss_pred = 2.753328839991912 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 122 Loss = 2.523205437857316 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 122 Loss_pred = 2.7718282915296983 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 123 Loss = 2.5746667413854 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 123 Loss_pred = 2.6502475331466253 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 124 Loss = 2.6111442754530847 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 124 Loss_pred = 2.796140059836501 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 125 Loss = 2.7270409866574417 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 125 Loss_pred = 2.814265152612232 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 126 Loss = 2.608692189151305 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 126 Loss_pred = 2.770366960994936 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 127 Loss = 2.610201425438205 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 127 Loss_pred = 2.685054562604788 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 128 Loss = 2.5607200945970967 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 128 Loss_pred = 2.666263700660124 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 129 Loss = 2.621391428478027 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 129 Loss_pred = 2.8610638663483132 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 130 Loss = 2.535581687721688 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 130 Loss_pred = 2.6790306751086725 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 131 Loss = 2.4456609358719072 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 131 Loss_pred = 2.6587917095799756 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 132 Loss = 2.580840098101418 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 132 Loss_pred = 2.6885969349655245 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 133 Loss = 2.380381356156485 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 6 batch 133 Loss_pred = 2.6324531655502352 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 6 batch 134 Loss = 2.59198269869844 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 134 Loss_pred = 2.7232576013117966 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 135 Loss = 2.624400880000769 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 135 Loss_pred = 2.696538290544687 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 136 Loss = 2.708060584727781 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 136 Loss_pred = 2.72110817739184 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 137 Loss = 2.5368679316344744 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 137 Loss_pred = 2.810232819430918 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 138 Loss = 2.5260370555267713 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 138 Loss_pred = 2.6004168307140585 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 139 Loss = 2.709841340723073 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 139 Loss_pred = 2.8412736925511584 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 140 Loss = 2.6173732990330807 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 140 Loss_pred = 2.706168547241932 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 141 Loss = 2.61399402307257 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 141 Loss_pred = 2.6868148274170576 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 142 Loss = 2.584081121602451 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 142 Loss_pred = 2.6658757215544786 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 143 Loss = 2.545644226356137 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 143 Loss_pred = 2.738128304468602 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 144 Loss = 2.688442861963472 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 144 Loss_pred = 2.7350621428061324 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 145 Loss = 2.699423176794994 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 145 Loss_pred = 2.6516058169165757 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 146 Loss = 2.6758548754973868 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 146 Loss_pred = 2.686113968773233 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 147 Loss = 2.576441922376209 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 147 Loss_pred = 2.6783549067178445 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 148 Loss = 2.7875559972460158 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 148 Loss_pred = 2.790240662736095 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 149 Loss = 2.6545224043603652 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 149 Loss_pred = 2.7076094333490164 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 150 Loss = 2.6916256204692024 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 150 Loss_pred = 2.7279814668575626 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 151 Loss = 2.623964838015636 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 151 Loss_pred = 2.6240631745646157 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 152 Loss = 2.8535588061789285 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 152 Loss_pred = 2.7844270007947056 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 153 Loss = 2.774540181921305 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 153 Loss_pred = 2.834442017790475 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 154 Loss = 2.673922634325917 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 154 Loss_pred = 2.692389792341545 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 155 Loss = 2.8089747824254316 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 155 Loss_pred = 2.758395538967776 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 156 Loss = 2.6424561699013727 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 156 Loss_pred = 2.669805075590088 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 157 Loss = 2.676256599914966 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 157 Loss_pred = 2.7231527113526477 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 158 Loss = 2.6028833698024916 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 158 Loss_pred = 2.6513096299679724 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 159 Loss = 2.749774604780423 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 159 Loss_pred = 2.834590574051632 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 160 Loss = 2.6207175946417487 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 160 Loss_pred = 2.671292607058723 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 161 Loss = 2.670181212302263 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 161 Loss_pred = 2.766377994533191 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 162 Loss = 2.7790695922185678 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 162 Loss_pred = 2.818660275975807 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 163 Loss = 2.614766240513093 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 163 Loss_pred = 2.7309210387186202 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 164 Loss = 2.7059885816912788 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 164 Loss_pred = 2.7642906946068457 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 165 Loss = 2.596024740196102 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 6 batch 165 Loss_pred = 2.768723513456935 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 166 Loss = 2.647227943264303 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 166 Loss_pred = 2.795454451180435 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 167 Loss = 2.738848550476528 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 167 Loss_pred = 2.7379220291031565 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 168 Loss = 2.746479235502365 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 168 Loss_pred = 2.7455306067464877 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 169 Loss = 2.6460648741036907 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 169 Loss_pred = 2.65724645596171 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 170 Loss = 2.542457429471313 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 170 Loss_pred = 2.5899344621713096 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 171 Loss = 2.677154374522622 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 171 Loss_pred = 2.7237888128480243 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 172 Loss = 2.800821658087924 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 6 batch 172 Loss_pred = 2.7330332026425816 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 173 Loss = 2.657367660431591 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 173 Loss_pred = 2.702208839158314 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 174 Loss = 2.6853999550653964 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 174 Loss_pred = 2.7193783611387703 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 175 Loss = 2.657714533933152 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 175 Loss_pred = 2.593908496283698 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 176 Loss = 2.6231194330336933 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 176 Loss_pred = 2.704757179631244 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 177 Loss = 2.626135606867448 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 177 Loss_pred = 2.675470516555321 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 178 Loss = 2.672576123360304 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 178 Loss_pred = 2.621679577839729 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 179 Loss = 2.7676167967135656 Accuracy = 0.32\n",
      "\n",
      "\n",
      "epoch 6 batch 179 Loss_pred = 2.703383493176131 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 180 Loss = 2.632428602796023 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 180 Loss_pred = 2.761149464481082 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 181 Loss = 2.7319434957790767 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 181 Loss_pred = 2.7313620459973373 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 182 Loss = 2.6188108424871097 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 182 Loss_pred = 2.6163844227745887 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 183 Loss = 2.7103656789122432 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 183 Loss_pred = 2.730446460317716 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 6 batch 184 Loss = 2.660320873588054 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 184 Loss_pred = 2.7615103318952445 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 185 Loss = 2.710130381211522 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 185 Loss_pred = 2.6741254570791857 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 186 Loss = 2.7984244162003753 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 186 Loss_pred = 2.67043922607752 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 187 Loss = 2.778467902797796 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 187 Loss_pred = 2.780520974033511 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 188 Loss = 2.818419509493335 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 188 Loss_pred = 2.684114600698509 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 189 Loss = 2.789877630976394 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 189 Loss_pred = 2.67737586810313 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 190 Loss = 2.7589649556332403 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 190 Loss_pred = 2.7135815328360646 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 191 Loss = 2.7250621275542275 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 6 batch 191 Loss_pred = 2.6376263663355735 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 192 Loss = 2.835808887772261 Accuracy = 0.34\n",
      "\n",
      "\n",
      "epoch 6 batch 192 Loss_pred = 2.627083878564233 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 6 batch 193 Loss = 2.8280060606790416 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 193 Loss_pred = 2.6528461536462906 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 194 Loss = 2.7054930767498515 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 194 Loss_pred = 2.6222701535847426 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 195 Loss = 2.8211212202646916 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 6 batch 195 Loss_pred = 2.6773793201457172 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 6 batch 196 Loss = 2.6775385340831748 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 196 Loss_pred = 2.4758225964714877 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 6 batch 197 Loss = 2.8162722045530844 Accuracy = 0.36\n",
      "\n",
      "\n",
      "epoch 6 batch 197 Loss_pred = 2.6585981517729222 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 6 batch 198 Loss = 2.605700252091655 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 6 batch 198 Loss_pred = 2.50850476919776 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 6 batch 199 Loss = 2.7374949034736558 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 6 batch 199 Loss_pred = 2.5184916044172136 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 6 batch 200 Loss = 2.7703653204093093 Accuracy = 0.38\n",
      "\n",
      "\n",
      "epoch 6 batch 200 Loss_pred = 2.487478236331356 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "\n",
      "epoch 6 Loss = 3.261510626517054 Accuracy = 0.11\n",
      "\n",
      "\n",
      "\n",
      "epoch 7 batch 1 Loss = 2.5167133619904702 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 1 Loss_pred = 2.5167133619904702 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 2 Loss = 2.572967752158827 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 2 Loss_pred = 2.5741753763253428 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 3 Loss = 2.5477363417107206 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 3 Loss_pred = 2.6003119455940897 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 4 Loss = 2.4167189617855946 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 4 Loss_pred = 2.4812065098227514 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 5 Loss = 2.4696545948844366 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 5 Loss_pred = 2.588312051692313 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 6 Loss = 2.5194477636729147 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 6 Loss_pred = 2.571578826051103 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 7 Loss = 2.367845086076389 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 7 Loss_pred = 2.5740775698935874 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 8 Loss = 2.546680010226593 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 8 Loss_pred = 2.745833544646686 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 9 Loss = 2.57906876593626 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 9 Loss_pred = 2.5684324135113763 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 10 Loss = 2.4312145672331793 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 10 Loss_pred = 2.5847520764276375 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 11 Loss = 2.3845734914691605 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 7 batch 11 Loss_pred = 2.6149837039988277 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 12 Loss = 2.604850694844734 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 12 Loss_pred = 2.7398991313093424 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 13 Loss = 2.4876983891086333 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 13 Loss_pred = 2.702681989239046 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 14 Loss = 2.4189815635778333 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 14 Loss_pred = 2.516331515938268 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 15 Loss = 2.4193718092035223 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 15 Loss_pred = 2.6648238325907205 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 16 Loss = 2.4464131064569488 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 16 Loss_pred = 2.7338765478176414 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 17 Loss = 2.445737383438924 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 17 Loss_pred = 2.6983672457451364 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 18 Loss = 2.3844078415334553 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 18 Loss_pred = 2.673138302293891 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 19 Loss = 2.495560919068282 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 19 Loss_pred = 2.6287597080396505 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 20 Loss = 2.4029216542272467 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 20 Loss_pred = 2.5102304280631285 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 21 Loss = 2.3474816806878533 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 7 batch 21 Loss_pred = 2.536265562593221 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 22 Loss = 2.4414340206113088 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 22 Loss_pred = 2.5579998231147507 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 23 Loss = 2.3474970260270323 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 23 Loss_pred = 2.595892282837638 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 24 Loss = 2.3955387529545265 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 24 Loss_pred = 2.6087823492035525 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 25 Loss = 2.4732644116379596 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 25 Loss_pred = 2.613440002158863 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 26 Loss = 2.3403120085492475 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 26 Loss_pred = 2.661248387027711 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 27 Loss = 2.4352771891258302 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 27 Loss_pred = 2.694270977381403 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 28 Loss = 2.3951768373465474 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 28 Loss_pred = 2.7022216572802313 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 29 Loss = 2.4572917885743055 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 29 Loss_pred = 2.640026038549421 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 30 Loss = 2.4227806818574242 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 30 Loss_pred = 2.6511078954588734 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 31 Loss = 2.3130468612967143 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 31 Loss_pred = 2.6076035250756537 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 32 Loss = 2.3648639355602175 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 32 Loss_pred = 2.541742584426883 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 33 Loss = 2.283331877438616 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 33 Loss_pred = 2.521762180942981 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 34 Loss = 2.3366625012532656 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 34 Loss_pred = 2.6474106941403317 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 35 Loss = 2.3000736860193123 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 35 Loss_pred = 2.6201880867974667 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 36 Loss = 2.3557806683538423 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 7 batch 36 Loss_pred = 2.566131602418052 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 37 Loss = 2.3614174754606303 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 37 Loss_pred = 2.559706613966422 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 38 Loss = 2.3820580718373194 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 38 Loss_pred = 2.558132010374934 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 39 Loss = 2.389849753622644 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 39 Loss_pred = 2.6569061278012693 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 40 Loss = 2.371653061674384 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 40 Loss_pred = 2.6459874743820517 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 41 Loss = 2.3632566111294455 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 41 Loss_pred = 2.5625237401433822 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 42 Loss = 2.2973201695062686 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 42 Loss_pred = 2.5283398192233566 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 43 Loss = 2.3756599349239043 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 43 Loss_pred = 2.6717883866210554 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 44 Loss = 2.265136443884004 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 44 Loss_pred = 2.544496212549961 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 45 Loss = 2.2729640900551744 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 45 Loss_pred = 2.6634408109188756 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 46 Loss = 2.404422326962049 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 46 Loss_pred = 2.6209472800515017 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 47 Loss = 2.4381353734937448 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 47 Loss_pred = 2.629444835520489 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 48 Loss = 2.3508118463980368 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 48 Loss_pred = 2.529434548601958 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 49 Loss = 2.31679084429751 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 49 Loss_pred = 2.651015970999602 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 50 Loss = 2.422605113840828 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 50 Loss_pred = 2.7577436285557826 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 51 Loss = 2.3519815131961046 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 51 Loss_pred = 2.611231508487723 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 52 Loss = 2.344642749547133 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 52 Loss_pred = 2.6127223029090545 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 53 Loss = 2.317849573029982 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 53 Loss_pred = 2.6469244794507008 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 54 Loss = 2.51954305162561 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 54 Loss_pred = 2.6376944728633327 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 55 Loss = 2.1852811843729962 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 55 Loss_pred = 2.435513604145067 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 56 Loss = 2.4577712201301707 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 56 Loss_pred = 2.538775878867598 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 57 Loss = 2.313247007706116 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 57 Loss_pred = 2.4796228875609536 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 58 Loss = 2.326586506580096 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 58 Loss_pred = 2.5580176615329218 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 59 Loss = 2.47976385633284 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 59 Loss_pred = 2.7049667382238276 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 60 Loss = 2.2802867609431234 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 7 batch 60 Loss_pred = 2.6107076146191917 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 61 Loss = 2.38599136888731 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 61 Loss_pred = 2.580235173690983 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 62 Loss = 2.266106834511657 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 62 Loss_pred = 2.520675475877287 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 63 Loss = 2.3960402033988455 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 63 Loss_pred = 2.701483008029766 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 64 Loss = 2.283826322230072 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 64 Loss_pred = 2.611782139392167 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 65 Loss = 2.287098014261855 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 7 batch 65 Loss_pred = 2.5218439138218702 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 66 Loss = 2.3616432448834117 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 66 Loss_pred = 2.6248795059175603 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 67 Loss = 2.252041627722706 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 7 batch 67 Loss_pred = 2.578089880131638 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 68 Loss = 2.3984514871451514 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 68 Loss_pred = 2.6364122765446147 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 69 Loss = 2.4267880071337813 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 69 Loss_pred = 2.6581276009017993 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 70 Loss = 2.3390403280070116 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 70 Loss_pred = 2.581687002105732 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 71 Loss = 2.2050054736000226 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 71 Loss_pred = 2.4936000865006425 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 72 Loss = 2.376637899512245 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 72 Loss_pred = 2.71885492527049 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 73 Loss = 2.340998844129014 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 73 Loss_pred = 2.675915458898404 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 74 Loss = 2.294298516868418 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 74 Loss_pred = 2.6325929978896436 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 75 Loss = 2.418339062048915 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 75 Loss_pred = 2.695020255776309 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 76 Loss = 2.458208991749147 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 76 Loss_pred = 2.815080088736278 Accuracy_pred = 0.34\n",
      "\n",
      "\n",
      "epoch 7 batch 77 Loss = 2.3545188753495183 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 77 Loss_pred = 2.6040784789354285 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 78 Loss = 2.4038160332978187 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 78 Loss_pred = 2.5863177697643485 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 79 Loss = 2.23660480491936 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 79 Loss_pred = 2.6604912710128783 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 80 Loss = 2.36585695080008 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 80 Loss_pred = 2.6315802370976162 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 81 Loss = 2.278093267487368 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 81 Loss_pred = 2.6711596248119527 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 82 Loss = 2.2884383684749525 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 82 Loss_pred = 2.5642881862892826 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 83 Loss = 2.2116375001386057 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 83 Loss_pred = 2.5104227599483746 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 84 Loss = 2.3738690878380306 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 84 Loss_pred = 2.6771140178519324 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 85 Loss = 2.3802151327641865 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 85 Loss_pred = 2.700963666671214 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 86 Loss = 2.441363581444335 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 86 Loss_pred = 2.585023828152571 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 87 Loss = 2.229970698866813 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 87 Loss_pred = 2.6035429443763802 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 88 Loss = 2.3482500165027136 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 88 Loss_pred = 2.614875493988519 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 89 Loss = 2.3747918296186237 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 89 Loss_pred = 2.5649833417307835 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 90 Loss = 2.4174353010935885 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 90 Loss_pred = 2.6618630761223434 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 91 Loss = 2.285805911553518 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 91 Loss_pred = 2.653125856479993 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 92 Loss = 2.3319925746703807 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 92 Loss_pred = 2.643666157153837 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 93 Loss = 2.416024152995764 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 93 Loss_pred = 2.6600726283084835 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 94 Loss = 2.399177163641148 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 94 Loss_pred = 2.763380174958618 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 95 Loss = 2.448046610280516 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 95 Loss_pred = 2.7040369219006783 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 7 batch 96 Loss = 2.3264105325718965 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 7 batch 96 Loss_pred = 2.5916313373999156 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 97 Loss = 2.2543026965540434 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 97 Loss_pred = 2.4949392208805317 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 98 Loss = 2.318079266725611 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 98 Loss_pred = 2.6913018983552104 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 99 Loss = 2.344616741871943 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 99 Loss_pred = 2.6757039347940355 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 100 Loss = 2.298453184202099 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 100 Loss_pred = 2.611604222920407 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 101 Loss = 2.287108768370347 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 101 Loss_pred = 2.608785289990462 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 102 Loss = 2.3244494897157484 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 102 Loss_pred = 2.828473914498787 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 7 batch 103 Loss = 2.273453724715322 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 103 Loss_pred = 2.57049609725 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 104 Loss = 2.3289097897316102 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 104 Loss_pred = 2.585409308148204 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 105 Loss = 2.4468323395108467 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 105 Loss_pred = 2.733216602910584 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 106 Loss = 2.3743351119922314 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 106 Loss_pred = 2.7040046231718446 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 107 Loss = 2.472264331102821 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 107 Loss_pred = 2.7540972615251134 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 7 batch 108 Loss = 2.2451430338905287 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 108 Loss_pred = 2.553717963939594 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 109 Loss = 2.3387489233621372 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 109 Loss_pred = 2.611347058323998 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 110 Loss = 2.368111553355206 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 110 Loss_pred = 2.7492756989846607 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 111 Loss = 2.5668524806800908 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 111 Loss_pred = 2.703521434566182 Accuracy_pred = 0.38\n",
      "\n",
      "\n",
      "epoch 7 batch 112 Loss = 2.3003485229891205 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 112 Loss_pred = 2.6069234811779496 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 113 Loss = 2.2578260705160456 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 113 Loss_pred = 2.543379631331285 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 114 Loss = 2.299019385384174 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 114 Loss_pred = 2.7027857585431776 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 115 Loss = 2.3098138539944695 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 115 Loss_pred = 2.653907852875946 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 116 Loss = 2.3581722045449527 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 116 Loss_pred = 2.5969763118600135 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 117 Loss = 2.488895998001841 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 117 Loss_pred = 2.7356855065495376 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 118 Loss = 2.3415956304572716 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 118 Loss_pred = 2.562007948284359 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 119 Loss = 2.327077563375563 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 119 Loss_pred = 2.549051833136443 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 120 Loss = 2.461404064460314 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 120 Loss_pred = 2.643361455396273 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 121 Loss = 2.37327586327729 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 121 Loss_pred = 2.5881210634608682 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 122 Loss = 2.320964235363675 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 7 batch 122 Loss_pred = 2.6032704265888493 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 123 Loss = 2.2757107570742825 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 123 Loss_pred = 2.4787499253602654 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 124 Loss = 2.4174627032801466 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 124 Loss_pred = 2.607944863121833 Accuracy_pred = 0.36\n",
      "\n",
      "\n",
      "epoch 7 batch 125 Loss = 2.5663846119986 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 125 Loss_pred = 2.6566412151809464 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 126 Loss = 2.3826786186608064 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 126 Loss_pred = 2.625343453774322 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 127 Loss = 2.3873678849503985 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 127 Loss_pred = 2.5650348808280787 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 128 Loss = 2.397863577319772 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 128 Loss_pred = 2.5461192705517672 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 129 Loss = 2.5061244470540722 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 129 Loss_pred = 2.7109656365146106 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 130 Loss = 2.298963225662846 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 130 Loss_pred = 2.5677818641698344 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 131 Loss = 2.236046989165026 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 131 Loss_pred = 2.519254370771688 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 132 Loss = 2.3964403605164946 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 132 Loss_pred = 2.580021822420349 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 133 Loss = 2.155282400574986 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 133 Loss_pred = 2.4688682450291877 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 134 Loss = 2.33857860352681 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 134 Loss_pred = 2.5897034850897325 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 135 Loss = 2.3244618262848 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 135 Loss_pred = 2.498531985969513 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 136 Loss = 2.3927885108477156 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 136 Loss_pred = 2.4914525881868945 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 137 Loss = 2.352609038837834 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 137 Loss_pred = 2.599043270501145 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 138 Loss = 2.30761322564913 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 7 batch 138 Loss_pred = 2.48614789156122 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 139 Loss = 2.4260019883082014 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 139 Loss_pred = 2.695169124506019 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 140 Loss = 2.427390271375519 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 140 Loss_pred = 2.5300210428595675 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 141 Loss = 2.4841142350211136 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 141 Loss_pred = 2.538237764187626 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 142 Loss = 2.362803079510067 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 142 Loss_pred = 2.5045730433358657 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 143 Loss = 2.2739579859257217 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 143 Loss_pred = 2.5999774250480034 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 144 Loss = 2.38957555913117 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 144 Loss_pred = 2.5522382006519075 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 145 Loss = 2.4297102410143707 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 145 Loss_pred = 2.4815628147314914 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 146 Loss = 2.524973711496179 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 146 Loss_pred = 2.480805825602629 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 147 Loss = 2.3447936300027643 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 147 Loss_pred = 2.4975046676473016 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 148 Loss = 2.638234188833572 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 148 Loss_pred = 2.6626513676982047 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 149 Loss = 2.4036462691359946 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 149 Loss_pred = 2.486764057471574 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 150 Loss = 2.5420751609293 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 150 Loss_pred = 2.5395628723052357 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 151 Loss = 2.3658444522065434 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 151 Loss_pred = 2.466991956017154 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 152 Loss = 2.6544876496545364 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 152 Loss_pred = 2.6419220138894106 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 153 Loss = 2.5578974924799507 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 153 Loss_pred = 2.608587278914031 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 154 Loss = 2.475516258448445 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 154 Loss_pred = 2.5518997422184384 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 155 Loss = 2.528880478787905 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 155 Loss_pred = 2.5626333855347974 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 156 Loss = 2.4363194105058703 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 156 Loss_pred = 2.469166700796879 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 157 Loss = 2.523473438263268 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 157 Loss_pred = 2.566979573574273 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 158 Loss = 2.425460132092277 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 158 Loss_pred = 2.504439929978931 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 159 Loss = 2.5205797700185104 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 159 Loss_pred = 2.628417058430947 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 160 Loss = 2.406120659227876 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 160 Loss_pred = 2.441757395723467 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 161 Loss = 2.4559793633664646 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 161 Loss_pred = 2.5406806804006514 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 162 Loss = 2.5848601347204676 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 162 Loss_pred = 2.625437560850326 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 163 Loss = 2.4571481724734014 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 163 Loss_pred = 2.516717269989906 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 164 Loss = 2.4679408469875996 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 164 Loss_pred = 2.574423267683021 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 165 Loss = 2.3895466950157744 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 165 Loss_pred = 2.5494371810939676 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 166 Loss = 2.4695361696724576 Accuracy = 0.42\n",
      "\n",
      "\n",
      "epoch 7 batch 166 Loss_pred = 2.540842371222428 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 167 Loss = 2.4885292555424945 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 167 Loss_pred = 2.58200711953691 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 168 Loss = 2.5698877060754253 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 168 Loss_pred = 2.5942444645287193 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 169 Loss = 2.4815802292704836 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 169 Loss_pred = 2.5198456688159165 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 170 Loss = 2.323412514433578 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 170 Loss_pred = 2.3638500278905177 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 171 Loss = 2.4377766997636368 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 171 Loss_pred = 2.54926555600678 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 172 Loss = 2.538389880832913 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 172 Loss_pred = 2.5054569296331155 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 173 Loss = 2.4245962468563333 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 173 Loss_pred = 2.4843264636938427 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 174 Loss = 2.418073139359554 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 174 Loss_pred = 2.515927582018954 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 175 Loss = 2.444463278652925 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 175 Loss_pred = 2.3768210421701093 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 176 Loss = 2.449440834427581 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 176 Loss_pred = 2.4699943409177574 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 177 Loss = 2.4575823259713263 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 177 Loss_pred = 2.4690265676003533 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 178 Loss = 2.4566300221063018 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 178 Loss_pred = 2.439340858011586 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 179 Loss = 2.5403165600185766 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 7 batch 179 Loss_pred = 2.4864152263787362 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 180 Loss = 2.410686046980809 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 180 Loss_pred = 2.582305266599011 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 181 Loss = 2.4300289211816772 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 181 Loss_pred = 2.512363483343328 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 182 Loss = 2.4133274105698144 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 182 Loss_pred = 2.411989919411126 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 7 batch 183 Loss = 2.520347213016355 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 183 Loss_pred = 2.5305794660116305 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 184 Loss = 2.4405637452724234 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 184 Loss_pred = 2.51519955671943 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 185 Loss = 2.4174658465309347 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 185 Loss_pred = 2.434403582346295 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 186 Loss = 2.476685544497893 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 186 Loss_pred = 2.4370600614685376 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 187 Loss = 2.5508959150796766 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 187 Loss_pred = 2.5470398997838966 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 188 Loss = 2.5671523124880458 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 188 Loss_pred = 2.4324308682824722 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 189 Loss = 2.498625382017967 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 7 batch 189 Loss_pred = 2.4451580855546604 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 190 Loss = 2.5306999492861255 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 190 Loss_pred = 2.5023910641825955 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 191 Loss = 2.521418145408469 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 191 Loss_pred = 2.419042837980886 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 192 Loss = 2.5192306965144016 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 7 batch 192 Loss_pred = 2.4832555347914487 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 193 Loss = 2.593138642095363 Accuracy = 0.44\n",
      "\n",
      "\n",
      "epoch 7 batch 193 Loss_pred = 2.468312875274335 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 7 batch 194 Loss = 2.5257843494953796 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 194 Loss_pred = 2.458841124472108 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 195 Loss = 2.586070485825439 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 7 batch 195 Loss_pred = 2.4467675423371187 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 196 Loss = 2.5220553944431736 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 7 batch 196 Loss_pred = 2.3298007865220596 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 7 batch 197 Loss = 2.5991597881520856 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 7 batch 197 Loss_pred = 2.4001419384372538 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 7 batch 198 Loss = 2.4104524999158357 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 7 batch 198 Loss_pred = 2.2932084741667027 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 199 Loss = 2.4620698368556737 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 7 batch 199 Loss_pred = 2.2742313433531396 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 7 batch 200 Loss = 2.5210241971368665 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 7 batch 200 Loss_pred = 2.317976612819829 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "\n",
      "epoch 7 Loss = 3.2965297703443217 Accuracy = 0.15\n",
      "\n",
      "\n",
      "\n",
      "epoch 8 batch 1 Loss = 2.3141066522631397 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 1 Loss_pred = 2.3141066522631397 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 2 Loss = 2.392028595385242 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 2 Loss_pred = 2.35478349738506 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 3 Loss = 2.326116893058717 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 3 Loss_pred = 2.3605948826146688 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 4 Loss = 2.2502730288988118 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 4 Loss_pred = 2.2613563010579742 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 5 Loss = 2.279289478034733 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 5 Loss_pred = 2.369364474452128 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 6 Loss = 2.2931043878887176 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 6 Loss_pred = 2.3309284334427525 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 7 Loss = 2.143978210409061 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 7 Loss_pred = 2.311831113480636 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 8 Loss = 2.345216204460485 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 8 Loss_pred = 2.609791079271736 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 8 batch 9 Loss = 2.4112860141926284 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 9 Loss_pred = 2.4239218699472294 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 10 Loss = 2.24064567565035 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 10 Loss_pred = 2.347335772579831 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 11 Loss = 2.1535881038295264 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 11 Loss_pred = 2.408309061047339 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 12 Loss = 2.384928577030055 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 12 Loss_pred = 2.552127624123558 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 8 batch 13 Loss = 2.282419520492961 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 13 Loss_pred = 2.4900867098823807 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 14 Loss = 2.21131333603766 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 14 Loss_pred = 2.368473635563766 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 15 Loss = 2.234728844388049 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 15 Loss_pred = 2.478464041219669 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 16 Loss = 2.2780917273048913 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 16 Loss_pred = 2.5241568989287275 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 17 Loss = 2.198431388139617 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 17 Loss_pred = 2.5571868671102336 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 18 Loss = 2.117413843285476 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 18 Loss_pred = 2.404781895579289 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 19 Loss = 2.1154136937728705 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 19 Loss_pred = 2.3694578319408253 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 20 Loss = 2.1900278625606355 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 20 Loss_pred = 2.276189557996005 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 21 Loss = 2.1671377171083686 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 21 Loss_pred = 2.3387937509502716 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 22 Loss = 2.216287263950753 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 22 Loss_pred = 2.403291922917329 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 23 Loss = 2.152650365260661 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 23 Loss_pred = 2.4032529575152903 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 24 Loss = 2.211251764193864 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 24 Loss_pred = 2.41463263892497 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 25 Loss = 2.2789664876100906 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 25 Loss_pred = 2.42312073262858 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 26 Loss = 2.125975471911711 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 26 Loss_pred = 2.4279522774573423 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 27 Loss = 2.2653631953625517 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 27 Loss_pred = 2.461988834725369 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 28 Loss = 2.1684938066159765 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 28 Loss_pred = 2.537190067724325 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 29 Loss = 2.2265414253618765 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 29 Loss_pred = 2.4531186361825603 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 30 Loss = 2.225170488981904 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 30 Loss_pred = 2.5058731624683532 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 31 Loss = 2.1442996596210855 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 31 Loss_pred = 2.462668161498013 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 32 Loss = 2.086403551828807 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 32 Loss_pred = 2.259696155184589 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 33 Loss = 2.13221614444734 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 33 Loss_pred = 2.3305868768166302 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 34 Loss = 2.1526302417649044 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 34 Loss_pred = 2.4855009771970726 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 35 Loss = 2.0683597802941676 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 35 Loss_pred = 2.47135564638879 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 36 Loss = 2.1687907401737165 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 36 Loss_pred = 2.373415136900279 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 37 Loss = 2.120920536316344 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 37 Loss_pred = 2.3781206064517213 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 38 Loss = 2.196348235540465 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 38 Loss_pred = 2.3809885123122143 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 39 Loss = 2.1720884474376145 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 39 Loss_pred = 2.449709248936525 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 40 Loss = 2.2179434938457523 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 40 Loss_pred = 2.5138620337637687 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 41 Loss = 2.153247896629361 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 41 Loss_pred = 2.3365338783199188 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 42 Loss = 2.1165194890323353 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 42 Loss_pred = 2.381045657403601 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 43 Loss = 2.1701979408109104 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 43 Loss_pred = 2.4779819980130178 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 44 Loss = 2.0688414799254002 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 8 batch 44 Loss_pred = 2.3197941980394257 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 45 Loss = 2.0678317625131224 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 45 Loss_pred = 2.456541582238098 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 8 batch 46 Loss = 2.2167030181694236 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 46 Loss_pred = 2.4333756932449235 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 47 Loss = 2.2224202803303843 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 47 Loss_pred = 2.4550356541681224 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 8 batch 48 Loss = 2.126818195778179 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 48 Loss_pred = 2.3300306645701676 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 49 Loss = 2.117439619066099 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 49 Loss_pred = 2.4777427588271332 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 50 Loss = 2.226084910812357 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 50 Loss_pred = 2.5445418769635424 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 51 Loss = 2.1345257735832672 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 51 Loss_pred = 2.4624330800677705 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 52 Loss = 2.1579799096332892 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 52 Loss_pred = 2.4531705543908773 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 53 Loss = 2.1172252520817545 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 53 Loss_pred = 2.530312725854578 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 54 Loss = 2.26370520991809 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 54 Loss_pred = 2.4621638661892944 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 55 Loss = 2.0037971401979413 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 55 Loss_pred = 2.280726346286113 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 56 Loss = 2.1899192993487158 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 56 Loss_pred = 2.331981424828679 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 57 Loss = 2.059375900752402 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 57 Loss_pred = 2.31707801759157 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 58 Loss = 2.190886728938113 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 58 Loss_pred = 2.351020193809054 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 59 Loss = 2.2457430042455786 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 59 Loss_pred = 2.5138984471041312 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 60 Loss = 2.0355712747868746 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 8 batch 60 Loss_pred = 2.3548485594347666 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 61 Loss = 2.154429122027583 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 61 Loss_pred = 2.430728026026351 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 62 Loss = 2.145404200463478 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 62 Loss_pred = 2.3688502211967033 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 63 Loss = 2.2212673275705472 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 63 Loss_pred = 2.4958126435688204 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 64 Loss = 2.167579315170071 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 64 Loss_pred = 2.448507602525396 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 65 Loss = 2.1150181465677975 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 65 Loss_pred = 2.3904468038982807 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 66 Loss = 2.2039218771405196 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 66 Loss_pred = 2.478340175113345 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 67 Loss = 2.004897916183909 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 67 Loss_pred = 2.4204379600023715 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 68 Loss = 2.200452080887225 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 68 Loss_pred = 2.4762772709601153 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 69 Loss = 2.2057218527817293 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 69 Loss_pred = 2.514354514409006 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 70 Loss = 2.1601923156616603 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 70 Loss_pred = 2.438910246958792 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 71 Loss = 2.011788048700639 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 71 Loss_pred = 2.3121468893313177 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 72 Loss = 2.1817684298412363 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 72 Loss_pred = 2.598531211858901 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 73 Loss = 2.126146697917661 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 73 Loss_pred = 2.523301280710613 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 74 Loss = 2.1147111775742737 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 74 Loss_pred = 2.4819917204276867 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 75 Loss = 2.1771663092281375 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 75 Loss_pred = 2.549275461805043 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 76 Loss = 2.2386548060670717 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 76 Loss_pred = 2.6589062165289272 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 8 batch 77 Loss = 2.1331825458908473 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 77 Loss_pred = 2.504201129540698 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 78 Loss = 2.1880790393715173 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 78 Loss_pred = 2.4684719257548564 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 79 Loss = 2.0197499012837032 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 79 Loss_pred = 2.5329131466898236 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 80 Loss = 2.196661537410287 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 80 Loss_pred = 2.362328467328397 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 81 Loss = 2.0645601665576114 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 81 Loss_pred = 2.520150022988954 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 82 Loss = 2.0751848341887733 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 82 Loss_pred = 2.4203597369934036 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 83 Loss = 2.019805593749429 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 83 Loss_pred = 2.3380389206062864 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 84 Loss = 2.202610923227239 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 84 Loss_pred = 2.513011674219665 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 85 Loss = 2.1880309605760058 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 85 Loss_pred = 2.534468670815918 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 86 Loss = 2.2514683656086913 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 86 Loss_pred = 2.5101235888872244 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 87 Loss = 2.0228630482944943 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 8 batch 87 Loss_pred = 2.4397826097370747 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 88 Loss = 2.063137205917418 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 88 Loss_pred = 2.4673549420469247 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 89 Loss = 2.1183440229043122 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 8 batch 89 Loss_pred = 2.3324026704435745 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 90 Loss = 2.2209470771509037 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 90 Loss_pred = 2.4371880124612217 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 91 Loss = 2.142157242616033 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 91 Loss_pred = 2.4609447524809385 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 92 Loss = 2.2702688148149126 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 8 batch 92 Loss_pred = 2.4713025409687615 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 93 Loss = 2.2624231048079824 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 93 Loss_pred = 2.4695070358197335 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 94 Loss = 2.1883508856960736 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 94 Loss_pred = 2.527388493386882 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 95 Loss = 2.2490391665551517 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 95 Loss_pred = 2.5181415826180302 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 96 Loss = 2.130325509973349 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 96 Loss_pred = 2.3746033464447143 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 97 Loss = 2.057303095985601 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 97 Loss_pred = 2.3268047767243796 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 98 Loss = 2.1781939012202334 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 98 Loss_pred = 2.525987805473409 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 99 Loss = 2.164504093208037 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 99 Loss_pred = 2.4823341355508983 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 100 Loss = 2.126845095221562 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 100 Loss_pred = 2.484874827895917 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 101 Loss = 2.126111404297505 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 101 Loss_pred = 2.449948165199321 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 102 Loss = 2.1646170888360365 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 102 Loss_pred = 2.690395444143562 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 8 batch 103 Loss = 2.051744753410233 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 103 Loss_pred = 2.3963505715250637 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 104 Loss = 2.122083980248563 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 104 Loss_pred = 2.478859147837653 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 105 Loss = 2.2454430769883853 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 105 Loss_pred = 2.5787950969947064 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 106 Loss = 2.181717209132988 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 106 Loss_pred = 2.5737686211701725 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 107 Loss = 2.3005116744605094 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 107 Loss_pred = 2.6279874065195687 Accuracy_pred = 0.4\n",
      "\n",
      "\n",
      "epoch 8 batch 108 Loss = 2.0503068663995374 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 8 batch 108 Loss_pred = 2.4376832992330395 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 109 Loss = 2.1807266306039805 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 109 Loss_pred = 2.4728415923712497 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 110 Loss = 2.1638285475477925 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 110 Loss_pred = 2.609252240460841 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 111 Loss = 2.3494463082708887 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 111 Loss_pred = 2.5396825106456102 Accuracy_pred = 0.44\n",
      "\n",
      "\n",
      "epoch 8 batch 112 Loss = 2.1188377882597114 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 112 Loss_pred = 2.4350952330514866 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 113 Loss = 2.0800480328499593 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 113 Loss_pred = 2.433401044341902 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 114 Loss = 2.1109575987133127 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 114 Loss_pred = 2.4834902269018557 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 115 Loss = 2.077488197549725 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 115 Loss_pred = 2.4063536340214844 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 116 Loss = 2.1386232713633073 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 116 Loss_pred = 2.4892767533937774 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 117 Loss = 2.24790325915963 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 117 Loss_pred = 2.5217301912977694 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 118 Loss = 2.1237420097003774 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 118 Loss_pred = 2.4438533352822676 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 119 Loss = 2.1365879100937035 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 119 Loss_pred = 2.3982436409775385 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 120 Loss = 2.2887212637509595 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 120 Loss_pred = 2.492637770418225 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 121 Loss = 2.2108149460482305 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 121 Loss_pred = 2.4363352468712614 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 122 Loss = 2.1175848168962683 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 122 Loss_pred = 2.5068238018541042 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 123 Loss = 2.034115720724464 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 123 Loss_pred = 2.3003303982173113 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 124 Loss = 2.151390699769307 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 124 Loss_pred = 2.4559090151775345 Accuracy_pred = 0.42\n",
      "\n",
      "\n",
      "epoch 8 batch 125 Loss = 2.3422929961807513 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 125 Loss_pred = 2.4554961632505328 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 126 Loss = 2.1624163479394545 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 126 Loss_pred = 2.444234185932932 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 127 Loss = 2.1291210305972554 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 127 Loss_pred = 2.352218869801397 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 128 Loss = 2.170820073057781 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 128 Loss_pred = 2.4022377103147012 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 129 Loss = 2.2712874884322445 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 129 Loss_pred = 2.5599862740727044 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 8 batch 130 Loss = 2.1688859702121897 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 130 Loss_pred = 2.3587702626999185 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 131 Loss = 2.001959232490554 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 131 Loss_pred = 2.329273047548359 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 132 Loss = 2.1774005623001464 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 132 Loss_pred = 2.4388243620884924 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 133 Loss = 1.953382355824442 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 133 Loss_pred = 2.2510993597992575 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 134 Loss = 2.11998276778305 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 134 Loss_pred = 2.3899573842117654 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 135 Loss = 2.08729215948164 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 135 Loss_pred = 2.3117954269240992 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 136 Loss = 2.15822960348222 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 136 Loss_pred = 2.339081438546884 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 137 Loss = 2.1124303014298755 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 137 Loss_pred = 2.393102692293242 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 138 Loss = 2.010225315867289 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 8 batch 138 Loss_pred = 2.2850990635016353 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 139 Loss = 2.1397856247746048 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 139 Loss_pred = 2.4577867857768054 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 140 Loss = 2.3198418292877503 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 140 Loss_pred = 2.4566737322799237 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 141 Loss = 2.2616697933656758 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 141 Loss_pred = 2.3785785645177855 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 142 Loss = 2.126503555842117 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 8 batch 142 Loss_pred = 2.3189884171071213 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 143 Loss = 2.02716961368831 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 143 Loss_pred = 2.3206727512856737 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 144 Loss = 2.1359506580373098 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 144 Loss_pred = 2.38858475201359 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 145 Loss = 2.1886606038314835 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 145 Loss_pred = 2.2876211422334825 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 146 Loss = 2.25393100234596 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 146 Loss_pred = 2.29191349275265 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 147 Loss = 2.130238864108882 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 147 Loss_pred = 2.3904026875084416 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 148 Loss = 2.3655181622035855 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 148 Loss_pred = 2.4971240313091925 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 149 Loss = 2.1804209502306424 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 149 Loss_pred = 2.2839918901502654 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 150 Loss = 2.3448425994603923 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 150 Loss_pred = 2.3413860844165457 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 151 Loss = 2.1964854624619936 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 151 Loss_pred = 2.325914698197571 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 152 Loss = 2.399041913418329 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 152 Loss_pred = 2.4341846724499123 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 153 Loss = 2.3763423647187483 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 153 Loss_pred = 2.3880550668538856 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 154 Loss = 2.320140628581416 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 154 Loss_pred = 2.385340475938695 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 155 Loss = 2.3429095915781013 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 155 Loss_pred = 2.4349768437325165 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 156 Loss = 2.2406431573973498 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 156 Loss_pred = 2.3399315028747005 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 157 Loss = 2.2479031120144306 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 157 Loss_pred = 2.3843928256138702 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 158 Loss = 2.2079996621321567 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 158 Loss_pred = 2.308290764135346 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 159 Loss = 2.3637704719426207 Accuracy = 0.48\n",
      "\n",
      "\n",
      "epoch 8 batch 159 Loss_pred = 2.4367037293727183 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 160 Loss = 2.2013238627478815 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 160 Loss_pred = 2.259794748863836 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 161 Loss = 2.204995705965671 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 161 Loss_pred = 2.3596455153933658 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 162 Loss = 2.29016396237653 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 162 Loss_pred = 2.3779194688802097 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 163 Loss = 2.2676716084469053 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 163 Loss_pred = 2.3264059230203915 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 164 Loss = 2.2786227922949243 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 164 Loss_pred = 2.459727710848376 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 165 Loss = 2.1602272295200446 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 165 Loss_pred = 2.3665342623146266 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 166 Loss = 2.230164489547403 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 166 Loss_pred = 2.367551177457813 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 167 Loss = 2.27282493719257 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 167 Loss_pred = 2.321703578248434 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 168 Loss = 2.3295035384502483 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 168 Loss_pred = 2.3554727213048103 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 169 Loss = 2.2922677314884687 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 169 Loss_pred = 2.3153236631740244 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 170 Loss = 2.1293536623727385 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 170 Loss_pred = 2.1642705105265208 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 171 Loss = 2.270527908877457 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 171 Loss_pred = 2.414272716052727 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 172 Loss = 2.3437739507497004 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 8 batch 172 Loss_pred = 2.2909068880626497 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 173 Loss = 2.127460205206867 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 173 Loss_pred = 2.202981103681486 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 8 batch 174 Loss = 2.1568124373737874 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 174 Loss_pred = 2.280939483250109 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 175 Loss = 2.1938936768309785 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 175 Loss_pred = 2.153543732149921 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 176 Loss = 2.179601447659995 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 176 Loss_pred = 2.310843064400147 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 177 Loss = 2.276336864461836 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 177 Loss_pred = 2.314774099765713 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 178 Loss = 2.200323842598315 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 178 Loss_pred = 2.1818110016842756 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 179 Loss = 2.257202317990236 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 179 Loss_pred = 2.3035024065802028 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 180 Loss = 2.1538145041206773 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 180 Loss_pred = 2.2839296281131083 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 181 Loss = 2.226440040717364 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 181 Loss_pred = 2.2993664453973786 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 182 Loss = 2.203557362925658 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 182 Loss_pred = 2.212987224954525 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 183 Loss = 2.329249670854756 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 183 Loss_pred = 2.288796800867562 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 184 Loss = 2.210940330646171 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 184 Loss_pred = 2.321846048412481 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 185 Loss = 2.194533478391376 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 185 Loss_pred = 2.238109317147597 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 186 Loss = 2.2657470176218197 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 8 batch 186 Loss_pred = 2.2085161029539604 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 8 batch 187 Loss = 2.336336632792378 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 187 Loss_pred = 2.3500804389593 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 188 Loss = 2.2777374079704233 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 188 Loss_pred = 2.2226741597864788 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 189 Loss = 2.3059813082596077 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 189 Loss_pred = 2.266545910138476 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 190 Loss = 2.3056375968492864 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 190 Loss_pred = 2.25948471560876 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 191 Loss = 2.25526937398962 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 191 Loss_pred = 2.1762436598164534 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 8 batch 192 Loss = 2.304887853422344 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 192 Loss_pred = 2.2875270590805146 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 8 batch 193 Loss = 2.418852429375826 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 193 Loss_pred = 2.280022482551231 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 194 Loss = 2.250362680344326 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 194 Loss_pred = 2.1786736478543185 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 8 batch 195 Loss = 2.387707064033616 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 195 Loss_pred = 2.1847552418627942 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 196 Loss = 2.3308218542853454 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 8 batch 196 Loss_pred = 2.1169826753056453 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 8 batch 197 Loss = 2.3506373697119596 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 8 batch 197 Loss_pred = 2.1381069233338468 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 198 Loss = 2.244086909503549 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 8 batch 198 Loss_pred = 2.070908636453572 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 8 batch 199 Loss = 2.254635229274806 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 8 batch 199 Loss_pred = 2.0728337180097283 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 8 batch 200 Loss = 2.2817463448974995 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 8 batch 200 Loss_pred = 2.08609437205322 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "\n",
      "epoch 8 Loss = 3.3610988560872195 Accuracy = 0.16\n",
      "\n",
      "\n",
      "\n",
      "epoch 9 batch 1 Loss = 2.0973265631819444 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 1 Loss_pred = 2.0973265631819444 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 2 Loss = 2.1574437737450967 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 2 Loss_pred = 2.1621989944562836 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 3 Loss = 2.12750399609912 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 3 Loss_pred = 2.188929749855969 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 4 Loss = 2.03507468352932 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 4 Loss_pred = 2.0915849387587975 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 5 Loss = 2.0445747784871866 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 5 Loss_pred = 2.1598363166399173 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 6 Loss = 2.036607379479856 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 6 Loss_pred = 2.1153209260861066 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 7 Loss = 1.965508436519439 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 7 Loss_pred = 2.115101438801087 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 8 Loss = 2.1317115127353485 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 8 Loss_pred = 2.416692652954008 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 9 Loss = 2.2195127229168765 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 9 Loss_pred = 2.248191064542286 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 10 Loss = 2.018868025507039 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 10 Loss_pred = 2.130714540546892 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 11 Loss = 1.945433698939396 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 11 Loss_pred = 2.1584115971872198 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 12 Loss = 2.1578178772966514 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 12 Loss_pred = 2.2817182502751274 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 13 Loss = 2.0755957967651786 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 13 Loss_pred = 2.257797089488835 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 14 Loss = 2.025072456499061 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 14 Loss_pred = 2.1591406023856394 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 15 Loss = 2.0374402906525124 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 15 Loss_pred = 2.193513956859844 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 16 Loss = 2.0602581114320584 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 16 Loss_pred = 2.278491047549766 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 17 Loss = 1.9978989534690288 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 17 Loss_pred = 2.310215610429359 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 18 Loss = 1.894126936076904 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 18 Loss_pred = 2.276600535769931 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 19 Loss = 1.8890014844257876 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 19 Loss_pred = 2.1901484833152494 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 20 Loss = 1.9086806066498878 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 20 Loss_pred = 2.1147081115804474 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 21 Loss = 1.9533125738209116 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 21 Loss_pred = 2.157528208106805 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 22 Loss = 2.020005310941178 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 22 Loss_pred = 2.2143557825612623 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 23 Loss = 1.9825188391195776 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 23 Loss_pred = 2.21802370237156 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 24 Loss = 1.9611597615329885 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 24 Loss_pred = 2.209689131433579 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 25 Loss = 2.055822179169105 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 25 Loss_pred = 2.192805357643438 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 26 Loss = 1.9425241071499924 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 26 Loss_pred = 2.2012589332057697 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 27 Loss = 2.0467809879957217 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 27 Loss_pred = 2.292282556610031 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 28 Loss = 1.9814014536221731 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 28 Loss_pred = 2.364025614641701 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 29 Loss = 2.0025794911568973 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 29 Loss_pred = 2.2457022512215303 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 30 Loss = 2.0275464382908117 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 30 Loss_pred = 2.3668258663347523 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 31 Loss = 1.9300026352009416 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 31 Loss_pred = 2.2646143233404814 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 32 Loss = 1.8831269370085955 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 32 Loss_pred = 2.0918517265120067 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 33 Loss = 1.9223702144910926 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 33 Loss_pred = 2.147147267667694 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 34 Loss = 1.941941691879401 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 34 Loss_pred = 2.3234199689698913 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 35 Loss = 1.8718291897910928 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 35 Loss_pred = 2.287203875582926 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 36 Loss = 1.9121893842356639 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 36 Loss_pred = 2.1464112299502314 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 37 Loss = 1.9327153316571424 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 37 Loss_pred = 2.1866797159577276 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 38 Loss = 1.983929538165383 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 38 Loss_pred = 2.2465691786403656 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 39 Loss = 1.8755061103186814 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 39 Loss_pred = 2.2427512910832155 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 40 Loss = 1.9915183700445869 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 40 Loss_pred = 2.3667439803742187 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 41 Loss = 1.9382369460939561 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 41 Loss_pred = 2.164565243942796 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 42 Loss = 1.9347081549428293 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 42 Loss_pred = 2.1979493964726813 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 43 Loss = 1.9547484569296916 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 43 Loss_pred = 2.2648189984409686 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 44 Loss = 1.8799583838075256 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 44 Loss_pred = 2.23357415732627 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 45 Loss = 1.9072956046333638 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 45 Loss_pred = 2.302435144152597 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 46 Loss = 1.992898320753161 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 46 Loss_pred = 2.2438421822893226 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 47 Loss = 2.0332167116324134 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 47 Loss_pred = 2.256328926223128 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 48 Loss = 1.8641576922397571 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 48 Loss_pred = 2.124391117226389 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 49 Loss = 1.91916384741781 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 49 Loss_pred = 2.281697612030131 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 50 Loss = 1.9825355274959133 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 50 Loss_pred = 2.410781931013143 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 51 Loss = 1.9176196046933922 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 51 Loss_pred = 2.2859947739719106 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 52 Loss = 1.9875843986846307 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 52 Loss_pred = 2.2684252016751607 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 53 Loss = 1.90081548929278 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 53 Loss_pred = 2.286825639646849 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 54 Loss = 2.0190540367266103 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 54 Loss_pred = 2.2493847556259494 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 55 Loss = 1.8008496094438278 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 55 Loss_pred = 2.1085191531468017 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 56 Loss = 1.9732066812285183 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 56 Loss_pred = 2.1135205605086247 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 57 Loss = 1.8755850244465655 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 57 Loss_pred = 2.149476280314246 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 58 Loss = 1.9996209290491411 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 58 Loss_pred = 2.1717407447640453 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 59 Loss = 1.979736342110096 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 59 Loss_pred = 2.28300649671554 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 60 Loss = 1.8527707861183984 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 60 Loss_pred = 2.1159497441469672 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 61 Loss = 2.0368828947376922 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 61 Loss_pred = 2.1757283076154246 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 62 Loss = 1.9910269800987204 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 62 Loss_pred = 2.2606409653061137 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 63 Loss = 1.978114259979838 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 63 Loss_pred = 2.2674512863944254 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 64 Loss = 1.8980973260902971 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 64 Loss_pred = 2.2339837261346265 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 65 Loss = 1.8261473632046028 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 65 Loss_pred = 2.1059667295584346 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 66 Loss = 1.9568670972067788 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 66 Loss_pred = 2.3172436548426467 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 67 Loss = 1.8093383500240787 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 67 Loss_pred = 2.260686340618339 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 68 Loss = 1.9796274146113007 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 68 Loss_pred = 2.246718774664163 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 69 Loss = 2.0068584298696246 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 69 Loss_pred = 2.40547982903569 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 70 Loss = 1.9096191656795236 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 70 Loss_pred = 2.2160611459277315 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 71 Loss = 1.7874014290397002 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 9 batch 71 Loss_pred = 2.070885273349247 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 72 Loss = 2.003493314530859 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 72 Loss_pred = 2.48973025226991 Accuracy_pred = 0.46\n",
      "\n",
      "\n",
      "epoch 9 batch 73 Loss = 1.883386255244895 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 73 Loss_pred = 2.2597531583374075 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 74 Loss = 1.8986687074272086 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 74 Loss_pred = 2.2978499348533012 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 75 Loss = 2.0018842958001577 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 75 Loss_pred = 2.321655936510752 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 76 Loss = 2.0078399151056074 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 76 Loss_pred = 2.463313103501375 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 77 Loss = 1.9153894784575096 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 77 Loss_pred = 2.3620375008119954 Accuracy_pred = 0.48\n",
      "\n",
      "\n",
      "epoch 9 batch 78 Loss = 1.9723247452448898 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 78 Loss_pred = 2.32873726919824 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 79 Loss = 1.8044895156132472 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 79 Loss_pred = 2.2919513204082125 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 80 Loss = 1.9642137251817406 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 80 Loss_pred = 2.178393833386491 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 81 Loss = 1.8439403203769176 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 81 Loss_pred = 2.2775927676322825 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 82 Loss = 1.8864340760829152 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 82 Loss_pred = 2.2679258054540576 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 83 Loss = 1.8445959024860048 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 83 Loss_pred = 2.1537018027011334 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 84 Loss = 1.9591836097227395 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 84 Loss_pred = 2.380659337613559 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 85 Loss = 1.999518778362383 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 85 Loss_pred = 2.4095734044805712 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 86 Loss = 2.039286522957161 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 86 Loss_pred = 2.2908259384942937 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 87 Loss = 1.8067881318453796 Accuracy = 0.86\n",
      "\n",
      "\n",
      "epoch 9 batch 87 Loss_pred = 2.24667849524621 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 88 Loss = 1.839199377703148 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 9 batch 88 Loss_pred = 2.2840159982906445 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 89 Loss = 1.8907565609989638 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 89 Loss_pred = 2.1065092323091816 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 90 Loss = 1.9996275422268963 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 90 Loss_pred = 2.273282749610106 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 91 Loss = 1.8748938942404936 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 91 Loss_pred = 2.221811329862727 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 92 Loss = 1.9355768858661322 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 92 Loss_pred = 2.3003442746988885 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 93 Loss = 2.026023408949031 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 93 Loss_pred = 2.368890983121005 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 94 Loss = 2.0318923305565155 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 94 Loss_pred = 2.3149728273806343 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 95 Loss = 2.0698468891198045 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 95 Loss_pred = 2.358144318562239 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 96 Loss = 1.9275227272138857 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 96 Loss_pred = 2.1947554221957404 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 97 Loss = 1.9215882828972033 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 97 Loss_pred = 2.090639549319379 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 98 Loss = 2.039323157828668 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 98 Loss_pred = 2.3614527279549065 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 99 Loss = 2.0075830057069846 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 99 Loss_pred = 2.3519475866127943 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 100 Loss = 1.9847518715906307 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 100 Loss_pred = 2.270923990701902 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 101 Loss = 1.8490619438692995 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 101 Loss_pred = 2.272157935323686 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 102 Loss = 1.8867362529010614 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 102 Loss_pred = 2.5013681861039703 Accuracy_pred = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 103 Loss = 1.8807378184873051 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 103 Loss_pred = 2.2003751129949753 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 104 Loss = 1.90780708321528 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 104 Loss_pred = 2.2089310525319665 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 105 Loss = 1.9833517035717183 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 105 Loss_pred = 2.4243880703020153 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 106 Loss = 1.9350509735919807 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 106 Loss_pred = 2.447441983645968 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 107 Loss = 2.100868818137756 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 107 Loss_pred = 2.4561063235423597 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 108 Loss = 1.839250366391168 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 9 batch 108 Loss_pred = 2.2594534162871303 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 109 Loss = 1.9954648471427319 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 109 Loss_pred = 2.2100778217301458 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 110 Loss = 1.9472604674824023 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 110 Loss_pred = 2.4141312626904674 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 111 Loss = 2.1690858419564183 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 111 Loss_pred = 2.3846004101161173 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 112 Loss = 1.9353674677059816 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 112 Loss_pred = 2.251758812837916 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 113 Loss = 1.824523188484496 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 113 Loss_pred = 2.179002319259038 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 114 Loss = 1.8772396755482532 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 114 Loss_pred = 2.2528348874479285 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 115 Loss = 1.938681454225679 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 115 Loss_pred = 2.263213789136325 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 116 Loss = 2.3717517510825314 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 116 Loss_pred = 2.213719871564411 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 117 Loss = 2.289479763321148 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 117 Loss_pred = 2.4046675801380646 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 118 Loss = 2.276691913541571 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 118 Loss_pred = 2.270794828166767 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 119 Loss = 2.1308092826379137 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 119 Loss_pred = 2.2171474317365805 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 120 Loss = 2.212481576827362 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 120 Loss_pred = 2.3510980659166765 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 121 Loss = 2.0222095160907485 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 121 Loss_pred = 2.241501127965156 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 122 Loss = 1.9759912606958931 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 122 Loss_pred = 2.306606971729563 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 123 Loss = 1.945145144313932 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 123 Loss_pred = 2.1044089429208173 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 124 Loss = 2.0582044015208387 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 124 Loss_pred = 2.1758445964906223 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 125 Loss = 2.1799149596642287 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 125 Loss_pred = 2.2766695743892584 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 126 Loss = 1.996830886958389 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 126 Loss_pred = 2.2798286007498034 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 127 Loss = 2.1305698476139434 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 127 Loss_pred = 2.0757151807632495 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 128 Loss = 2.0525289058399876 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 128 Loss_pred = 2.215557340461758 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 129 Loss = 2.174146285193219 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 129 Loss_pred = 2.435097854178276 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 130 Loss = 2.1104982868654902 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 130 Loss_pred = 2.205852070148783 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 131 Loss = 1.873607061920912 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 131 Loss_pred = 2.1060330516048817 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 132 Loss = 2.102676580945964 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 132 Loss_pred = 2.2790166447110702 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 133 Loss = 1.7510532691421883 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 133 Loss_pred = 2.104669346279887 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 134 Loss = 1.960724919183532 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 134 Loss_pred = 2.218762543463244 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 135 Loss = 1.9730750684372367 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 135 Loss_pred = 2.0751880270683256 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 136 Loss = 2.079738640325245 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 136 Loss_pred = 2.130515896430536 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 137 Loss = 2.1024452370120055 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 137 Loss_pred = 2.175390732372542 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 138 Loss = 1.8842850192866285 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 138 Loss_pred = 2.1060583385108194 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 139 Loss = 1.8756877985129234 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 139 Loss_pred = 2.1788034487037926 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 140 Loss = 2.083309960978412 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 140 Loss_pred = 2.1896590332233745 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 141 Loss = 2.049710397529884 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 141 Loss_pred = 2.2322867882000104 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 142 Loss = 1.9182135825236537 Accuracy = 0.84\n",
      "\n",
      "\n",
      "epoch 9 batch 142 Loss_pred = 2.1715783214740467 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 143 Loss = 1.8272599541153038 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 143 Loss_pred = 2.1464871190342882 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 144 Loss = 1.9206539082730176 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 144 Loss_pred = 2.184666329298183 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 145 Loss = 2.0139363825680534 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 145 Loss_pred = 2.087300311856546 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 146 Loss = 2.018010006472729 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 146 Loss_pred = 2.0997970882830703 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 147 Loss = 1.8925957645703222 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 147 Loss_pred = 2.190600368663805 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 148 Loss = 2.1318282232320267 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 148 Loss_pred = 2.3741597268311176 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 149 Loss = 1.946780519349618 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 149 Loss_pred = 2.0802790272393827 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 150 Loss = 2.0594592965079532 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 150 Loss_pred = 2.120855165700129 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 151 Loss = 2.0249499994877915 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 151 Loss_pred = 2.090767331789002 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 152 Loss = 2.2816092577678395 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 152 Loss_pred = 2.1821849178548565 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 153 Loss = 2.3574687854505774 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 153 Loss_pred = 2.217709991003162 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 154 Loss = 2.14764038317654 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 154 Loss_pred = 2.1895242079378536 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 155 Loss = 2.0911756628986073 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 155 Loss_pred = 2.261016196243343 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 156 Loss = 2.047031930827702 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 156 Loss_pred = 2.098554879951798 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 157 Loss = 2.0358719848868216 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 157 Loss_pred = 2.123487406756323 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 158 Loss = 1.9721385803022409 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 158 Loss_pred = 2.043342374126494 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 159 Loss = 2.111624262239244 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 159 Loss_pred = 2.141369028176972 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 160 Loss = 1.9312092208187757 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 160 Loss_pred = 1.9922048911731634 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 9 batch 161 Loss = 2.0550408113360197 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 161 Loss_pred = 2.0785567329424945 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 162 Loss = 2.0281684466007177 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 162 Loss_pred = 2.0983323460060497 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 163 Loss = 2.067385736948115 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 163 Loss_pred = 2.063330982032984 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 164 Loss = 2.0527157539058845 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 164 Loss_pred = 2.2031862658577093 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 165 Loss = 1.895949081067526 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 165 Loss_pred = 2.0827067803241794 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 166 Loss = 2.0574244639249835 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 166 Loss_pred = 2.079577241590888 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 167 Loss = 1.992113726957085 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 167 Loss_pred = 2.056013075943828 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 168 Loss = 2.0501170922295997 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 168 Loss_pred = 2.1222262903367475 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 169 Loss = 2.0371256959310666 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 169 Loss_pred = 2.1404319507052283 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 170 Loss = 1.869045204968748 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 170 Loss_pred = 1.9206334623866825 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 171 Loss = 2.0690999911060404 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 171 Loss_pred = 2.1886273578077566 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 172 Loss = 2.144320495034243 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 9 batch 172 Loss_pred = 2.0940386747556676 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 173 Loss = 2.0480061005827257 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 173 Loss_pred = 1.9675274437397583 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 9 batch 174 Loss = 2.265661410954855 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 174 Loss_pred = 2.022913767933057 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 175 Loss = 2.169134587296365 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 175 Loss_pred = 1.8971888839608557 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 176 Loss = 2.2625020604216703 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 176 Loss_pred = 2.058083536519884 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 177 Loss = 2.374841038801476 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 177 Loss_pred = 2.078162983309913 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 178 Loss = 2.646714243595343 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 178 Loss_pred = 1.9325298908415236 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 179 Loss = 2.4244493584192996 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 179 Loss_pred = 1.9988548709993632 Accuracy_pred = 0.84\n",
      "\n",
      "\n",
      "epoch 9 batch 180 Loss = 2.4752864264896033 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 9 batch 180 Loss_pred = 1.9936652806255135 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 181 Loss = 2.3014454989567112 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 181 Loss_pred = 2.0336289641032197 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 182 Loss = 2.1330857251136024 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 9 batch 182 Loss_pred = 1.9727677259676324 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 183 Loss = 2.3976930022708434 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 9 batch 183 Loss_pred = 2.0290585754432664 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 9 batch 184 Loss = 2.443916428139199 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 184 Loss_pred = 2.091379474400014 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 185 Loss = 2.166262316508089 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 185 Loss_pred = 1.9851803430235433 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 9 batch 186 Loss = 2.0322338678544716 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 186 Loss_pred = 1.9203057801763124 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 187 Loss = 2.2426614763406634 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 187 Loss_pred = 2.0652260015688335 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 188 Loss = 2.0379481035737297 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 188 Loss_pred = 1.9154248199866062 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 9 batch 189 Loss = 2.1090025802040238 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 189 Loss_pred = 2.007111962010456 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 190 Loss = 2.0979495084425857 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 190 Loss_pred = 2.016240671543617 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 191 Loss = 1.9695697677512705 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 191 Loss_pred = 1.918969905505127 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 9 batch 192 Loss = 2.1263585749051277 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 9 batch 192 Loss_pred = 2.053679982480614 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 9 batch 193 Loss = 2.1800593086802134 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 193 Loss_pred = 1.993516267945182 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 9 batch 194 Loss = 2.103355086020307 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 194 Loss_pred = 1.9493788908204819 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 9 batch 195 Loss = 2.1182517842614836 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 195 Loss_pred = 1.9298124272538286 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 196 Loss = 2.1873604368866775 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 9 batch 196 Loss_pred = 1.9003926757526295 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 197 Loss = 2.096772629007514 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 9 batch 197 Loss_pred = 1.869288060214389 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 198 Loss = 2.029241856245107 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 9 batch 198 Loss_pred = 1.8339161302485016 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 9 batch 199 Loss = 2.043022224621896 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 9 batch 199 Loss_pred = 1.8566474904032226 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 9 batch 200 Loss = 2.1459726237040004 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 9 batch 200 Loss_pred = 1.8772196107921404 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "\n",
      "epoch 9 Loss = 3.410003423495663 Accuracy = 0.14\n",
      "\n",
      "\n",
      "\n",
      "epoch 10 batch 1 Loss = 1.7838555044843094 Accuracy = 0.86\n",
      "\n",
      "\n",
      "epoch 10 batch 1 Loss_pred = 1.7838555044843094 Accuracy_pred = 0.86\n",
      "\n",
      "\n",
      "epoch 10 batch 2 Loss = 1.9948070254457226 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 2 Loss_pred = 2.0133806445028912 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 3 Loss = 1.9555788695175784 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 3 Loss_pred = 1.9994691426687643 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 4 Loss = 1.8145612545997938 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 4 Loss_pred = 1.8655681187688598 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 5 Loss = 1.822974037572342 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 5 Loss_pred = 1.9052053282931596 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 6 Loss = 1.8426424258257452 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 6 Loss_pred = 1.9424141714449354 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 7 Loss = 1.7634246897957866 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 7 Loss_pred = 1.9165462404563132 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 8 Loss = 1.895523273113797 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 8 Loss_pred = 2.137784358207367 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 9 Loss = 2.051695386560379 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 9 Loss_pred = 2.089051229782521 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 10 Loss = 1.7815185078019797 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 10 Loss_pred = 1.9316714684019791 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 11 Loss = 1.7221571161143112 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 11 Loss_pred = 1.9290636076950853 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 12 Loss = 1.936074360748586 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 12 Loss_pred = 2.0691424678409702 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 13 Loss = 1.9277447691647043 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 13 Loss_pred = 2.0106495554749726 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 14 Loss = 1.8504505781560787 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 14 Loss_pred = 1.9726576021408972 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 15 Loss = 1.880981659913402 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 15 Loss_pred = 2.010191093116441 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 16 Loss = 1.8427363283044604 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 16 Loss_pred = 2.0286014504486913 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 17 Loss = 1.790786300810265 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 17 Loss_pred = 2.1382933599622156 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 18 Loss = 1.669907606832926 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 18 Loss_pred = 2.0364925846550874 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 19 Loss = 1.6865764037134703 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 19 Loss_pred = 2.0493970209460333 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 20 Loss = 1.6609932254248159 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 20 Loss_pred = 1.9225911182132267 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 21 Loss = 1.7896602783459827 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 21 Loss_pred = 2.0360111746025273 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 22 Loss = 1.8810396021792186 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 22 Loss_pred = 2.091317313376177 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 23 Loss = 1.7798992477968056 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 23 Loss_pred = 2.0145670121928343 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 24 Loss = 1.7629058043380206 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 24 Loss_pred = 2.0334448176735145 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 25 Loss = 1.844861997573479 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 25 Loss_pred = 2.046764699782218 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 26 Loss = 1.703866953738318 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 26 Loss_pred = 1.9213704991870162 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 27 Loss = 1.8537661194305477 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 27 Loss_pred = 2.035488268630326 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 28 Loss = 1.7940583689884082 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 28 Loss_pred = 2.1056245476016024 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 29 Loss = 1.7900059236488404 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 29 Loss_pred = 2.0610615994064587 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 30 Loss = 1.8387604317643085 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 30 Loss_pred = 2.0988209712126102 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 31 Loss = 1.7804426842607075 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 31 Loss_pred = 2.1806403614752727 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 32 Loss = 1.6920503605037887 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 32 Loss_pred = 1.9411186926133552 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 33 Loss = 1.716256701517467 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 33 Loss_pred = 1.9768442831597994 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 34 Loss = 1.715053277136513 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 34 Loss_pred = 2.1644462735730383 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 35 Loss = 1.665975895695369 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 35 Loss_pred = 2.162748670014486 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 36 Loss = 1.7460538837979676 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 36 Loss_pred = 1.9875405833974586 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 37 Loss = 1.8466262822954485 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 37 Loss_pred = 2.0520387865391405 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 38 Loss = 1.790357339100683 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 38 Loss_pred = 2.026328180095943 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 39 Loss = 1.7256050010113972 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 39 Loss_pred = 2.0442164671058087 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 40 Loss = 1.8870976731963398 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 40 Loss_pred = 2.225461401728735 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 41 Loss = 1.7935240583536485 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 41 Loss_pred = 1.9788894920287703 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 42 Loss = 1.8161184152377223 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 42 Loss_pred = 2.047629580348399 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 43 Loss = 1.6965235529333629 Accuracy = 0.86\n",
      "\n",
      "\n",
      "epoch 10 batch 43 Loss_pred = 2.0932455941336183 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 44 Loss = 1.6880206475839066 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 44 Loss_pred = 2.003782225645678 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 45 Loss = 1.813993453433464 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 45 Loss_pred = 2.1370525015835633 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 46 Loss = 1.8043337663402321 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 46 Loss_pred = 2.017965247140322 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 47 Loss = 1.8368769826315918 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 47 Loss_pred = 2.1476610703521026 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 48 Loss = 1.7879459059759089 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 48 Loss_pred = 1.9742946015896843 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 49 Loss = 1.8560204957561872 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 49 Loss_pred = 2.1641312305509626 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 50 Loss = 1.8374851752738166 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 50 Loss_pred = 2.2511892127800635 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 51 Loss = 1.6983788302028426 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 51 Loss_pred = 2.0956730255652514 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 52 Loss = 1.7837163457235934 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 52 Loss_pred = 2.0649643112389744 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 53 Loss = 1.7636013017942611 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 53 Loss_pred = 2.2865922197389 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 54 Loss = 1.8979877636924218 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 54 Loss_pred = 2.1058050153930528 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 55 Loss = 1.5961386713156365 Accuracy = 0.86\n",
      "\n",
      "\n",
      "epoch 10 batch 55 Loss_pred = 1.9671012233184302 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 56 Loss = 1.7645845071668107 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 56 Loss_pred = 2.022583507025029 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 57 Loss = 1.6919214037897559 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 57 Loss_pred = 2.003687708601374 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 58 Loss = 1.7334503393712886 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 58 Loss_pred = 1.958356043892625 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 59 Loss = 2.2661394003952524 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 59 Loss_pred = 2.0742198364065305 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 60 Loss = 2.1215685832334987 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 60 Loss_pred = 1.9762400464096914 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 61 Loss = 1.888301405741493 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 61 Loss_pred = 1.9375132304929608 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 62 Loss = 1.9478842127426281 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 62 Loss_pred = 2.093151929611679 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 63 Loss = 2.0133173739060473 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 63 Loss_pred = 2.0584031918719514 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 64 Loss = 1.907260324451601 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 64 Loss_pred = 2.1361759168257466 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 65 Loss = 1.756398460992925 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 65 Loss_pred = 1.9958843474417873 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 66 Loss = 2.115843094093469 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 66 Loss_pred = 2.1505739261063277 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 67 Loss = 1.822442386379436 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 67 Loss_pred = 2.0748024182839395 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 68 Loss = 1.8109974411890313 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 68 Loss_pred = 2.053859770899218 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 69 Loss = 1.943416725057234 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 69 Loss_pred = 2.319863729700326 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 70 Loss = 2.146307231145938 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 70 Loss_pred = 2.0366418788007876 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 71 Loss = 2.1449702923521126 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 71 Loss_pred = 1.9243031155975958 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 72 Loss = 2.290978988532587 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 10 batch 72 Loss_pred = 2.333909718751512 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 73 Loss = 2.8610620072380577 Accuracy = 0.46\n",
      "\n",
      "\n",
      "epoch 10 batch 73 Loss_pred = 2.144208668014503 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 74 Loss = 2.7642544412581413 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 74 Loss_pred = 2.2043066453241624 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 75 Loss = 2.0998145160226445 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 75 Loss_pred = 2.1357907781941465 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 76 Loss = 2.363734157579594 Accuracy = 0.54\n",
      "\n",
      "\n",
      "epoch 10 batch 76 Loss_pred = 2.2503082282637563 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 10 batch 77 Loss = 2.1213420466916513 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 77 Loss_pred = 2.370295681729734 Accuracy_pred = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 78 Loss = 2.1774140771458335 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 78 Loss_pred = 2.1706055812037066 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 79 Loss = 1.8150184366851831 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 79 Loss_pred = 2.1872679538190902 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 80 Loss = 2.1582991501262727 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 80 Loss_pred = 1.993284834774625 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 81 Loss = 2.4258432507591094 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 81 Loss_pred = 2.196348221508899 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 82 Loss = 1.943976347125025 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 82 Loss_pred = 2.2149782051513847 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 83 Loss = 1.7527586315806851 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 83 Loss_pred = 1.9792644667343728 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 84 Loss = 1.8672519027467007 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 84 Loss_pred = 2.2026362150960717 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 85 Loss = 1.959037941665527 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 85 Loss_pred = 2.3223322883921735 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 86 Loss = 2.030193134705584 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 86 Loss_pred = 2.155427264877835 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 87 Loss = 1.76965581665622 Accuracy = 0.88\n",
      "\n",
      "\n",
      "epoch 10 batch 87 Loss_pred = 2.0755790220747956 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 88 Loss = 1.8608289711747046 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 88 Loss_pred = 2.10754925356941 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 89 Loss = 1.9457948492916421 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 89 Loss_pred = 1.9838369903409705 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 90 Loss = 1.8353340684572705 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 90 Loss_pred = 2.0585277181831616 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 91 Loss = 1.8069937132792457 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 91 Loss_pred = 2.0170437366146765 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 92 Loss = 1.74800886542121 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 92 Loss_pred = 2.1239502947447826 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 93 Loss = 1.7365581401769397 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 93 Loss_pred = 2.1537474072405978 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 94 Loss = 1.757447515338491 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 94 Loss_pred = 2.075098283972829 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 95 Loss = 1.9275887948334196 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 95 Loss_pred = 2.151109958467603 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 96 Loss = 1.690743062549255 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 96 Loss_pred = 1.929661970063848 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 97 Loss = 1.6836988634117882 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 97 Loss_pred = 1.9777018118006664 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 98 Loss = 1.7568137549575027 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 98 Loss_pred = 2.1639392342679447 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 99 Loss = 1.7651538368167952 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 99 Loss_pred = 2.080613817545596 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 100 Loss = 1.7589435053854336 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 100 Loss_pred = 2.1015049377939827 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 101 Loss = 1.6757627827102277 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 101 Loss_pred = 2.126890112174384 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 102 Loss = 1.7887495907249291 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 102 Loss_pred = 2.356633734308867 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 103 Loss = 2.006648028388276 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 103 Loss_pred = 1.9498664838226205 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 104 Loss = 1.9916871831040428 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 104 Loss_pred = 2.1538679643890437 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 105 Loss = 1.9245281656838436 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 105 Loss_pred = 2.2694555295734613 Accuracy_pred = 0.54\n",
      "\n",
      "\n",
      "epoch 10 batch 106 Loss = 1.8229030080939383 Accuracy = 0.84\n",
      "\n",
      "\n",
      "epoch 10 batch 106 Loss_pred = 2.2276262199943204 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 107 Loss = 1.9091853461001034 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 107 Loss_pred = 2.2445790005108126 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 108 Loss = 1.7047366156689094 Accuracy = 0.84\n",
      "\n",
      "\n",
      "epoch 10 batch 108 Loss_pred = 2.080773965733753 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 109 Loss = 1.7813069266629202 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 109 Loss_pred = 2.002461951627198 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 110 Loss = 1.8474799485834936 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 110 Loss_pred = 2.263880233195496 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 111 Loss = 2.036263350212372 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 111 Loss_pred = 2.2127507552963177 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 112 Loss = 1.860409741719937 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 112 Loss_pred = 2.1077619900395876 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 113 Loss = 1.9649991546810435 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 113 Loss_pred = 2.024529013662635 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 114 Loss = 2.0723491950461903 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 114 Loss_pred = 2.12497823379064 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 115 Loss = 1.8180415306445843 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 115 Loss_pred = 1.9635030258251975 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 116 Loss = 1.7905673220151643 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 116 Loss_pred = 2.0111811176363243 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 117 Loss = 2.014733299489715 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 117 Loss_pred = 2.226516155327185 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 118 Loss = 1.986279176069952 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 118 Loss_pred = 2.093971995488746 Accuracy_pred = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 119 Loss = 1.8078317000771498 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 119 Loss_pred = 2.003963898940901 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 120 Loss = 2.0087504053299017 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 120 Loss_pred = 2.180624245162868 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 121 Loss = 1.7584508498284614 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 121 Loss_pred = 2.0302174620768354 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 122 Loss = 1.7431396600965412 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 122 Loss_pred = 2.0924881080040163 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 123 Loss = 1.7455411714675861 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 123 Loss_pred = 1.8959981174375216 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 124 Loss = 1.8117899382411282 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 124 Loss_pred = 2.0916308376107633 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 125 Loss = 2.0322819170594855 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 125 Loss_pred = 2.1030862380007114 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 126 Loss = 1.8691187749427765 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 126 Loss_pred = 2.1652231062498712 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 127 Loss = 1.8315500966803873 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 127 Loss_pred = 1.931833282243759 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 128 Loss = 2.01308761187995 Accuracy = 0.52\n",
      "\n",
      "\n",
      "epoch 10 batch 128 Loss_pred = 1.9824622138050512 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 129 Loss = 2.058721399234002 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 129 Loss_pred = 2.2741073290839253 Accuracy_pred = 0.52\n",
      "\n",
      "\n",
      "epoch 10 batch 130 Loss = 1.8253025898705977 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 130 Loss_pred = 2.0408566621562274 Accuracy_pred = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 131 Loss = 1.7234388325979932 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 131 Loss_pred = 1.9037370495864059 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 132 Loss = 2.200835118999219 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 132 Loss_pred = 2.1818792681350048 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 133 Loss = 1.8263320439099624 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 133 Loss_pred = 1.995776517264756 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 134 Loss = 2.477525168282578 Accuracy = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 134 Loss_pred = 2.051335168292092 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 135 Loss = 2.0826627209363506 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 135 Loss_pred = 1.8052504615005074 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 136 Loss = 2.1217908410931305 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 136 Loss_pred = 1.965795382845426 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 137 Loss = 1.8171848072445573 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 137 Loss_pred = 1.991588098095714 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 138 Loss = 1.8172201083844595 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 138 Loss_pred = 1.9025311285371778 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 139 Loss = 1.7163405802200793 Accuracy = 0.84\n",
      "\n",
      "\n",
      "epoch 10 batch 139 Loss_pred = 2.003023614122438 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 140 Loss = 1.8217366493801144 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 140 Loss_pred = 2.00299023707876 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 141 Loss = 1.8636886824451702 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 141 Loss_pred = 2.0486711029105207 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 142 Loss = 1.7386191307532288 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 142 Loss_pred = 1.9375480655983035 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 143 Loss = 1.6878861094515463 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 143 Loss_pred = 1.9688701803167628 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 144 Loss = 1.7064216313936118 Accuracy = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 144 Loss_pred = 2.014297598531363 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 145 Loss = 1.8571053965297204 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 145 Loss_pred = 1.8737787855229813 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 146 Loss = 1.8000346824678863 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 146 Loss_pred = 1.9599451341340892 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 147 Loss = 1.7405067762708815 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 147 Loss_pred = 1.9906140280575202 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 148 Loss = 1.876405152003201 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 148 Loss_pred = 2.129914913147699 Accuracy_pred = 0.58\n",
      "\n",
      "\n",
      "epoch 10 batch 149 Loss = 1.7550695647641346 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 149 Loss_pred = 1.8850949727698463 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 150 Loss = 1.87821278671245 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 150 Loss_pred = 1.956210380566273 Accuracy_pred = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 151 Loss = 1.7988359982175717 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 151 Loss_pred = 1.9047144187760119 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 152 Loss = 1.864589886404157 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 152 Loss_pred = 1.9620187529891704 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 153 Loss = 1.8568082249379154 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 153 Loss_pred = 1.9484314304792998 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 154 Loss = 1.8653775526991319 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 154 Loss_pred = 1.9858681212000813 Accuracy_pred = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 155 Loss = 1.9896355173364253 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 155 Loss_pred = 2.08088492295138 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 156 Loss = 1.8181626999674478 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 156 Loss_pred = 1.9680348056295502 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 157 Loss = 1.8006493663533099 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 157 Loss_pred = 1.9038621691845614 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 158 Loss = 1.8448704609460083 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 158 Loss_pred = 1.8899053290252084 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 159 Loss = 2.020570067323991 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 159 Loss_pred = 1.9866858354561998 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 160 Loss = 1.7444931442341716 Accuracy = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 160 Loss_pred = 1.8182752005732128 Accuracy_pred = 0.86\n",
      "\n",
      "\n",
      "epoch 10 batch 161 Loss = 1.8208001893025727 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 161 Loss_pred = 1.9077989161426143 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 162 Loss = 1.8196503045140866 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 162 Loss_pred = 1.8607638913808102 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 163 Loss = 1.8525660270986588 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 163 Loss_pred = 1.9058477029360064 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 164 Loss = 1.8470432326094783 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 164 Loss_pred = 2.013965017025013 Accuracy_pred = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 165 Loss = 1.7471693712956533 Accuracy = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 165 Loss_pred = 1.9318759870052895 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 166 Loss = 1.824531992204936 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 166 Loss_pred = 1.9109865256573693 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 167 Loss = 1.7873745436139723 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 167 Loss_pred = 1.8599204995715977 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 168 Loss = 1.9073391672139284 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 168 Loss_pred = 1.910550428506665 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 169 Loss = 1.8952316823796462 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 169 Loss_pred = 1.960254422359995 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 170 Loss = 1.8168674260596482 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 170 Loss_pred = 1.7676574857376022 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 171 Loss = 2.456468854243187 Accuracy = 0.5\n",
      "\n",
      "\n",
      "epoch 10 batch 171 Loss_pred = 1.9976029018349066 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 172 Loss = 2.7973955236158874 Accuracy = 0.4\n",
      "\n",
      "\n",
      "epoch 10 batch 172 Loss_pred = 1.9089587951818119 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 173 Loss = 2.148885855156696 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 173 Loss_pred = 1.8088900235291636 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 174 Loss = 2.0837040835058445 Accuracy = 0.6\n",
      "\n",
      "\n",
      "epoch 10 batch 174 Loss_pred = 1.8200243176681918 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 175 Loss = 1.8668635871505113 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 175 Loss_pred = 1.7010318311258583 Accuracy_pred = 0.84\n",
      "\n",
      "\n",
      "epoch 10 batch 176 Loss = 1.9260447755409371 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 176 Loss_pred = 1.8597806207425578 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 177 Loss = 1.9758209339255521 Accuracy = 0.56\n",
      "\n",
      "\n",
      "epoch 10 batch 177 Loss_pred = 1.9726157922875955 Accuracy_pred = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 178 Loss = 1.8162753438147254 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 178 Loss_pred = 1.7462206136575216 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 179 Loss = 1.8349835095871192 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 179 Loss_pred = 1.7806324172630168 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 180 Loss = 1.7072930299637017 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 180 Loss_pred = 1.8094812964957527 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 181 Loss = 1.867244423175934 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 181 Loss_pred = 1.8608282833370788 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 182 Loss = 1.7610094090121182 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 182 Loss_pred = 1.8042368537582045 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 183 Loss = 1.8645745891818581 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 183 Loss_pred = 1.8560611828350937 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 184 Loss = 1.895390700140095 Accuracy = 0.62\n",
      "\n",
      "\n",
      "epoch 10 batch 184 Loss_pred = 1.8981751180305244 Accuracy_pred = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 185 Loss = 1.8193771495509992 Accuracy = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 185 Loss_pred = 1.776244787603742 Accuracy_pred = 0.82\n",
      "\n",
      "\n",
      "epoch 10 batch 186 Loss = 1.859896401135339 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 186 Loss_pred = 1.734421715446812 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 187 Loss = 1.9613563782443344 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 187 Loss_pred = 1.8628377817125068 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 188 Loss = 1.8748945708103002 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 188 Loss_pred = 1.7068303010325871 Accuracy_pred = 0.86\n",
      "\n",
      "\n",
      "epoch 10 batch 189 Loss = 1.9415975089479067 Accuracy = 0.64\n",
      "\n",
      "\n",
      "epoch 10 batch 189 Loss_pred = 1.8146834356481578 Accuracy_pred = 0.74\n",
      "\n",
      "\n",
      "epoch 10 batch 190 Loss = 1.8514563514637588 Accuracy = 0.66\n",
      "\n",
      "\n",
      "epoch 10 batch 190 Loss_pred = 1.8305016925874744 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 191 Loss = 1.8878846022833187 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 191 Loss_pred = 1.6533669447542647 Accuracy_pred = 0.84\n",
      "\n",
      "\n",
      "epoch 10 batch 192 Loss = 1.890464618694395 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 192 Loss_pred = 1.8225671892447148 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 193 Loss = 1.9147830171347051 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 193 Loss_pred = 1.7860741441068075 Accuracy_pred = 0.88\n",
      "\n",
      "\n",
      "epoch 10 batch 194 Loss = 1.7862810797633335 Accuracy = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 194 Loss_pred = 1.6874836005097853 Accuracy_pred = 0.78\n",
      "\n",
      "\n",
      "epoch 10 batch 195 Loss = 1.8854553244709882 Accuracy = 0.7\n",
      "\n",
      "\n",
      "epoch 10 batch 195 Loss_pred = 1.7094853200885702 Accuracy_pred = 0.84\n",
      "\n",
      "\n",
      "epoch 10 batch 196 Loss = 1.9046532408795074 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 196 Loss_pred = 1.7012643262724243 Accuracy_pred = 0.88\n",
      "\n",
      "\n",
      "epoch 10 batch 197 Loss = 1.8293115706526153 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 197 Loss_pred = 1.6960384654518592 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 198 Loss = 1.847288738105009 Accuracy = 0.72\n",
      "\n",
      "\n",
      "epoch 10 batch 198 Loss_pred = 1.623639958430536 Accuracy_pred = 0.76\n",
      "\n",
      "\n",
      "epoch 10 batch 199 Loss = 1.8749542445055867 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 199 Loss_pred = 1.6571499570655257 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "epoch 10 batch 200 Loss = 1.9227092574826055 Accuracy = 0.68\n",
      "\n",
      "\n",
      "epoch 10 batch 200 Loss_pred = 1.7020096804640639 Accuracy_pred = 0.8\n",
      "\n",
      "\n",
      "\n",
      "epoch 10 Loss = 3.442956459310193 Accuracy = 0.17\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop!(trainlayers, predictlayers;\n",
    "        x_train=x_train_shuf, y_train=y_train_shuf, batch_size=batch_size,epochs=10,\n",
    "        minibatch_size = 50, lr=0.01)  # pool1 after conv2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
