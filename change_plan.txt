What we need to do:

- Set different slice ranges for several arrays

- Set different looping ranges

- Both can be done with similar approach.

- Test with last minibatch of different size

We can:
- set a new range variable or set a new range upper value.  no perf difference--using upper value gives more flexibility.

When using the normal range we don't want any overhead: there is really no difference because the array operations dominate.

OK to have overhead for the different minibatch size-but, turns out there won't be any.

We can have a range variable that we set with an if test.  as long as type stable no execution overhead, but we have the overhead of the if test--several times.

Do it in the feedfwd and backprop functions so we do it once for all layers.


What will the size of the result be for the sliced layer functions?  the sliced size of mutated arrays or the full size of the mutated arrays?  
	a: the next function in the feedforward or backprop loop receives the full layer as its input, accesses an array field--so the callee must slice what it wants from that array

Make sure that array dimensions don't drop:  
	A: a single value in a slice drops the dimension, a : or range keeps the dimension.

How much microbenchmarking value is there in setting a temporary variable equal to a view if the view is reused?  or will the compiler do it for me?  Or does it matter enough?

Change the layer functions to use the batch_size variable:

conv forward:  need range and stop.  DONE
conv backward: same. DONE
linear forward: views on arrays with range.  DONE
linear backward: views on arrays with range, loop range. DONE
maxpool forward: need stop only. DONE
maxpool backward: need stop only. DONE
flatten forward: might not need any change at all. DONE
flatten backward: need range and stop.  DONE
batchnorm forward: need range and stop. tricky to replace 
	linear  DONE
	conv    DONE
batchnorm backward: need to provide new slicing for array 
references, using range
	linear  DONE
	conv    DONE

Look at each activation. Theoretically, we could do activation 
   function on cells we won't use. But, there could be some slight 
   performance advantage of only calculating the values we use.
setup the calls and inputs.
relu!: loop ranges. DONE
relu_grad!: loop ranges. DONE
leaky_relu!, leaky_relu_grad!: DONE

**logistic!**: possibly replace broadcast with loop->then loop range
regression: probably need ranges. assume dense only?

Look at each cost function for use of array sizes:
dloss_dz!: need ranges,  replaces broadcasting with loop  DONE
	passed in y is already a minibatch of the right size
softmax: loop range    DONE
**cross_entropy_cost**: depends on array sizes passed in:  not 	
	using layer structs--don't know as depends on gather_stats!
**mse_cost**: same


Considering another approach.  Which arrays must have views by layer type:

Conv
- a_below
- pad_a_below
- x (can't do in the struct)
- pad_x
- z
- z_norm
- eps_l
- grad_a
- pad_above_eps
- a

Linear
- z
- z_norm
- x  (can't do in struct--unless we can use a view from another layer
- a_below
- eps_l
- grad_a

Maxpool
- mask: not using view
- a: not using view
- eps: not using view

Flatten
- a: not using view
- dl_flat
- eps_l
- a

BatchNorm:  arrays are in corresponding layers
	conv
	- z_norm
	- z
	- eps_l_above

	linear
	- z_norm
	- z
	- eps_l
	- pad_above_eps

Which arrays must have views by activation function:

relu, leaky_relu from input layers
- a
- z
- grad_a

softmax!
- z: not using view
- a: not using view
